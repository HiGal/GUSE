{"id": "624", "url": "https://en.wikipedia.org/wiki?curid=624", "title": "Alaska", "text": "Alaska\n\nAlaska () is a U.S. state located in the northwest extremity of North America. The Canadian administrative divisions of British Columbia and Yukon border the state to the east, its most extreme western part is Attu Island, and it has a maritime border with Russia to the west across the Bering Strait. To the north are the Chukchi and Beaufort seas–the southern parts of the Arctic Ocean. The Pacific Ocean lies to the south and southwest. It is the largest state in the United States by area and the \nseventh largest subnational division in the world. In addition, it is the 3rd least populous and the most sparsely populated of the 50 United States; nevertheless, it is by far the most populous territory located mostly north of the 60th parallel in North America, its population (the total estimated at 738,432 by the U.S. Census Bureau in 2015) more than quadrupling the combined populations of Northern Canada and Greenland. Approximately half of Alaska's residents live within the Anchorage metropolitan area. Alaska's economy is dominated by the fishing, natural gas, and oil industries, resources which it has in abundance. Military bases and tourism are also a significant part of the economy.\n\nThe United States purchased Alaska from the Russian Empire on March 30, 1867, for 7.2 million U.S. dollars at approximately two cents per acre ($4.74/km). The area went through several administrative changes before becoming organized as a territory on May 11, 1912. It was admitted as the 49th state of the U.S. on January 3, 1959.\n\nThe name \"Alaska\" () was introduced in the Russian colonial period when it was used to refer to the peninsula. It was derived from an Aleut, or Unangam idiom, which figuratively refers to the mainland of Alaska. Literally, it means \"object to which the action of the sea is directed\".\n\nAlaska is the northernmost and westernmost state in the United States and has the most easterly longitude in the United States because the Aleutian Islands extend into the Eastern Hemisphere. Alaska is the only non-contiguous U.S. state on continental North America; about of British Columbia (Canada) separates Alaska from Washington. It is technically part of the continental U.S., but is sometimes not included in colloquial use; Alaska is not part of the contiguous U.S., often called \"the Lower 48\". The capital city, Juneau, is situated on the mainland of the North American continent but is not connected by road to the rest of the North American highway system.\n\nThe state is bordered by Yukon and British Columbia in Canada, to the east, the Gulf of Alaska and the Pacific Ocean to the south and southwest, the Bering Sea, Bering Strait, and Chukchi Sea to the west and the Arctic Ocean to the north. Alaska's territorial waters touch Russia's territorial waters in the Bering Strait, as the Russian Big Diomede Island and Alaskan Little Diomede Island are only apart. Alaska has a longer coastline than all the other U.S. states combined.\n\nAlaska is the largest state in the United States by total area at , over twice the size of Texas, the next largest state. Alaska is larger than all but 18 sovereign countries. Counting territorial waters, Alaska is larger than the combined area of the next three largest states: Texas, California, and Montana. It is also larger than the combined area of the 22 smallest U.S. states.\n\nThere are no officially defined borders demarcating the various regions of Alaska, but there are six widely accepted regions:\n\nThe most populous region of Alaska, containing Anchorage, the Matanuska-Susitna Valley and the Kenai Peninsula. Rural, mostly unpopulated areas south of the Alaska Range and west of the Wrangell Mountains also fall within the definition of South Central, as do the Prince William Sound area and the communities of Cordova and Valdez.\n\nAlso referred to as the Panhandle or Inside Passage, this is the region of Alaska closest to the rest of the United States. As such, this was where most of the initial non-indigenous settlement occurred in the years following the Alaska Purchase. The region is dominated by the Alexander Archipelago as well as the Tongass National Forest, the largest national forest in the United States. It contains the state capital Juneau, the former capital Sitka, and Ketchikan, at one time Alaska's largest city. The Alaska Marine Highway provides a vital surface transportation link throughout the area, as only three communities (Haines, Hyder and Skagway) enjoy direct connections to the contiguous North American road system. Officially designated in 1963.\n\nThe Interior is the largest region of Alaska; much of it is uninhabited wilderness. Fairbanks is the only large city in the region. Denali National Park and Preserve is located here. \"Denali\" is the highest mountain in North America.\n\nSouthwest Alaska is a sparsely inhabited region stretching some inland from the Bering Sea. Most of the population lives along the coast. Kodiak Island is also located in Southwest. The massive Yukon–Kuskokwim Delta, one of the largest river deltas in the world, is here. Portions of the Alaska Peninsula are considered part of Southwest, with the remaining portions included with the Aleutian Islands (see below).\n\nThe North Slope is mostly tundra peppered with small villages. The area is known for its massive reserves of crude oil, and contains both the National Petroleum Reserve–Alaska and the Prudhoe Bay Oil Field. Barrow, the northernmost city in the United States, is located here. The Northwest Arctic area, anchored by Kotzebue and also containing the Kobuk River valley, is often regarded as being part of this region. However, the respective Inupiat of the North Slope and of the Northwest Arctic seldom consider themselves to be one people.\n\nMore than 300 small volcanic islands make up this chain, which stretches over into the Pacific Ocean. Some of these islands fall in the Eastern Hemisphere, but the International Date Line was drawn west of 180° to keep the whole state, and thus the entire North American continent, within the same legal day. Two of the islands, Attu and Kiska, were occupied by Japanese forces during World War II.\n\nWith its myriad islands, Alaska has nearly of tidal shoreline. The Aleutian Islands chain extends west from the southern tip of the Alaska Peninsula. Many active volcanoes are found in the Aleutians and in coastal regions. Unimak Island, for example, is home to Mount Shishaldin, which is an occasionally smoldering volcano that rises to above the North Pacific. It is the most perfect volcanic cone on Earth, even more symmetrical than Japan's Mount Fuji. The chain of volcanoes extends to Mount Spurr, west of Anchorage on the mainland. Geologists have identified Alaska as part of Wrangellia, a large region consisting of multiple states and Canadian provinces in the Pacific Northwest, which is actively undergoing continent building.\n\nOne of the world's largest tides occurs in Turnagain Arm, just south of Anchorage – tidal differences can be more than .\nAlaska has more than three million lakes. Marshlands and wetland permafrost cover (mostly in northern, western and southwest flatlands). Glacier ice covers some of land and of tidal zone. The Bering Glacier complex near the southeastern border with Yukon covers alone. With over 100,000 glaciers, Alaska has half of all in the world.\n\nAccording to an October 1998 report by the United States Bureau of Land Management, approximately 65% of Alaska is owned and managed by the U.S. federal government as public lands, including a multitude of national forests, national parks, and national wildlife refuges. Of these, the Bureau of Land Management manages , or 23.8% of the state. The Arctic National Wildlife Refuge is managed by the United States Fish and Wildlife Service. It is the world's largest wildlife refuge, comprising .\n\nOf the remaining land area, the state of Alaska owns , its entitlement under the Alaska Statehood Act. A portion of that acreage is occasionally ceded to organized boroughs, under the statutory provisions pertaining to newly formed boroughs. Smaller portions are set aside for rural subdivisions and other homesteading-related opportunities. These are not very popular due to the often remote and roadless locations. The University of Alaska, as a land grant university, also owns substantial acreage which it manages independently.\n\nAnother are owned by 12 regional, and scores of local, Native corporations created under the Alaska Native Claims Settlement Act (ANCSA) of 1971. Regional Native corporation Doyon, Limited often promotes itself as the largest private landowner in Alaska in advertisements and other communications. Provisions of ANCSA allowing the corporations' land holdings to be sold on the open market starting in 1991 were repealed before they could take effect. Effectively, the corporations hold title (including subsurface title in many cases, a privilege denied to individual Alaskans) but cannot sell the land. Individual Native allotments can be and are sold on the open market, however.\n\nVarious private interests own the remaining land, totaling about one percent of the state. Alaska is, by a large margin, the state with the smallest percentage of private land ownership when Native corporation holdings are excluded.\n\nThe climate in Southeast Alaska is a mid-latitude oceanic climate (Köppen climate classification: \"Cfb\") in the southern sections and a subarctic oceanic climate (Köppen \"Cfc\") in the northern parts. On an annual basis, Southeast is both the wettest and warmest part of Alaska with milder temperatures in the winter and high precipitation throughout the year. Juneau averages over of precipitation a year, and Ketchikan averages over . This is also the only region in Alaska in which the average daytime high temperature is above freezing during the winter months.\n\nThe climate of Anchorage and south central Alaska is mild by Alaskan standards due to the region's proximity to the seacoast. While the area gets less rain than southeast Alaska, it gets more snow, and days tend to be clearer. On average, Anchorage receives of precipitation a year, with around of snow, although there are areas in the south central which receive far more snow. It is a subarctic climate () due to its brief, cool summers.\n\nThe climate of Western Alaska is determined in large part by the Bering Sea and the Gulf of Alaska. It is a subarctic oceanic climate in the southwest and a continental subarctic climate farther north. The temperature is somewhat moderate considering how far north the area is. This region has a tremendous amount of variety in precipitation. An area stretching from the northern side of the Seward Peninsula to the Kobuk River valley (i. e., the region around Kotzebue Sound) is technically a desert, with portions receiving less than of precipitation annually. On the other extreme, some locations between Dillingham and Bethel average around of precipitation.\n\nThe climate of the interior of Alaska is subarctic. Some of the highest and lowest temperatures in Alaska occur around the area near Fairbanks. The summers may have temperatures reaching into the 90s °F (the low-to-mid 30s °C), while in the winter, the temperature can fall below . Precipitation is sparse in the Interior, often less than a year, but what precipitation falls in the winter tends to stay the entire winter.\n\nThe highest and lowest recorded temperatures in Alaska are both in the Interior. The highest is in Fort Yukon (which is just inside the arctic circle) on June 27, 1915, making Alaska tied with Hawaii as the state with the lowest high temperature in the United States. The lowest official Alaska temperature is in Prospect Creek on January 23, 1971, one degree above the lowest temperature recorded in continental North America (in Snag, Yukon, Canada).\n\nThe climate in the extreme north of Alaska is Arctic () with long, very cold winters and short, cool summers. Even in July, the average low temperature in Barrow is . Precipitation is light in this part of Alaska, with many places averaging less than per year, mostly as snow which stays on the ground almost the entire year.\n\nNumerous indigenous peoples occupied Alaska for thousands of years before the arrival of European peoples to the area. Linguistic and DNA studies done here have provided evidence for the settlement of North America by way of the Bering land bridge. The Tlingit people developed a society with a matrilineal kinship system of property inheritance and descent in what is today Southeast Alaska, along with parts of British Columbia and the Yukon. Also in Southeast were the Haida, now well known for their unique arts. The Tsimshian people came to Alaska from British Columbia in 1887, when President Grover Cleveland, and later the U.S. Congress, granted them permission to settle on Annette Island and found the town of Metlakatla. All three of these peoples, as well as other indigenous peoples of the Pacific Northwest Coast, experienced smallpox outbreaks from the late 18th through the mid-19th century, with the most devastating epidemics occurring in the 1830s and 1860s, resulting in high fatalities and social disruption.\n\nThe Aleutian Islands are still home to the Aleut people's seafaring society, although they were the first Native Alaskans to be exploited by Russians. Western and Southwestern Alaska are home to the Yup'ik, while their cousins the Alutiiq ~ Sugpiaq lived in what is now Southcentral Alaska. The Gwich'in people of the northern Interior region are Athabaskan and primarily known today for their dependence on the caribou within the much-contested Arctic National Wildlife Refuge. The North Slope and Little Diomede Island are occupied by the widespread Inupiat people.\n\nSome researchers believe that the first Russian settlement in Alaska was established in the 17th century. According to this hypothesis, in 1648 several koches of Semyon Dezhnyov's expedition came ashore in Alaska by storm and founded this settlement. This hypothesis is based on the testimony of Chukchi geographer Nikolai Daurkin, who had visited Alaska in 1764–1765 and who had reported on a village on the Kheuveren River, populated by \"bearded men\" who \"pray to the icons\". Some modern researchers associate Kheuveren with Koyuk River.\n\nThe first European vessel to reach Alaska is generally held to be the \"St. Gabriel\" under the authority of the surveyor M. S. Gvozdev and assistant navigator I. Fyodorov on August 21, 1732, during an expedition of Siberian cossak A. F. Shestakov and Belorussian explorer Dmitry Pavlutsky (1729–1735).\n\nAnother European contact with Alaska occurred in 1741, when Vitus Bering led an expedition for the Russian Navy aboard the \"St. Peter\". After his crew returned to Russia with sea otter pelts judged to be the finest fur in the world, small associations of fur traders began to sail from the shores of Siberia toward the Aleutian Islands. The first permanent European settlement was founded in 1784.\nBetween 1774 and 1800, Spain sent several expeditions to Alaska in order to assert its claim over the Pacific Northwest. In 1789 a Spanish settlement and fort were built in Nootka Sound. These expeditions gave names to places such as Valdez, Bucareli Sound, and Cordova. Later, the Russian-American Company carried out an expanded colonization program during the early-to-mid-19th century.\n\nSitka, renamed New Archangel from 1804 to 1867, on Baranof Island in the Alexander Archipelago in what is now Southeast Alaska, became the capital of Russian America. It remained the capital after the colony was transferred to the United States. The Russians never fully colonized Alaska, and the colony was never very profitable. Evidence of Russian settlement in names and churches survive throughout southeast Alaska.\n\nWilliam H. Seward, the United States Secretary of State, negotiated the Alaska Purchase (also known as Seward's Folly) with the Russians in 1867 for $7.2 million. Alaska was loosely governed by the military initially, and was administered as a district starting in 1884, with a governor appointed by the President of the United States. A federal district court was headquartered in Sitka.\n\nFor most of Alaska's first decade under the United States flag, Sitka was the only community inhabited by American settlers. They organized a \"provisional city government,\" which was Alaska's first municipal government, but not in a legal sense. Legislation allowing Alaskan communities to legally incorporate as cities did not come about until 1900, and home rule for cities was extremely limited or unavailable until statehood took effect in 1959.\n\nStarting in the 1890s and stretching in some places to the early 1910s, gold rushes in Alaska and the nearby Yukon Territory brought thousands of miners and settlers to Alaska. Alaska was officially incorporated as an organized territory in 1912. Alaska's capital, which had been in Sitka until 1906, was moved north to Juneau. Construction of the Alaska Governor's Mansion began that same year. European immigrants from Norway and Sweden also settled in southeast Alaska, where they entered the fishing and logging industries.\n\nDuring World War II, the Aleutian Islands Campaign focused on the three outer Aleutian Islands – Attu, Agattu and Kiska – that were invaded by Japanese troops and occupied between June 1942 and August 1943. During the occupation, one Alaskan civilian was killed by Japanese troops and nearly fifty were interned in Japan, where about half of them died. Unalaska/Dutch Harbor became a significant base for the United States Army Air Forces and Navy submariners.\n\nThe United States Lend-Lease program involved the flying of American warplanes through Canada to Fairbanks and then Nome; Soviet pilots took possession of these aircraft, ferrying them to fight the German invasion of the Soviet Union. The construction of military bases contributed to the population growth of some Alaskan cities.\n\nStatehood for Alaska was an important cause of James Wickersham early in his tenure as a congressional delegate. Decades later, the statehood movement gained its first real momentum following a territorial referendum in 1946. The Alaska Statehood Committee and Alaska's Constitutional Convention would soon follow. Statehood supporters also found themselves fighting major battles against political foes, mostly in the U.S. Congress but also within Alaska. Statehood was approved by Congress on July 7, 1958. Alaska was officially proclaimed a state on January 3, 1959.\n\nIn 1960, the Census Bureau reported Alaska's population as 77.2% White, 3% Black, and 18.8% American Indian and Alaska Native.\n\nOn March 27, 1964, the massive Good Friday earthquake killed 133 people and destroyed several villages and portions of large coastal communities, mainly by the resultant tsunamis and landslides. It was the second-most-powerful earthquake in the recorded history of the world, with a moment magnitude of 9.2. It was over one thousand times more powerful than the 1989 San Francisco earthquake. The time of day (5:36 pm), time of year and location of the epicenter were all cited as factors in potentially sparing thousands of lives, particularly in Anchorage.\n\nThe 1968 discovery of oil at Prudhoe Bay and the 1977 completion of the Trans-Alaska Pipeline System led to an oil boom. Royalty revenues from oil have funded large state budgets from 1980 onward. That same year, not coincidentally, Alaska repealed its state income tax.\n\nIn 1989, the \"Exxon Valdez\" hit a reef in the Prince William Sound, spilling over of crude oil over of coastline. Today, the battle between philosophies of development and conservation is seen in the contentious debate over oil drilling in the Arctic National Wildlife Refuge and the proposed Pebble Mine.\n\nThe Alaska Heritage Resources Survey (AHRS) is a restricted inventory of all reported historic and prehistoric sites within the state of Alaska; it is maintained by the Office of History and Archaeology. The survey's inventory of cultural resources includes objects, structures, buildings, sites, districts, and travel ways, with a general provision that they are over 50 years old. As of January 31, 2012, over 35,000 sites have been reported.\n\nThe United States Census Bureau estimates that the population of Alaska was 738,432 on July 1, 2015, a 3.97% increase since the 2010 United States Census.\n\nIn 2010, Alaska ranked as the 47th state by population, ahead of North Dakota, Vermont, and Wyoming (and Washington, D.C.) Alaska is the least densely populated state, and one of the most sparsely populated areas in the world, at , with the next state, Wyoming, at . Alaska is the largest U.S. state by area, and the tenth wealthiest (per capita income). As of November 2014, the state's unemployment rate was 6.6%.\n\nAccording to the 2010 United States Census, Alaska had a population of 710,231. In terms of race and ethnicity, the state was 66.7% White (64.1% Non-Hispanic White), 14.8% American Indian and Alaska Native, 5.4% Asian, 3.3% Black or African American, 1.0% Native Hawaiian and Other Pacific Islander, 1.6% from Some Other Race, and 7.3% from Two or More Races. Hispanics or Latinos of any race made up 5.5% of the population.\n\n, 50.7% of Alaska's population younger than one year of age belonged to minority groups (i.e., did not have two parents of non-Hispanic white ancestry).\n\nAccording to the 2011 American Community Survey, 83.4% of people over the age of five speak only English at home. About 3.5% speak Spanish at home. About 2.2% speak another Indo-European language at home and about 4.3% speak an Asian language (including Tagalog) at home. About 5.3% speak other languages at home.\n\nThe Alaska Native Language Center at the University of Alaska Fairbanks claims that at least 20 Alaskan native languages exist and there are also some languages with different dialects. Most of Alaska's native languages belong to either the Eskimo–Aleut or Na-Dene language families however some languages are thought to be isolates (e.g. Haida) or have not yet been classified (e.g. Tsimshianic).\n\nA total of 5.2% of Alaskans speak one of the state's 20 indigenous languages, known locally as \"native languages\".\n\nIn October 2014, the governor of Alaska signed a bill declaring the state's 20 indigenous languages as official languages. This bill gave the languages symbolic recognition as official languages, though they have not been adopted for official use within the government. The 20 languages that were included in the bill are:\n\nAccording to statistics collected by the Association of Religion Data Archives from 2010, about 34% of Alaska residents were members of religious congregations. 100,960 people identified as Evangelical Protestants, 50,866 as Roman Catholic, and 32,550 as mainline Protestants. Roughly 4% are Mormon, 0.5% are Jewish, 1% are Muslim, 0.5% are Buddhist, and 0.5% are Hindu. The largest religious denominations in Alaska were the Catholic Church with 50,866 adherents, non-denominational Evangelical Protestants with 38,070 adherents, The Church of Jesus Christ of Latter-day Saints with 32,170 adherents, and the Southern Baptist Convention with 19,891 adherents. Alaska has been identified, along with Pacific Northwest states Washington and Oregon, as being the least religious states of the USA, in terms of church membership.\n\nIn 1795, the First Russian Orthodox Church was established in Kodiak. Intermarriage with Alaskan Natives helped the Russian immigrants integrate into society. As a result, an increasing number of Russian Orthodox churches gradually became established within Alaska. Alaska also has the largest Quaker population (by percentage) of any state. In 2009 there were 6,000 Jews in Alaska (for whom observance of halakha may pose special problems). Alaskan Hindus often share venues and celebrations with members of other Asian religious communities, including Sikhs and Jains.\n\nEstimates for the number of Muslims in Alaska range from 2,000 to 5,000. The Islamic Community Center of Anchorage began efforts in the late 1990s to construct a mosque in Anchorage. They broke ground on a building in south Anchorage in 2010 and were nearing completion in late 2014. When completed, the mosque will be the first in the state and one of the northernmost mosques in the world.\n\nThe 2007 gross state product was $44.9 billion, 45th in the nation. Its per capita personal income for 2007 was $40,042, ranking 15th in the nation. According to a 2013 study by Phoenix Marketing International, Alaska had the fifth-largest number of millionaires per capita in the United States, with a ratio of 6.75 percent. The oil and gas industry dominates the Alaskan economy, with more than 80% of the state's revenues derived from petroleum extraction. Alaska's main export product (excluding oil and natural gas) is seafood, primarily salmon, cod, Pollock and crab.\n\nAgriculture represents a very small fraction of the Alaskan economy. Agricultural production is primarily for consumption within the state and includes nursery stock, dairy products, vegetables, and livestock. Manufacturing is limited, with most foodstuffs and general goods imported from elsewhere.\n\nEmployment is primarily in government and industries such as natural resource extraction, shipping, and transportation. Military bases are a significant component of the economy in the Fairbanks North Star, Anchorage and Kodiak Island boroughs, as well as Kodiak. Federal subsidies are also an important part of the economy, allowing the state to keep taxes low. Its industrial outputs are crude petroleum, natural gas, coal, gold, precious metals, zinc and other mining, seafood processing, timber and wood products. There is also a growing service and tourism sector. Tourists have contributed to the economy by supporting local lodging.\n\nAlaska has vast energy resources, although its oil reserves have been largely depleted. Major oil and gas reserves were found in the Alaska North Slope (ANS) and Cook Inlet basins, but according to the Energy Information Administration, by February 2014 Alaska had fallen to fourth place in the nation in crude oil production after Texas, North Dakota, and California. Prudhoe Bay on Alaska's North Slope is still the second highest-yielding oil field in the United States, typically producing about , although by early 2014 North Dakota's Bakken Formation was producing over . Prudhoe Bay was the largest conventional oil field ever discovered in North America, but was much smaller than Canada's enormous Athabasca oil sands field, which by 2014 was producing about of unconventional oil, and had hundreds of years of producible reserves at that rate.\n\nThe Trans-Alaska Pipeline can transport and pump up to of crude oil per day, more than any other crude oil pipeline in the United States. Additionally, substantial coal deposits are found in Alaska's bituminous, sub-bituminous, and lignite coal basins. The United States Geological Survey estimates that there are of undiscovered, technically recoverable gas from natural gas hydrates on the Alaskan North Slope. Alaska also offers some of the highest hydroelectric power potential in the country from its numerous rivers. Large swaths of the Alaskan coastline offer wind and geothermal energy potential as well.\nAlaska's economy depends heavily on increasingly expensive diesel fuel for heating, transportation, electric power and light. Though wind and hydroelectric power are abundant and underdeveloped, proposals for statewide energy systems (e.g. with special low-cost electric interties) were judged uneconomical (at the time of the report, 2001) due to low (less than 50¢/gal) fuel prices, long distances and low population. The cost of a gallon of gas in urban Alaska today is usually 30–60¢ higher than the national average; prices in rural areas are generally significantly higher but vary widely depending on transportation costs, seasonal usage peaks, nearby petroleum development infrastructure and many other factors.\n\nThe Alaska Permanent Fund is a constitutionally authorized appropriation of oil revenues, established by voters in 1976 to manage a surplus in state petroleum revenues from oil, largely in anticipation of the then recently constructed Trans-Alaska Pipeline System. The fund was originally proposed by Governor Keith Miller on the eve of the 1969 Prudhoe Bay lease sale, out of fear that the legislature would spend the entire proceeds of the sale (which amounted to $900 million) at once. It was later championed by Governor Jay Hammond and Kenai state representative Hugh Malone. It has served as an attractive political prospect ever since, diverting revenues which would normally be deposited into the general fund.\n\nThe Alaska Constitution was written so as to discourage dedicating state funds for a particular purpose. The Permanent Fund has become the rare exception to this, mostly due to the political climate of distrust existing during the time of its creation. From its initial principal of $734,000, the fund has grown to $50 billion as a result of oil royalties and capital investment programs. Most if not all the principal is invested conservatively outside Alaska. This has led to frequent calls by Alaskan politicians for the Fund to make investments within Alaska, though such a stance has never gained momentum.\n\nStarting in 1982, dividends from the fund's annual growth have been paid out each year to eligible Alaskans, ranging from an initial $1,000 in 1982 (equal to three years' payout, as the distribution of payments was held up in a lawsuit over the distribution scheme) to $3,269 in 2008 (which included a one-time $1,200 \"Resource Rebate\"). Every year, the state legislature takes out 8% from the earnings, puts 3% back into the principal for inflation proofing, and the remaining 5% is distributed to all qualifying Alaskans. To qualify for the Permanent Fund Dividend, one must have lived in the state for a minimum of 12 months, maintain constant residency subject to allowable absences, and not be subject to court judgments or criminal convictions which fall under various disqualifying classifications or may subject the payment amount to civil garnishment.\n\nThe Permanent Fund is often considered to be one of the leading examples of a \"Basic Income\" policy in the world.\n\nThe cost of goods in Alaska has long been higher than in the contiguous 48 states. Federal government employees, particularly United States Postal Service (USPS) workers and active-duty military members, receive a Cost of Living Allowance usually set at 25% of base pay because, while the cost of living has gone down, it is still one of the highest in the country.\n\nRural Alaska suffers from extremely high prices for food and consumer goods compared to the rest of the country, due to the relatively limited transportation infrastructure.\n\nDue to the northern climate and short growing season, relatively little farming occurs in Alaska. Most farms are in either the Matanuska Valley, about northeast of Anchorage, or on the Kenai Peninsula, about southwest of Anchorage. The short 100-day growing season limits the crops that can be grown, but the long sunny summer days make for productive growing seasons. The primary crops are potatoes, carrots, lettuce, and cabbage.\n\nThe Tanana Valley is another notable agricultural locus, especially the Delta Junction area, about southeast of Fairbanks, with a sizable concentration of farms growing agronomic crops; these farms mostly lie north and east of Fort Greely. This area was largely set aside and developed under a state program spearheaded by Hammond during his second term as governor. Delta-area crops consist predominately of barley and hay. West of Fairbanks lies another concentration of small farms catering to restaurants, the hotel and tourist industry, and community-supported agriculture.\n\nAlaskan agriculture has experienced a surge in growth of market gardeners, small farms and farmers' markets in recent years, with the highest percentage increase (46%) in the nation in growth in farmers' markets in 2011, compared to 17% nationwide. The peony industry has also taken off, as the growing season allows farmers to harvest during a gap in supply elsewhere in the world, thereby filling a niche in the flower market.\nAlaska, with no counties, lacks county fairs. However, a small assortment of state and local fairs (with the Alaska State Fair in Palmer the largest), are held mostly in the late summer. The fairs are mostly located in communities with historic or current agricultural activity, and feature local farmers exhibiting produce in addition to more high-profile commercial activities such as carnival rides, concerts and food. \"Alaska Grown\" is used as an agricultural slogan.\n\nAlaska has an abundance of seafood, with the primary fisheries in the Bering Sea and the North Pacific. Seafood is one of the few food items that is often cheaper within the state than outside it. Many Alaskans take advantage of salmon seasons to harvest portions of their household diet while fishing for subsistence, as well as sport. This includes fish taken by hook, net or wheel.\n\nHunting for subsistence, primarily caribou, moose, and Dall sheep is still common in the state, particularly in remote Bush communities. An example of a traditional native food is Akutaq, the Eskimo ice cream, which can consist of reindeer fat, seal oil, dried fish meat and local berries.\n\nAlaska's reindeer herding is concentrated on Seward Peninsula, where wild caribou can be prevented from mingling and migrating with the domesticated reindeer.\n\nMost food in Alaska is transported into the state from \"Outside\", and shipping costs make food in the cities relatively expensive. In rural areas, subsistence hunting and gathering is an essential activity because imported food is prohibitively expensive. Though most small towns and villages in Alaska lie along the coastline, the cost of importing food to remote villages can be high, because of the terrain and difficult road conditions, which change dramatically, due to varying climate and precipitation changes. The cost of transport can reach as high as 50¢ per pound ($1.10/kg) or more in some remote areas, during the most difficult times, if these locations can be reached at all during such inclement weather and terrain conditions. The cost of delivering a of milk is about $3.50 in many villages where per capita income can be $20,000 or less. Fuel cost per gallon is routinely 20–30¢ higher than the continental United States average, with only Hawaii having higher prices.\n\nAlaska has few road connections compared to the rest of the U.S. The state's road system covers a relatively small area of the state, linking the central population centers and the Alaska Highway, the principal route out of the state through Canada. The state capital, Juneau, is not accessible by road, only a car ferry, which has spurred several debates over the decades about moving the capital to a city on the road system, or building a road connection from Haines. The western part of Alaska has no road system connecting the communities with the rest of Alaska.\n\nOne unique feature of the Alaska Highway system is the Anton Anderson Memorial Tunnel, an active Alaska Railroad tunnel recently upgraded to provide a paved roadway link with the isolated community of Whittier on Prince William Sound to the Seward Highway about southeast of Anchorage at Portage. At , the tunnel was the longest road tunnel in North America until 2007. The tunnel is the longest combination road and rail tunnel in North America.\n\nBuilt around 1915, the Alaska Railroad (ARR) played a key role in the development of Alaska through the 20th century. It links north Pacific shipping through providing critical infrastructure with tracks that run from Seward to Interior Alaska by way of South Central Alaska, passing through Anchorage, Eklutna, Wasilla, Talkeetna, Denali, and Fairbanks, with spurs to Whittier, Palmer and North Pole. The cities, towns, villages, and region served by ARR tracks are known statewide as \"The Railbelt\". In recent years, the ever-improving paved highway system began to eclipse the railroad's importance in Alaska's economy.\n\nThe railroad played a vital role in Alaska's development, moving freight into Alaska while transporting natural resources southward (i.e., coal from the Usibelli coal mine near Healy to Seward and gravel from the Matanuska Valley to Anchorage). It is well known for its summertime tour passenger service.\n\nThe Alaska Railroad was one of the last railroads in North America to use cabooses in regular service and still uses them on some gravel trains. It continues to offer one of the last flag stop routes in the country. A stretch of about of track along an area north of Talkeetna remains inaccessible by road; the railroad provides the only transportation to rural homes and cabins in the area. Until construction of the Parks Highway in the 1970s, the railroad provided the only land access to most of the region along its entire route.\n\nIn northern Southeast Alaska, the White Pass and Yukon Route also partly runs through the state from Skagway northwards into Canada (British Columbia and Yukon Territory), crossing the border at White Pass Summit. This line is now mainly used by tourists, often arriving by cruise liner at Skagway. It was featured in the 1983 BBC television series \"Great Little Railways.\"\n\nThe Alaska Rail network is not connected to Outside. In 2000, the U.S. Congress authorized $6 million to study the feasibility of a rail link between Alaska, Canada, and the lower 48.\n\nAlaska Rail Marine provides car float service between Whittier and Seattle.\n\nMany cities, towns and villages in the state do not have road or highway access; the only modes of access involve travel by air, river, or the sea.\n\nAlaska's well-developed state-owned ferry system (known as the Alaska Marine Highway) serves the cities of southeast, the Gulf Coast and the Alaska Peninsula. The ferries transport vehicles as well as passengers. The system also operates a ferry service from Bellingham, Washington and Prince Rupert, British Columbia in Canada through the Inside Passage to Skagway. The Inter-Island Ferry Authority also serves as an important marine link for many communities in the Prince of Wales Island region of Southeast and works in concert with the Alaska Marine Highway.\n\nIn recent years, cruise lines have created a summertime tourism market, mainly connecting the Pacific Northwest to Southeast Alaska and, to a lesser degree, towns along Alaska's gulf coast. The population of Ketchikan may rise by over 10,000 people on many days during the summer, as up to four large cruise ships at a time can dock, debarking thousands of passengers.\n\nCities not served by road, sea, or river can be reached only by air, foot, dogsled, or snowmachine, accounting for Alaska's extremely well developed bush air services—an Alaskan novelty. Anchorage and, to a lesser extent Fairbanks, is served by many major airlines. Because of limited highway access, air travel remains the most efficient form of transportation in and out of the state. Anchorage recently completed extensive remodeling and construction at Ted Stevens Anchorage International Airport to help accommodate the upsurge in tourism (in 2012–2013, Alaska received almost 2 million visitors).\n\nRegular flights to most villages and towns within the state that are commercially viable are challenging to provide, so they are heavily subsidized by the federal government through the Essential Air Service program. Alaska Airlines is the only major airline offering in-state travel with jet service (sometimes in combination cargo and passenger Boeing 737-400s) from Anchorage and Fairbanks to regional hubs like Bethel, Nome, Kotzebue, Dillingham, Kodiak, and other larger communities as well as to major Southeast and Alaska Peninsula communities.\n\nThe bulk of remaining commercial flight offerings come from small regional commuter airlines such as Ravn Alaska, PenAir, and Frontier Flying Service. The smallest towns and villages must rely on scheduled or chartered bush flying services using general aviation aircraft such as the Cessna Caravan, the most popular aircraft in use in the state. Much of this service can be attributed to the Alaska bypass mail program which subsidizes bulk mail delivery to Alaskan rural communities. The program requires 70% of that subsidy to go to carriers who offer passenger service to the communities.\n\nMany communities have small air taxi services. These operations originated from the demand for customized transport to remote areas. Perhaps the most quintessentially Alaskan plane is the bush seaplane. The world's busiest seaplane base is Lake Hood, located next to Ted Stevens Anchorage International Airport, where flights bound for remote villages without an airstrip carry passengers, cargo, and many items from stores and warehouse clubs. In 2006 Alaska had the highest number of pilots per capita of any U.S. state.\n\nAnother Alaskan transportation method is the dogsled. In modern times (that is, any time after the mid-late 1920s), dog mushing is more of a sport than a true means of transportation. Various races are held around the state, but the best known is the Iditarod Trail Sled Dog Race, a trail from Anchorage to Nome (although the distance varies from year to year, the official distance is set at ). The race commemorates the famous 1925 serum run to Nome in which mushers and dogs like Togo and Balto took much-needed medicine to the diphtheria-stricken community of Nome when all other means of transportation had failed. Mushers from all over the world come to Anchorage each March to compete for cash, prizes, and prestige. The \"Serum Run\" is another sled dog race that more accurately follows the route of the famous 1925 relay, leaving from the community of Nenana (southwest of Fairbanks) to Nome.\n\nIn areas not served by road or rail, primary transportation in summer is by all-terrain vehicle and in winter by snowmobile or \"snow machine,\" as it is commonly referred to in Alaska.\n\nAlaska's internet and other data transport systems are provided largely through the two major telecommunications companies: GCI and Alaska Communications. GCI owns and operates what it calls the Alaska United Fiber Optic system and as of late 2011 Alaska Communications advertised that it has \"two fiber optic paths to the lower 48 and two more across Alaska. In January 2011, it was reported that a $1 billion project to connect Asia and rural Alaska was being planned, aided in part by $350 million in stimulus from the federal government.\n\nLike all other U.S. states, Alaska is governed as a republic, with three branches of government: an executive branch consisting of the Governor of Alaska and the other independently elected constitutional officers; a legislative branch consisting of the Alaska House of Representatives and Alaska Senate; and a judicial branch consisting of the Alaska Supreme Court and lower courts.\n\nThe state of Alaska employs approximately 16,000 people statewide.\n\nThe Alaska Legislature consists of a 40-member House of Representatives and a 20-member Senate. Senators serve four-year terms and House members two. The Governor of Alaska serves four-year terms. The lieutenant governor runs separately from the governor in the primaries, but during the general election, the nominee for governor and nominee for lieutenant governor run together on the same ticket.\n\nAlaska's court system has four levels: the Alaska Supreme Court, the Alaska Court of Appeals, the superior courts and the district courts. The superior and district courts are trial courts. Superior courts are courts of general jurisdiction, while district courts only hear certain types of cases, including misdemeanor criminal cases and civil cases valued up to $100,000.\n\nThe Supreme Court and the Court of Appeals are appellate courts. The Court of Appeals is required to hear appeals from certain lower-court decisions, including those regarding criminal prosecutions, juvenile delinquency, and habeas corpus. The Supreme Court hears civil appeals and may in its discretion hear criminal appeals.\n\nAlthough in its early years of statehood Alaska was a Democratic state, since the early 1970s it has been characterized as Republican-leaning. Local political communities have often worked on issues related to land use development, fishing, tourism, and individual rights. Alaska Natives, while organized in and around their communities, have been active within the Native corporations. These have been given ownership over large tracts of land, which require stewardship.\n\nAlaska was formerly the only state in which possession of one ounce or less of marijuana in one's home was completely legal under state law, though the federal law remains in force.\n\nThe state has an independence movement favoring a vote on secession from the United States, with the Alaskan Independence Party.\n\nSix Republicans and four Democrats have served as governor of Alaska. In addition, Republican Governor Wally Hickel was elected to the office for a second term in 1990 after leaving the Republican party and briefly joining the Alaskan Independence Party ticket just long enough to be reelected. He officially rejoined the Republican party in 1994.\n\nAlaska's voter initiative making marijuana legal took effect on February 24, 2015, placing Alaska alongside Colorado and Washington as the first three U.S. states where recreational marijuana is legal. The new law means people over age 21 can consume small amounts of pot – if they can find it. There is a rather lengthy and involved application process, per Alaska Measure 2 (2014). The first legal marijuana store opened in Valdez in October 2016.\n\nTo finance state government operations, Alaska depends primarily on petroleum revenues and federal subsidies. This allows it to have the lowest individual tax burden in the United States. It is one of five states with no state sales tax, one of seven states that do not levy an individual income tax, and one of the two states that has neither. The Department of Revenue Tax Division reports regularly on the state's revenue sources. The Department also issues an annual summary of its operations, including new state laws that directly affect the tax division.\n\nWhile Alaska has no state sales tax, 89 municipalities collect a local sales tax, from 1.0–7.5%, typically 3–5%. Other local taxes levied include raw fish taxes, hotel, motel, and bed-and-breakfast 'bed' taxes, severance taxes, liquor and tobacco taxes, gaming (pull tabs) taxes, tire taxes and fuel transfer taxes. A part of the revenue collected from certain state taxes and license fees (such as petroleum, aviation motor fuel, telephone cooperative) is shared with municipalities in Alaska.\n\nFairbanks has one of the highest property taxes in the state as no sales or income taxes are assessed in the Fairbanks North Star Borough (FNSB). A sales tax for the FNSB has been voted on many times, but has yet to be approved, leading lawmakers to increase taxes dramatically on goods such as liquor and tobacco.\n\nIn 2014 the Tax Foundation ranked Alaska as having the fourth most \"business friendly\" tax policy, behind only Wyoming, South Dakota, and Nevada.\n\nAlaska regularly supports Republicans in presidential elections and has done so since statehood. Republicans have won the state's electoral college votes in all but one election that it has participated in (1964). No state has voted for a Democratic presidential candidate fewer times. Alaska was carried by Democratic nominee Lyndon B. Johnson during his landslide election in 1964, while the 1960 and 1968 elections were close. Since 1972, however, Republicans have carried the state by large margins. In 2008, Republican John McCain defeated Democrat Barack Obama in Alaska, 59.49% to 37.83%. McCain's running mate was Sarah Palin, the state's governor and the first Alaskan on a major party ticket. Obama lost Alaska again in 2012, but he captured 40% of the state's vote in that election, making him the first Democrat to do so since 1968.\n\nThe Alaska Bush, central Juneau, midtown and downtown Anchorage, and the areas surrounding the University of Alaska Fairbanks campus and Ester have been strongholds of the Democratic Party. The Matanuska-Susitna Borough, the majority of Fairbanks (including North Pole and the military base), and South Anchorage typically have the strongest Republican showing. , well over half of all registered voters have chosen \"Non-Partisan\" or \"Undeclared\" as their affiliation, despite recent attempts to close primaries to unaffiliated voters.\n\nBecause of its population relative to other U.S. states, Alaska has only one member in the U.S. House of Representatives. This seat is held by Republican Don Young, who was re-elected to his 21st consecutive term in 2012. Alaska's at-large congressional district is one of the largest parliamentary constituencies in the world.\n\nIn 2008, Governor Sarah Palin became the first Republican woman to run on a national ticket when she became John McCain's running mate. She continued to be a prominent national figure even after resigning from the governor's job in July 2009.\n\nAlaska's United States Senators belong to Class 2 and Class 3. In 2008, Democrat Mark Begich, mayor of Anchorage, defeated long-time Republican senator Ted Stevens. Stevens had been convicted on seven felony counts of failing to report gifts on Senate financial discloser forms one week before the election. The conviction was set aside in April 2009 after evidence of prosecutorial misconduct emerged.\n\nRepublican Frank Murkowski held the state's other senatorial position. After being elected governor in 2002, he resigned from the Senate and appointed his daughter, State Representative Lisa Murkowski as his successor. She won full six-year terms in 2004 and 2010.\n\nAlaska is not divided into counties, as most of the other U.S. states, but it is divided into \"boroughs\". Many of the more densely populated parts of the state are part of Alaska's 16 boroughs, which function somewhat similarly to counties in other states. However, unlike county-equivalents in the other 49 states, the boroughs do not cover the entire land area of the state. The area not part of any borough is referred to as the Unorganized Borough.\n\nThe Unorganized Borough has no government of its own, but the U.S. Census Bureau in cooperation with the state divided the Unorganized Borough into 11 census areas solely for the purposes of statistical analysis and presentation. A \"recording district\" is a mechanism for administration of the public record in Alaska. The state is divided into 34 recording districts which are centrally administered under a State Recorder. All recording districts use the same acceptance criteria, fee schedule, etc., for accepting documents into the public record.\n\nWhereas many U.S. states use a three-tiered system of decentralization—state/county/township—most of Alaska uses only two tiers—state/borough. Owing to the low population density, most of the land is located in the Unorganized Borough. As the name implies, it has no intermediate borough government but is administered directly by the state government. In 2000, 57.71% of Alaska's area has this status, with 13.05% of the population.\n\nAnchorage merged the city government with the Greater Anchorage Area Borough in 1975 to form the Municipality of Anchorage, containing the city proper and the communities of Eagle River, Chugiak, Peters Creek, Girdwood, Bird, and Indian. Fairbanks has a separate borough (the Fairbanks North Star Borough) and municipality (the City of Fairbanks).\n\nThe state's most populous city is Anchorage, home to 278,700 people in 2006, 225,744 of whom live in the urbanized area. The richest location in Alaska by per capita income is Halibut Cove ($89,895). Yakutat City, Sitka, Juneau, and Anchorage are the four largest cities in the U.S. by area.\n\nAs reflected in the 2010 United States Census, Alaska has a total of 355 incorporated cities and census-designated places (CDPs). The tally of cities includes four unified municipalities, essentially the equivalent of a consolidated city–county. The majority of these communities are located in the rural expanse of Alaska known as \"The Bush\" and are unconnected to the contiguous North American road network. The table at the bottom of this section lists the 100 largest cities and census-designated places in Alaska, in population order.\n\nOf Alaska's 2010 Census population figure of 710,231, 20,429 people, or 2.88% of the population, did not live in an incorporated city or census-designated place. Approximately three-quarters of that figure were people who live in urban and suburban neighborhoods on the outskirts of the city limits of Ketchikan, Kodiak, Palmer and Wasilla. CDPs have not been established for these areas by the United States Census Bureau, except that seven CDPs were established for the Ketchikan-area neighborhoods in the 1980 Census (Clover Pass, Herring Cove, Ketchikan East, Mountain Point, North Tongass Highway, Pennock Island and Saxman East), but have not been used since. The remaining population was scattered throughout Alaska, both within organized boroughs and in the Unorganized Borough, in largely remote areas.\n\nThe Alaska Department of Education and Early Development administers many school districts in Alaska. In addition, the state operates a boarding school, Mt. Edgecumbe High School in Sitka, and provides partial funding for other boarding schools, including Nenana Student Living Center in Nenana and The Galena Interior Learning Academy in Galena.\n\nThere are more than a dozen colleges and universities in Alaska. Accredited universities in Alaska include the University of Alaska Anchorage, University of Alaska Fairbanks, University of Alaska Southeast, and Alaska Pacific University. Alaska is the only state that has no institutions that are part of the NCAA Division I.\n\nThe Alaska Department of Labor and Workforce Development operates AVTEC, Alaska's Institute of Technology. Campuses in Seward and Anchorage offer 1 week to 11-month training programs in areas as diverse as Information Technology, Welding, Nursing, and Mechanics.\n\nAlaska has had a problem with a \"brain drain\". Many of its young people, including most of the highest academic achievers, leave the state after high school graduation and do not return. , Alaska did not have a law school or medical school. The University of Alaska has attempted to combat this by offering partial four-year scholarships to the top 10% of Alaska high school graduates, via the Alaska Scholars Program.\n\nThe Alaska State Troopers are Alaska's statewide police force. They have a long and storied history, but were not an official organization until 1941. Before the force was officially organized, law enforcement in Alaska was handled by various federal agencies. Larger towns usually have their own local police and some villages rely on \"Public Safety Officers\" who have police training but do not carry firearms. In much of the state, the troopers serve as the only police force available. In addition to enforcing traffic and criminal law, wildlife Troopers enforce hunting and fishing regulations. Due to the varied terrain and wide scope of the Troopers' duties, they employ a wide variety of land, air, and water patrol vehicles.\n\nMany rural communities in Alaska are considered \"dry,\" having outlawed the importation of alcoholic beverages. Suicide rates for rural residents are higher than urban.\n\nDomestic abuse and other violent crimes are also at high levels in the state; this is in part linked to alcohol abuse. Alaska has the highest rate of sexual assault in the nation, especially in rural areas. The average age of sexually assaulted victims is 16 years old. In four out of five cases, the suspects were relatives, friends or acquaintances.\n\nSome of Alaska's popular annual events are the Iditarod Trail Sled Dog Race that starts in Anchorage and ends in Nome, World Ice Art Championships in Fairbanks, the Blueberry Festival and Alaska Hummingbird Festival in Ketchikan, the Sitka Whale Fest, and the Stikine River Garnet Fest in Wrangell. The Stikine River attracts the largest springtime concentration of American bald eagles in the world.\n\nThe Alaska Native Heritage Center celebrates the rich heritage of Alaska's 11 cultural groups. Their purpose is to encourage cross-cultural exchanges among all people and enhance self-esteem among Native people. The Alaska Native Arts Foundation promotes and markets Native art from all regions and cultures in the State, using the internet.\n\nInfluences on music in Alaska include the traditional music of Alaska Natives as well as folk music brought by later immigrants from Russia and Europe. Prominent musicians from Alaska include singer Jewel, traditional Aleut flautist Mary Youngblood, folk singer-songwriter Libby Roderick, Christian music singer-songwriter Lincoln Brewster, metal/post hardcore band 36 Crazyfists and the groups Pamyua and Portugal. The Man.\n\nThere are many established music festivals in Alaska, including the Alaska Folk Festival, the Fairbanks Summer Arts Festival, the Anchorage Folk Festival, the Athabascan Old-Time Fiddling Festival, the Sitka Jazz Festival, and the Sitka Summer Music Festival. The most prominent orchestra in Alaska is the Anchorage Symphony Orchestra, though the Fairbanks Symphony Orchestra and Juneau Symphony are also notable. The Anchorage Opera is currently the state's only professional opera company, though there are several volunteer and semi-professional organizations in the state as well.\n\nThe official state song of Alaska is \"Alaska's Flag\", which was adopted in 1955; it celebrates the flag of Alaska.\n\nAlaska's first independent picture entirely made in Alaska was \"The Chechahcos\", produced by Alaskan businessman Austin E. Lathrop and filmed in and around Anchorage. Released in 1924 by the Alaska Moving Picture Corporation, it was the only film the company made.\n\nOne of the most prominent movies filmed in Alaska is MGM's \"Eskimo/Mala The Magnificent\", starring Alaska Native Ray Mala. In 1932 an expedition set out from MGM's studios in Hollywood to Alaska to film what was then billed as \"The Biggest Picture Ever Made.\" Upon arriving in Alaska, they set up \"Camp Hollywood\" in Northwest Alaska, where they lived during the duration of the filming. Louis B. Mayer spared no expense in spite of the remote location, going so far as to hire the chef from the Hotel Roosevelt in Hollywood to prepare meals.\n\nWhen \"Eskimo\" premiered at the Astor Theatre in New York City, the studio received the largest amount of feedback in its history to that point. \"Eskimo\" was critically acclaimed and released worldwide; as a result, Mala became an international movie star. \"Eskimo\" won the first Oscar for Best Film Editing at the Academy Awards, and showcased and preserved aspects of Inupiat culture on film.\n\nThe 1983 Disney movie \"Never Cry Wolf\" was at least partially shot in Alaska. The 1991 film \"White Fang\", based on Jack London's novel and starring Ethan Hawke, was filmed in and around Haines. Steven Seagal's 1994 \"On Deadly Ground\", starring Michael Caine, was filmed in part at the Worthington Glacier near Valdez. The 1999 John Sayles film \"Limbo\", starring David Strathairn, Mary Elizabeth Mastrantonio, and Kris Kristofferson, was filmed in Juneau.\n\nThe psychological thriller \"Insomnia\", starring Al Pacino and Robin Williams, was shot in Canada, but was set in Alaska. The 2007 film directed by Sean Penn, \"Into The Wild\", was partially filmed and set in Alaska. The film, which is based on the novel of the same name, follows the adventures of Christopher McCandless, who died in a remote abandoned bus along the Stampede Trail west of Healy in 1992.\n\nMany films and television shows set in Alaska are not filmed there; for example, \"Northern Exposure\", set in the fictional town of Cicely, Alaska, was filmed in Roslyn, Washington. The 2007 horror feature \"30 Days of Night\" is set in Barrow, but was filmed in New Zealand.\n\nMany reality television shows are filmed in Alaska. In 2011 the \"Anchorage Daily News\" found ten set in the state.\n\n\n\nUS federal government\n\nAlaska state government\n"}
{"id": "627", "url": "https://en.wikipedia.org/wiki?curid=627", "title": "Agriculture", "text": "Agriculture\n\nAgriculture or farming is the cultivation and breeding of animals, plants and fungi for food, fiber, biofuel, medicinal plants and other products used to sustain and enhance human life. Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that nurtured the development of civilization. The study of agriculture is known as agricultural science. The history of agriculture dates back thousands of years, and its development has been driven and defined by greatly different climates, cultures, and technologies. Industrial agriculture based on large-scale monoculture farming has become the dominant agricultural methodology.\n\nModern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have in many cases sharply increased yields from cultivation, but at the same time have caused widespread ecological damage and negative human health effects. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and the health effects of the antibiotics, growth hormones, and other chemicals commonly used in industrial meat production. Genetically modified organisms are an increasing component of agriculture, although they are banned in several countries. Agricultural food production and water management are increasingly becoming global issues that are fostering debate on a number of fronts. Significant degradation of land and water resources, including the depletion of aquifers, has been observed in recent decades, and the effects of global warming on agriculture and of agriculture on global warming are still not fully understood.\n\nThe major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials. Specific foods include cereals (grains), vegetables, fruits, oils, meats and spices. Fibers include cotton, wool, hemp, silk and flax. Raw materials include lumber and bamboo. Other useful materials are also produced by plants, such as resins, dyes, drugs, perfumes, biofuels and ornamental products such as cut flowers and nursery plants. Over one third of the world's workers are employed in agriculture, second only to the service sector, although the percentages of agricultural workers in developed countries has decreased significantly over the past several centuries.\n\nThe word \"agriculture\" is a late Middle English adaptation of Latin \"agricultūra\", from \"ager\", \"field\", and \"cultūra\", \"cultivation\" or \"growing\". Agriculture usually refers to human activities, although it is also observed in certain species of ant, termite and ambrosia beetle. To practice agriculture means to use natural resources to \"produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services.\" This definition includes arable farming or agronomy, and horticulture, all terms for the growing of plants, animal husbandry and forestry. A distinction is sometimes made between forestry and agriculture, based on the former's longer management rotations, extensive versus intensive management practices and development mainly by nature, rather than by man. Even then, it is acknowledged that there is a large amount of knowledge transfer and overlap between silviculture (the management of forests) and agriculture. In traditional farming, the two are often combined even on small landholdings, leading to the term agroforestry.\n\nAgriculture began independently in different parts of the globe, and included a diverse range of taxa. At least 11 separate regions of the Old and New World were involved as independent centers of origin. Wild grains were collected and eaten from at least 105,000 years ago. Pigs were domesticated in Mesopotamia around 15,000 years ago. Rice was domesticated in China between 13,500 and 8,200 years ago, followed by mung, soy and azuki beans. Sheep were domesticated in Mesopotamia between 13,000 and 11,000 years ago. From around 11,500 years ago, the eight Neolithic founder crops, emmer and einkorn wheat, hulled barley, peas, lentils, bitter vetch, chick peas and flax were cultivated in the Levant. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan some 10,500 years ago. In the Andes of South America, the potato was domesticated between 10,000 and 7,000 years ago, along with beans, coca, llamas, alpacas, and guinea pigs. Sugarcane and some root vegetables were domesticated in New Guinea around 9,000 years ago. Sorghum was domesticated in the Sahel region of Africa by 7,000 years ago. Cotton was domesticated in Peru by 5,600 years ago, and was independently domesticated in Eurasia at an unknown time. In Mesoamerica, wild teosinte was domesticated to maize by 6,000 years ago.\n\nIn the Middle Ages, both in the Islamic world and in Europe, agriculture was transformed with improved techniques and the diffusion of crop plants, including the introduction of sugar, rice, cotton and fruit trees such as the orange to Europe by way of Al-Andalus. After 1492, the Columbian exchange brought New World crops such as maize, potatoes, sweet potatoes and manioc to Europe, and Old World crops such as wheat, barley, rice and turnips, and livestock including horses, cattle, sheep and goats to the Americas.\nIrrigation, crop rotation, and fertilizers were introduced soon after the Neolithic Revolution and developed much further in the past 200 years, starting with the British Agricultural Revolution. Since 1900, agriculture in the developed nations, and to a lesser extent in the developing world, has seen large rises in productivity as human labor has been replaced by mechanization, and assisted by synthetic fertilizers, pesticides, and selective breeding. The Haber-Bosch method allowed the synthesis of ammonium nitrate fertilizer on an industrial scale, greatly increasing crop yields. Modern agriculture has raised political issues including water pollution, biofuels, genetically modified organisms, tariffs and farm subsidies, leading to alternative approaches such as the organic movement and regenerative agriculture.\n\nCivilization was the product of the Agricultural Neolithic Revolution; as H. G. Wells put it, \"civilization was the agricultural surplus.\" In the course of history, civilization coincided in space with fertile areas such as The Fertile Crescent, and states formed mainly in circumscribed agricultural lands. The Great Wall of China and the Roman empire's \"limes\" (borders) demarcated the same northern frontier of cereal agriculture. This cereal belt fed the civilizations formed in the Axial Age and connected by the Silk Road.\n\nAncient Egyptians, whose agriculture depended exclusively on the Nile, deified the river, worshipped, and exalted it in a great hymn. The Chinese imperial court issued numerous edicts, stating: \"Agriculture is the foundation of this Empire.\" Egyptian, Mesopotamian, Chinese, and Inca Emperors themselves plowed ceremonial fields in order to show personal example to everyone.\n\nAncient strategists, Chinese Guan Zhong and Shang Yang and Indian Kautilya, drew doctrines linking agriculture with military power. Agriculture defined the limits on how large and for how long an army could be mobilized. Shang Yang called agriculture and war the \"One\". In the vast human pantheon of agricultural deities there are several deities who combined the functions of agriculture and war.\n\nAs the Neolithic Agricultural Revolution produced civilization, the modern Agricultural Revolution, begun in Britain (British Agricultural Revolution), made possible the industrial civilization. The first precondition for industry was greater yields by less manpower, resulting in greater percentage of manpower available for non-agricultural sectors.\n\nPastoralism involves managing domesticated animals. In nomadic pastoralism, herds of livestock are moved from place to place in search of pasture, fodder, and water. This type of farming is practised in arid and semi-arid regions of Sahara, Central Asia and some parts of India.\n\nIn shifting cultivation, a small area of a forest is cleared by cutting down all the trees and the area is burned. The land is then used for growing crops for several years. When the soil becomes less fertile, the area is then abandoned. Another patch of land is selected and the process is repeated. This type of farming is practiced mainly in areas with abundant rainfall where the forest regenerates quickly. This practice is used in Northeast India, Southeast Asia, and the Amazon Basin.\n\nSubsistence farming is practiced to satisfy family or local needs alone, with little left over for transport elsewhere. It is intensively practiced in Monsoon Asia and South-East Asia.\n\nIn intensive farming, the crops are cultivated for commercial purpose i.e., for selling. The main motive of the farmer is to make profit, with a low fallow ratio and a high use of inputs. This type of farming is mainly practiced in highly developed countries.\n\nIn the past century, agriculture has been characterized by increased productivity, the substitution of synthetic fertilizers and pesticides for labor, water pollution, and farm subsidies. In recent years there has been a backlash against the external environmental effects of conventional agriculture, resulting in the organic, regenerative, and sustainable agriculture movements. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding. Recent mainstream technological developments include genetically modified food.\n\nIn 2007, higher incentives for farmers to grow non-food biofuel crops combined with other factors, such as over development of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth, caused food shortages in Asia, the Middle East, Africa, and Mexico, as well as rising food prices around the globe. As of December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. Some of these shortages resulted in food riots and even deadly stampedes. The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security. They in part base this on the experience of Vietnam, which went from a food importer to large food exporter and saw a significant drop in poverty, due mainly to the development of smallholder agriculture in the country.\n\nDisease and land degradation are two of the major concerns in agriculture today. For example, an epidemic of stem rust on wheat caused by the Ug99 lineage is currently spreading across Africa and into Asia and is causing major concerns due to crop losses of 70% or more under some conditions. Approximately 40% of the world's agricultural land is seriously degraded. In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to United Nations University's Ghana-based Institute for Natural Resources in Africa.\n\nAgrarian structure is a long-term structure in the Braudelian understanding of the concept. On a larger scale the agrarian structure is more dependent on the regional, social, cultural and historical factors than on the state’s undertaken activities. Like in Poland, where despite running an intense agrarian policy for many years, the agrarian structure in 2002 has much in common with that found in 1921 soon after the partitions period.\n\nIn 2009, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States, according to the International Monetary Fund (\"see below\"). Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 1.7 times more productive than it was in 1948.\n\n, the International Labour Organization states that approximately one billion people, or over 1/3 of the available work force, are employed in the global agricultural sector. Agriculture constitutes approximately 70% of the global employment of children, and in many countries employs the largest percentage of women of any industry. The service sector only overtook the agricultural sector as the largest global employer in 2007. Between 1997 and 2007, the percentage of people employed in agriculture fell by over four percentage points, a trend that is expected to continue. The number of people employed in agriculture varies widely on a per-country basis, ranging from less than 2% in countries like the US and Canada to over 80% in many African nations. In developed countries, these figures are significantly lower than in previous centuries. During the 16th century in Europe, for example, between 55 and 75 percent of the population was engaged in agriculture, depending on the country. By the 19th century in Europe, this had dropped to between 35 and 65 percent. In the same countries today, the figure is less than 10%.\n\nAgriculture, specifically farming, remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers. Pesticides and other chemicals used in farming can also be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects. As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death. Common causes of fatal injuries among young farm workers include drowning, machinery and motor vehicle-related accidents.\n\nThe International Labour Organization considers agriculture \"one of the most hazardous of all economic sectors.\" It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs. In addition, incidences of death, injury and illness related to agricultural activities often go unreported. The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.\n\nCropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer.\n\nShifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10–20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs.\n\nFurther industrialization led to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.\n\nIn subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual farming is the dominant agricultural system.\n\nImportant categories of crops include cereals and pseudocereals, pulses (legumes), forage, and fruits and vegetables. Specific crops are cultivated in distinct growing regions throughout the world. In millions of metric tons, based on FAO estimate.\n\nAnimals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, are often used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers. Animal husbandry not only refers to the breeding and raising of animals for meat or to harvest animal products (like milk, eggs, or wool) on a continual basis, but also to the breeding and care of species for work and companionship.\n\nLivestock production systems can be defined based on feed source, as grassland-based, mixed, and landless. , 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people. Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10. Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases. Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050. Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.\n\nDuring the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity. This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.\n\nGrassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30–40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops.\n\nLandless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organisation for Economic Co-operation and Development(OECD) member countries. Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution. Industrialized countries use these operations to produce much of the global supplies of poultry and pork. Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming. Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa. Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.\n\nFarming is the practice of agriculture by specialized labor in an area primarily devoted to agricultural processes, in service of a dislocated population usually in a city.\n\nTillage is the practice of plowing soil to prepare for planting or for nutrient incorporation or for pest control. Tillage varies in intensity from conventional to no-till. It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO, and reduces the abundance and diversity of soil organisms.\n\nPest control includes the management of weeds, insects, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.\n\nNutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and mined minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.\n\nWater management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year. Agriculture represents 70% of freshwater use worldwide.\n\nAccording to a report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.\n\n\"Payment for ecosystem services (PES) can further incentivise efforts to green the agriculture sector. This is an approach that verifies values and rewards the benefits of ecosystem services provided by green agricultural practices.\" \"Innovative PES measures could include reforestation payments made by cities to upstream communities in rural areas of shared watersheds for improved quantities and quality of fresh water for municipal users. Ecoservice payments by farmers to upstream forest stewards for properly managing the flow of soil nutrients, and methods to monetise the carbon sequestration and emission reduction credit benefits of green agriculture practices in order to compensate farmers for their efforts to restore and build SOM and employ other practices.\"\n\nCrop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.\n\nDomestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.\n\nThe Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating \"high-yielding varieties\". For example, average yields of corn (maize) in the USA have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging).\n\nGenetically modified organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops. Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering. For some, GMO crops cause food safety and food labeling concerns. Numerous countries have placed restrictions on the production, import or use of GMO foods and crops, which have been put in place due to concerns over potential health issues, declining agricultural diversity and contamination of non-GMO crops. Currently a global treaty, the Biosafety Protocol, regulates the trade of GMOs. There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.\n\nHerbicide-resistant seed has a gene implanted into its genome that allows the plants to tolerate exposure to herbicides, including glyphosates. These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.\n\nOther GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium \"Bacillus thuringiensis\" (Bt), which produces a toxin specific to insects. These crops protect plants from damage by insects. Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.\n\nAgriculture, as implemented through the method of farming, imposes external costs upon society through pesticides, nutrient runoff, excessive water usage, loss of natural environment and assorted other problems. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare. A 2005 analysis of these costs in the USA concluded that cropland imposes approximately $5 to 16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs. Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society. In 2010, the International Resource Panel of the United Nations Environment Programme published a report assessing the environmental impacts of consumption and production. The study found that agriculture and food consumption are two of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions. The 2011 UNEP Green Economy report states that \"[a]gricultural operations, excluding land use changes, produce approximately 13 per cent of anthropogenic global GHG emissions. This includes GHGs emitted by the use of inorganic fertilisers agro-chemical pesticides and herbicides; (GHG emissions resulting from production of these inputs are included in industrial emissions); and fossil fuel-energy inputs. \"On average we find that the total amount of fresh residues from agricultural and forestry production for second- generation biofuel production amounts to 3.8 billion tonnes per year between 2011 and 2050 (with an average annual growth rate of 11 per cent throughout the period analysed, accounting for higher growth during early years, 48 per cent for 2011–2020 and an average 2 per cent annual expansion after 2020).\"\n\nA senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said \"Livestock are one of the most significant contributors to today's most serious environmental problems\". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO equivalents. By comparison, all transportation emits 13.5% of the CO. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO) and 37% of all human-induced methane (which is 23 times as warming as CO.) It also generates 64% of the ammonia emission. Livestock expansion is cited as a key factor driving deforestation; in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity. Furthermore, the UNEP states that \"methane emissions from global livestock are projected to increase by 60 per cent by 2030 under current practices and consumption patterns.\"\n\nLand transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity. Estimates of the amount of land transformed by humans vary from 39 to 50%. Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented. The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land. Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).\n\nEutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems.\n\nAgriculture accounts for 70 percent of withdrawals of freshwater resources. Agriculture is a major draw on water from aquifers, and currently draws from those underground water sources at an unsustainable rate. It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia. Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources. Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.\n\nPesticide use has increased since 1950 to 2.5million short tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that 3million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the \"pesticide treadmill\" in which pest resistance warrants the development of a new pesticide.\n\nAn alternative argument is that the way to \"save the environment\" and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides simply replace good agronomic practices such as crop rotation. The UNEP introduces the Push–pull agricultural pest management technique which involves intercropping that uses plant aromas to repel or push away pests while pulling in or attracting the right insects. \"The implementation of push-pull in eastern Africa has significantly increased maize yields and the combined cultivation of N-fixing forage crops has enriched the soil and has also provided farmers with feed for livestock. With increased livestock operations, the farmers are able to produce meat, milk and other dairy products and they use the manure as organic fertiliser that returns nutrients to the fields.\"\n\nClimate change has the potential to affect agriculture through changes in temperature, rainfall (timing and quantity), CO, solar radiation and the interaction of these elements. Extreme events, such as droughts and floods, are forecast to increase as climate change takes hold. Agriculture is among sectors most vulnerable to the impacts of climate change; water supply for example, will be critical to sustain agricultural production and provide the increase in food output required to sustain the world's growing population. Fluctuations in the flow of rivers are likely to increase in the twenty-first century. Based on the experience of countries in the Nile river basin (Ethiopia, Kenya and Sudan) and other developing countries, depletion of water resources during seasons crucial for agriculture can lead to a decline in yield by up to 50%. Transformational approaches will be needed to manage natural resources in the future. For example, policies, practices and tools promoting climate-smart agriculture will be important, as will better use of scientific information on climate for assessing risks and vulnerability. Planners and policy-makers will need to help create suitable policies that encourage funding for such agricultural transformation.\n\nAgriculture in its many forms can both mitigate or worsen global warming. Some of the increase in CO in the atmosphere comes from the decomposition of organic matter in the soil, and much of the methane emitted into the atmosphere is caused by the decomposition of organic matter in wet soils such as rice paddy fields, as well as the normal digestive activities of farm animals. Further, wet or anaerobic soils also lose nitrogen through denitrification, releasing the greenhouse gases nitric oxide and nitrous oxide. Changes in management can reduce the release of these greenhouse gases, and soil can further be used to sequester some of the CO in the atmosphere. Informed by the UNEP, \"[a]griculture also produces about 58 per cent of global nitrous oxide emissions and about 47 per cent of global methane emissions. Cattle and rice farms release methane, fertilized fields release nitrous oxide, and the cutting down of rainforests to grow crops or raise livestock releases carbon dioxide. Both of these gases have a far greater global warming potential per tonne than CO2 (298 times and 25 times respectively).\"\n\nThere are several factors within the field of agriculture that contribute to the large amount of CO2 emissions. The diversity of the sources ranges from the production of farming tools to the transport of harvested produce. Approximately 8% of the national carbon footprint is due to agricultural sources. Of that, 75% is of the carbon emissions released from the production of crop assisting chemicals. Factories producing insecticides, herbicides, fungicides, and fertilizers are a major culprit of the greenhouse gas. Productivity on the farm itself and the use of machinery is another source of the carbon emission. Almost all the industrial machines used in modern farming are powered by fossil fuels. These instruments are burning fossil fuels from the beginning of the process to the end. Tractors are the root of this source. The tractor is going to burn fuel and release CO2 just to run. The amount of emissions from the machinery increase with the attachment of different units and need for more power. During the soil preparation stage tillers and plows will be used to disrupt the soil. During growth watering pumps and sprayers are used to keep the crops hydrated. And when the crops are ready for picking a forage or combine harvester is used. These types of machinery all require additional energy which leads to increased carbon dioxide emissions from the basic tractors. The final major contribution to CO2 emissions in agriculture is in the final transport of produce. Local farming suffered a decline over the past century due to large amounts of farm subsidies. The majority of crops are shipped hundreds of miles to various processing plants before ending up in the grocery store. These shipments are made using fossil fuel burning modes of transportation. Inevitably these transport adds to carbon dioxide emissions.\n\nSome major organizations are hailing farming within agroecosystems as the way forward for mainstream agriculture. Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. According to a report by the International Water Management Institute and UNEP, there is not enough water to continue farming using current practices; therefore how critical water, land, and ecosystem resources are used to boost crop yields must be reconsidered. The report suggested assigning value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests. Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for\nmore productive farmland, or the preservation of a wetland system that limits fishing rights.\n\nTechnological advancements help provide farmers with tools and resources to make farming more sustainable. New technologies have given rise to innovations like conservation tillage, a farming process which helps prevent land loss to erosion, water pollution and enhances carbon sequestration.\n\nAccording to a report by the International Food Policy Research Institute (IFPRI), agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, IFPRI found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.\n\nAgricultural economics refers to economics as it relates to the \"production, distribution and consumption of [agricultural] goods and services\". Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century. Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States to the European feudal system of manorialism. In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.\n\nNational government policies can significantly change the economic marketplace for agricultural products, in the form of taxation, subsidies, tariffs and other measures. Since at least the 1960s, a combination of import/export restrictions, exchange rate policies and subsidies have affected farmers in both the developing and developed world. In the 1980s, it was clear that non-subsidized farmers in developing countries were experiencing adverse effects from national policies that created artificially low global prices for farm products. Between the mid-1980s and the early 2000s, several international agreements were put into place that limited agricultural tariffs, subsidies and other trade restrictions.\n\nHowever, , there was still a significant amount of policy-driven distortion in global agricultural product prices. The three agricultural products with the greatest amount of trade distortion were sugar, milk and rice, mainly due to taxation. Among the oilseeds, sesame had the greatest amount of taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products. Since the 1980s, policy-driven distortions have seen a greater decrease among livestock products than crops during the worldwide reforms in agricultural policy. Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers. Unprocessed commodities (i.e. corn, soybeans, cows) are generally graded to indicate quality. The quality affects the price the producer receives. Commodities are generally reported by production quantities, such as volume, number or weight.\n\nSince the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between the 1960–65 measuring cycle and the cycle from 1986 to 1990, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area) as world population doubled. Modern agriculture's heavy reliance on petrochemicals and mechanization has raised concerns that oil shortages could increase costs and reduce agricultural output, causing food shortages.\n\nModern or industrialized agriculture is dependent on fossil fuels in two fundamental ways: 1. direct consumption on the farm and 2. indirect consumption to manufacture inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery; and use of gasoline, liquid propane, and electricity to power dryers, pumps, lights, heaters, and coolers. American farms directly consumed about 1.2 exajoules (1.1 quadrillion BTU) in 2002, or just over 1% of the nation's total energy.\n\nIndirect consumption is mainly oil and natural gas used to manufacture fertilizers and pesticides, which accounted for 0.6 exajoules (0.6 quadrillion BTU) in 2002. The natural gas and coal consumed by the production of nitrogen fertilizer can account for over half of the agricultural energy usage. China utilizes mostly coal in the production of nitrogen fertilizer, while most of Europe uses large amounts of natural gas and small amounts of coal. According to a 2010 report published by The Royal Society, agriculture is increasingly dependent on the direct and indirect input of fossil fuels. Overall, the fuels used in agriculture vary based on several factors, including crop, production system and location. The energy used to manufacture farm machinery is also a form of indirect agricultural energy consumption. Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has gradually declined over the past 30 years. Food systems encompass not just agricultural production, but also off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for less than one-fifth of food system energy use in the US.\n\nIn the event of a petroleum shortage (see peak oil for global concerns), organic agriculture can be more attractive than conventional practices that use petroleum-based pesticides, herbicides, or fertilizers. Some studies using modern organic-farming methods have reported yields equal to or higher than those available from conventional farming. In the aftermath of the fall of the Soviet Union, with shortages of conventional petroleum-based inputs, Cuba made use of mostly organic practices, including biopesticides, plant-based pesticides and sustainable cropping practices, to feed its populace. However, organic farming may be more labor-intensive and would require a shift of the workforce from urban to rural areas. The reconditioning of soil to restore organic matter lost during the use of monoculture agriculture techniques is important to provide a reservoir of plant-available nutrients, to maintain texture, and to minimize erosion.\n\nIt has been suggested that rural communities might obtain fuel from the biochar and synfuel process, which uses agricultural \"waste\" to provide charcoal fertilizer, some fuel \"and\" food, instead of the normal food vs. fuel debate. As the synfuel would be used on-site, the process would be more efficient and might just provide enough fuel for a new organic-agriculture fusion.\n\nIt has been suggested that some transgenic plants may some day be developed which would allow for maintaining or increasing yields while requiring fewer fossil-fuel-derived inputs than conventional crops. The possibility of success of these programs is questioned by ecologists and economists concerned with unsustainable GMO practices such as terminator seeds. While there has been some research on sustainability using GMO crops, at least one prominent multi-year attempt by Monsanto Company has been unsuccessful, though during the same period traditional breeding techniques yielded a more sustainable variety of the same crop.\n\nAgricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products. Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets. Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries). Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation. Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.\n\nThere are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups. Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions. Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities. The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements. Dr. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment. For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.\n\n\n\n"}
{"id": "628", "url": "https://en.wikipedia.org/wiki?curid=628", "title": "Aldous Huxley", "text": "Aldous Huxley\n\nAldous Leonard Huxley (; 26 July 1894 – 22 November 1963) was an English writer, novelist, philosopher, and prominent member of the Huxley family. He graduated from Balliol College at the University of Oxford with a first-class honours in English literature.\n\nThe author of nearly fifty books, he was best known for his novels including \"Brave New World\", set in a dystopian future; for non-fiction works, such as \"The Doors of Perception\", which recalls experiences when taking a psychedelic drug; and a wide-ranging output of essays. Early in his career Huxley edited the magazine \"Oxford Poetry\" and published short stories and poetry. Mid career and later, he published travel writing, film stories, and scripts. He spent the later part of his life in the United States, living in Los Angeles from 1937 until his death. In 1962, a year before his death, he was elected Companion of Literature by the Royal Society of Literature.\n\nHuxley was a humanist, pacifist, and satirist. He later became interested in spiritual subjects such as parapsychology and philosophical mysticism, in particular universalism. By the end of his life, Huxley was widely acknowledged as one of the pre-eminent intellectuals of his time. He was nominated for the Nobel Prize in Literature in seven different years.\n\nHuxley was born in Godalming, Surrey, England, in 1894. He was the third son of the writer and schoolmaster Leonard Huxley, who edited \"Cornhill Magazine\", and his first wife, Julia Arnold, who founded Prior's Field School. Julia was the niece of poet and critic Matthew Arnold and the sister of Mrs. Humphrey Ward. Aldous was the grandson of Thomas Henry Huxley, the zoologist, agnostic, and controversialist (\"Darwin's Bulldog\"). His brother Julian Huxley and half-brother Andrew Huxley also became outstanding biologists. Aldous had another brother, Noel Trevelyan Huxley (1891–1914), who committed suicide after a period of clinical depression.\n\nAs a child, Huxley's nickname was \"Ogie\", short for \"Ogre\". He was described by his brother, Julian, as someone who frequently \"[contemplated] the strangeness of things\". According to his cousin and contemporary, Gervas Huxley, he had an early interest in drawing.\n\nHuxley's education began in his father's well-equipped botanical laboratory, after which he enrolled at Hillside School near Godalming. He was taught there by his own mother for several years until she became terminally ill. After Hillside, he went on to Eton College. His mother died in 1908 when he was 14. In 1911 he contracted the eye disease (keratitis punctata) which \"left [him] practically blind for two to three years\". This \"ended his early dreams of becoming a doctor.\" In October 1913, Huxley went up to Balliol College, Oxford, where he read English Literature. In January 1916, he volunteered to join the British Army in the Great War, but was rejected on health grounds, being half-blind in one eye. His eyesight later partly recovered. In 1916 he edited \"Oxford Poetry\" and in June of that year graduated BA with First Class honours. His brother Julian wrote:\n\nFollowing his years at Balliol, Huxley, being financially indebted to his father, decided to find employment. From April to July 1917, he was in charge of ordering supplies at the Air Ministry . He taught French for a year at Eton, where Eric Blair (who was to take the pen name George Orwell) and Steven Runciman were among his pupils. He was mainly remembered as being an incompetent schoolmaster unable to keep order in class. Nevertheless, Blair and others spoke highly of his brilliant command of language.\n\nSignificantly, Huxley also worked for a time during the 1920s at Brunner and Mond, a high-tech chemical plant in Billingham in County Durham, northeast England. According to the introduction to the latest edition of his great science fiction novel \"Brave New World\" (1932), the experience he had there of \"an ordered universe in a world of planless incoherence\" was an important source for the novel.\n\nHuxley completed his first (unpublished) novel at the age of 17 and began writing seriously in his early 20s, establishing himself as a successful writer and social satirist. His first published novels were social satires, \"Crome Yellow\" (1921), \"Antic Hay\" (1923), \"Those Barren Leaves\" (1925), and \"Point Counter Point\" (1928). \"Brave New World\" was Huxley's fifth novel and first dystopian work. In the 1920s he was also a contributor to \"Vanity Fair\" and British \"Vogue\" magazines.\n\nDuring World War I, Huxley spent much of his time at Garsington Manor near Oxford, home of Lady Ottoline Morrell, working as a farm labourer. There he met several Bloomsbury figures, including Bertrand Russell, Alfred North Whitehead, and Clive Bell. Later, in \"Crome Yellow\" (1921) he caricatured the Garsington lifestyle. Jobs were very scarce, but in 1919 John Middleton Murry was reorganising the \"Athenaeum\" and invited Huxley to join the staff. He accepted immediately, and quickly married the Belgian refugee Maria Nys, also at Garsington. They lived with their young son in Italy part of the time during the 1920s, where Huxley would visit his friend D. H. Lawrence. Following Lawrence's death in 1930, Huxley edited Lawrence's letters (1932).\n\nWorks of this period included important novels on the dehumanising aspects of scientific progress, most famously \"Brave New World\", and on pacifist themes (for example, \"Eyeless in Gaza\"). In \"Brave New World\", set in a dystopian London, Huxley portrays a society operating on the principles of mass production and Pavlovian conditioning. Huxley was strongly influenced by F. Matthias Alexander and included him as a character in \"Eyeless in Gaza\".\n\nStarting from this period, Huxley began to write and edit non-fiction works on pacifist issues, including \"Ends and Means\", \"An Encyclopedia of Pacifism\", and \"Pacifism and Philosophy\", and was an active member of the Peace Pledge Union.\n\nIn 1937, Huxley moved to Hollywood with his wife Maria, son Matthew Huxley, and friend Gerald Heard. He lived in the US, mainly in southern California, until his death, but also for a time in Taos, New Mexico, where he wrote \"Ends and Means\" (published in 1937). The book contains tracts on war, religion, nationalism and ethics.\n\nHeard introduced Huxley to Vedanta (Upanishad-centered philosophy), meditation, and vegetarianism through the principle of ahimsa. In 1938, Huxley befriended Jiddu Krishnamurti, whose teachings he greatly admired. He also became a Vedantist in the circle of Hindu Swami Prabhavananda, and introduced Christopher Isherwood to this circle. Not long after, Huxley wrote his book on widely held spiritual values and ideas, \"The Perennial Philosophy\", which discussed the teachings of renowned mystics of the world. Huxley's book affirmed a sensibility that insists there are realities beyond the generally accepted \"five senses\" and that there is genuine meaning for humans beyond both sensual satisfactions and sentimentalities.\n\nHuxley became a close friend of Remsen Bird, president of Occidental College. He spent much time at the college, which is in the Eagle Rock neighbourhood of Los Angeles. The college appears as \"Tarzana College\" in his satirical novel \"After Many a Summer\" (1939). The novel won Huxley a British literary award, the 1939 James Tait Black Memorial Prize for fiction. Huxley also incorporated Bird into the novel.\n\nDuring this period, Huxley earned a substantial income as a Hollywood screenwriter; Christopher Isherwood, in his autobiography \"My Guru and His Disciple\", states that Huxley earned more than $3,000 per week (an enormous sum in those days) as a screenwriter, and that he used much of it to transport Jewish and left-wing writer and artist refugees from Hitler's Germany to the U.S. In March 1938, his friend Anita Loos, a novelist and screenwriter, put him in touch with Metro-Goldwyn-Mayer who hired Huxley for \"Madame Curie\", which was originally to star Greta Garbo and be directed by George Cukor. (Eventually, the film was completed by MGM in 1943 with a different director and cast.) Huxley received screen credit for \"Pride and Prejudice\" (1940) and was paid for his work on a number of other films, including \"Jane Eyre\" (1944). Huxley was commissioned by Walt Disney in 1945 to write a script based on \"Alice's Adventures in Wonderland\" and the biography of the story's author, Lewis Carroll. The script was not used, however.\n\nHuxley wrote an introduction to the posthumous publication of J. D. Unwin's 1940 book \"Hopousia or The Sexual and Economic Foundations of a New Society\".\n\nOn 21 October 1949, Huxley wrote to George Orwell, author of \"Nineteen Eighty-Four\", congratulating him on \"how fine and how profoundly important the book is\". In his letter to Orwell, he predicted:\nHuxley had deeply felt apprehensions about the future the developed world might make for itself. From these, he made some warnings in his writings and talks. In a 1958 televised interview conducted by journalist Mike Wallace, Huxley outlined several major concerns: the difficulties and dangers of world overpopulation; the tendency toward distinctly hierarchical social organisation; the crucial importance of evaluating the use of technology in mass societies susceptible to wily persuasion; the tendency to promote modern politicians to a naive public as well-marketed commodities.\n\nIn 1953, Huxley and Maria applied for United States citizenship and presented themselves for examination. When Huxley refused to bear arms for the US and would not state that his objections were based on religious ideals, the only excuse allowed under the McCarran Act, the judge had to adjourn the proceedings. He withdrew his application. Nevertheless, he remained in the US. In 1959 Huxley turned down an offer of a Knight Bachelor by the Macmillan government without putting forward a reason; his brother Julian had been knighted in 1958, while another brother Andrew would be knighted in 1974.\n\nBeginning in 1939 and continuing until his death in 1963, Huxley had an extensive association with the Vedanta Society of Southern California, founded and headed by Swami Prabhavananda. Together with Gerald Heard, Christopher Isherwood, and other followers he was initiated by the Swami and was taught meditation and spiritual practices.\n\nIn 1944, Huxley wrote the introduction to the \"Bhagavad Gita: The Song of God\", translated by Swami Prabhavananda and Christopher Isherwood, which was published by The Vedanta Society of Southern California.\n\nFrom 1941 until 1960, Huxley contributed 48 articles to \"Vedanta and the West\", published by the society. He also served on the editorial board with Isherwood, Heard, and playwright John van Druten from 1951 through 1962.\n\nHuxley also occasionally lectured at the Hollywood and Santa Barbara Vedanta temples. Two of those lectures have been released on CD: \"Knowledge and Understanding\" and \"Who Are We?\" from 1955. Nonetheless, Huxley's agnosticism, together with his speculative propensity, made it difficult for him to fully embrace any form of institutionalized religion.\n\nIn the spring of 1953, Huxley had his first experience with psychedelic drugs (in this case, mescaline). Huxley had initiated a correspondence with Dr. Humphry Osmond, a British psychiatrist then employed in a Canadian institution, and eventually asked him to supply a dose of mescaline; Osmond obliged and supervised Huxley’s session in southern California. After the publication of \"The Doors of Perception\", in which he recounted this experience, Huxley and Swami Prabhavananda disagreed about the meaning and importance of the psychedelic drug experience, which may have caused the relationship to cool, but Huxley continued to write articles for the society's journal, lecture at the temple, and attend social functions.\n\nThere are differing accounts about the details of the quality of Huxley's eyesight at specific points in his life. About 1939, Huxley encountered the Bates method for better eyesight, and a teacher, Margaret Darst Corbett, who was able to teach the method to him. In 1940, Huxley relocated from Hollywood to a \"ranchito\" in the high desert hamlet of Llano, California, in northernmost Los Angeles County. Huxley then said that his sight improved dramatically with the Bates Method and the extreme and pure natural lighting of the southwestern American desert. He reported that, for the first time in more than 25 years, he was able to read without glasses and without strain. He even tried driving a car along the dirt road beside the ranch. He wrote a book about his successes with the Bates Method, \"The Art of Seeing\", which was published in 1942 (U.S.), 1943 (UK). The book contained some generally disputed theories, and its publication created a growing degree of popular controversy about Huxley's eyesight.\n\nIt was, and is, widely believed that Huxley was nearly blind since the illness in his teens, despite the partial recovery that had enabled him to study at Oxford. For example, some ten years after publication of \"The Art of Seeing\", in 1952, Bennett Cerf was present when Huxley spoke at a Hollywood banquet, wearing no glasses and apparently reading his paper from the lectern without difficulty: \"Then suddenly he faltered — and the disturbing truth became obvious. He wasn't reading his address at all. He had learned it by heart. To refresh his memory he brought the paper closer and closer to his eyes. When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him. It was an agonising moment.\"\n\nOn the other hand, Huxley's second wife, Laura Archera, would later emphasise in her biographical account, \"This Timeless Moment\": \"One of the great achievements of his life: that of having regained his sight.\" After revealing a letter she wrote to the \"Los Angeles Times\" disclaiming the label of Huxley as a \"poor fellow who can hardly see\" by Walter C. Alvarez, she tempered this: \"Although I feel it was an injustice to treat Aldous as though he were blind, it is true there were many indications of his impaired vision. For instance, although Aldous did not wear glasses, he would quite often use a magnifying lens.\" Laura Huxley proceeded to elaborate a few nuances of inconsistency peculiar to Huxley's vision. Her account, in this respect, is discernibly congruent with the following sample of Huxley's own words from \"The Art of Seeing\": \"The most characteristic fact about the functioning of the total organism, or any part of the organism, is that it is not constant, but highly variable.\" Nevertheless, the topic of Huxley's eyesight continues to endure similar, significant controversy, regardless of how trivial a subject matter it might initially appear.\n\nAmerican popular science author Steven Johnson, in his book \"Mind Wide Open\", quotes Huxley about his difficulties with visual encoding: \"I am and, for as long as I can remember, I have always been a poor visualizer. Words, even the pregnant words of poets, do not evoke pictures in my mind. No hypnagogic visions greet me on the verge of sleep. When I recall something, the memory does not present itself to me as a vividly seen event or object. By an effort of the will, I can evoke a not very vivid image of what happened yesterday afternoon ...\"\n\nHuxley married Maria Nys (10 September 1899 – 12 February 1955), a Belgian he met at Garsington, Oxfordshire, in 1919. They had one child, Matthew Huxley (19 April 1920 – 10 February 2005), who had a career as an author, anthropologist, and prominent epidemiologist. In 1955, Maria died of cancer.\n\nIn 1956, Huxley married Laura Archera (1911–2007), also an author as well as a violinist and psychotherapist. She wrote \"This Timeless Moment\", a biography of Huxley. Laura felt inspired to illuminate the story of their marriage through Mary Ann Braubach's 2010 documentary, \"Huxley on Huxley\".\n\nIn 1960, Huxley was diagnosed with laryngeal cancer and, in the years that followed, with his health deteriorating, he wrote the Utopian novel \"Island\", and gave lectures on \"Human Potentialities\" both at the University of California's San Francisco Medical Center and at the Esalen Institute. These lectures were fundamental to the beginning of the Human Potential Movement.\n\nHuxley was a close friend of Jiddu Krishnamurti and Rosalind Rajagopal and was involved in the creation of the Happy Valley School (now Besant Hill School of Happy Valley) in Ojai, California.\n\nThe most substantial collection of Huxley's few remaining papers (following the destruction of most in a fire) is at the Library of the University of California, Los Angeles. Some are also at the Stanford University Libraries.\n\nOn 9 April 1962, Huxley was informed he was elected Companion of Literature by the Royal Society of Literature, the senior literary organisation in Britain, and he accepted the title via letter on 28 April 1962. The correspondence between Huxley and the society are kept at the Cambridge University Library. The society invited Huxley to appear at a banquet and give a lecture at Somerset House, London in June 1963. Huxley wrote a draft of the speech he intended to give at the society; however, his deteriorating health meant he would not be able to attend.\n\nOn his deathbed, unable to speak due to advanced laryngeal cancer, Huxley made a written request to his wife Laura for \"LSD, 100 µg, intramuscular\". According to her account of his death in \"This Timeless Moment\", she obliged with an injection at 11:20 a.m. and a second dose an hour later; Huxley died aged 69, at 5:20 p.m. (Los Angeles time), on 22 November 1963.\n\nMedia coverage of Huxley's passing — as with that of the author C. S. Lewis – was overshadowed by the assassination of U.S. President John F. Kennedy on the same day. This coincidence served as the basis for Peter Kreeft's book \"Between Heaven and Hell: A Dialog Somewhere Beyond Death with John F. Kennedy, C. S. Lewis, & Aldous Huxley\", which imagines a conversation among the three men taking place in Purgatory following their deaths.\n\nHuxley's memorial service took place in London in December 1963 which was led by his older brother Julian, and on 27 October 1971 his ashes were interred in the family grave at the Watts Cemetery, home of the Watts Mortuary Chapel in Compton, Guildford, Surrey, England.\n\nHuxley had been a long-time friend of Russian composer Igor Stravinsky, who later dedicated his last orchestral composition to Huxley. Stravinsky began \"Variations\" in Santa Fé, New Mexico, in July 1963, and completed the composition in Hollywood on 28 October 1964. It was first performed in Chicago on 17 April 1965, by the Chicago Symphony Orchestra conducted by Robert Craft.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "630", "url": "https://en.wikipedia.org/wiki?curid=630", "title": "Ada", "text": "Ada\n\nAda may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "632", "url": "https://en.wikipedia.org/wiki?curid=632", "title": "Aberdeen (disambiguation)", "text": "Aberdeen (disambiguation)\n\nAberdeen is a city in Scotland, United Kingdom.\n\nAberdeen may also refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "633", "url": "https://en.wikipedia.org/wiki?curid=633", "title": "Algae", "text": "Algae\n\nAlgae (; singular alga ) is an informal term for a large, diverse group of photosynthetic organisms which are not necessarily closely related, and is thus polyphyletic. Included organisms range from unicellular genera, such as \"Chlorella\" and the diatoms, to multicellular forms, such as the giant kelp, a large brown alga which may grow up to 50 m in length. Most are aquatic and autotrophic and lack many of the distinct cell and tissue types, such as stomata, xylem, and phloem, which are found in land plants. The largest and most complex marine algae are called seaweeds, while the most complex freshwater forms are the Charophyta, a division of green algae which includes, for example, \"Spirogyra\" and the stoneworts.\n\nNo definition of algae is generally accepted. One definition is that algae \"have chlorophyll as their primary photosynthetic pigment and lack a sterile covering of cells around their reproductive cells\". Some authors exclude all prokaryotes and thus do not consider cyanobacteria (blue-green algae) as algae.\n\nAlgae constitute a polyphyletic group since they do not include a common ancestor, and although their plastids seem to have a single origin, from cyanobacteria, they were acquired in different ways. Green algae are examples of algae that have primary chloroplasts derived from endosymbiotic cyanobacteria. Diatoms and brown algae are examples of algae with secondary chloroplasts derived from an endosymbiotic red alga.\n\nAlgae exhibit a wide range of reproductive strategies, from simple asexual cell division to complex forms of sexual reproduction.\n\nAlgae lack the various structures that characterize land plants, such as the phyllids (leaf-like structures) of bryophytes, rhizoids in nonvascular plants, and the roots, leaves, and other organs found in tracheophytes (vascular plants). Most are phototrophic, although some are mixotrophic, deriving energy both from photosynthesis and uptake of organic carbon either by osmotrophy, myzotrophy, or phagotrophy. Some unicellular species of green algae, many golden algae, euglenids, dinoflagellates, and other algae have become heterotrophs (also called colorless or apochlorotic algae), sometimes parasitic, relying entirely on external energy sources and have limited or no photosynthetic apparatus. Some other heterotrophic organisms, such as the apicomplexans, are also derived from cells whose ancestors possessed plastids, but are not traditionally considered as algae. Algae have photosynthetic machinery ultimately derived from cyanobacteria that produce oxygen as a by-product of photosynthesis, unlike other photosynthetic bacteria such as purple and green sulfur bacteria. Fossilized filamentous algae from the Vindhya basin have been dated back to 1.6 to 1.7 billion years ago.\n\nThe singular \"alga\" is the Latin word for \"seaweed\" and retains that meaning in English. The etymology is obscure. Although some speculate that it is related to Latin \"algēre\", \"be cold\", no reason is known to associate seaweed with temperature. A more likely source is \"alliga\", \"binding, entwining\".\n\nThe Ancient Greek word for seaweed was φῦκος (\"phŷcos\"), which could mean either the seaweed (probably red algae) or a red dye derived from it. The Latinization, \"fūcus\", meant primarily the cosmetic rouge. The etymology is uncertain, but a strong candidate has long been some word related to the Biblical פוך (\"pūk\"), \"paint\" (if not that word itself), a cosmetic eye-shadow used by the ancient Egyptians and other inhabitants of the eastern Mediterranean. It could be any color: black, red, green, or blue.\n\nAccordingly, the modern study of marine and freshwater algae is called either phycology or algology, depending on whether the Greek or Latin root is used. The name \"Fucus\" appears in a number of taxa.\n\nMost algae contain chloroplasts that are similar in structure to cyanobacteria. Chloroplasts contain circular DNA like that in cyanobacteria and presumably represent reduced endosymbiotic cyanobacteria. However, the exact origin of the chloroplasts is different among separate lineages of algae, reflecting their acquisition during different endosymbiotic events. The table below describes the composition of the three major groups of algae. Their lineage relationships are shown in the figure in the upper right. Many of these groups contain some members that are no longer photosynthetic. Some retain plastids, but not chloroplasts, while others have lost plastids entirely.\n\nPhylogeny based on plastid not nucleocytoplasmic genealogy:\n\nLinnaeus, in \"Species Plantarum\" (1753), the starting point for modern botanical nomenclature, recognized 14 genera of algae, of which only four are currently considered among algae. In \"Systema Naturae\", Linnaeus described the genera \"Volvox\" and \"Corallina\", and a species of \"Acetabularia\" (as \"Madrepora\"), among the animals.\n\nIn 1768, Samuel Gottlieb Gmelin (1744–1774) published the \"Historia Fucorum\", the first work dedicated to marine algae and the first book on marine biology to use the then new binomial nomenclature of Linnaeus. It included elaborate illustrations of seaweed and marine algae on folded leaves.\n\nW.H.Harvey (1811—1866) and Lamouroux (1813) were the first to divide macroscopic algae into four divisions based on their pigmentation. This is the first use of a biochemical criterion in plant systematics. Harvey's four divisions are: red algae (Rhodospermae), brown algae (Melanospermae), green algae (Chlorospermae), and Diatomaceae.\n\nAt this time, microscopic algae were discovered and reported by a different group of workers (e.g., O. F. Müller and Ehrenberg) studying the Infusoria (microscopic organisms). Unlike macroalgae, which were clearly viewed as plants, microalgae were frequently considered animals because they are often motile. Even the nonmotile (coccoid) microalgae were sometimes merely seen as stages of the lifecycle of plants, macroalgae, or animals.\n\nAlthough used as a taxonomic category in some pre-Darwinian classifications, e.g., Linnaeus (1753), de Jussieu (1789), Horaninow (1843), Agassiz (1859), Wilson & Cassin (1864), in further classifications, the \"algae\" are seen as an artificial, polyphyletic group.\n\nThroughout the 20th century, most classifications treated the following groups as divisions or classes of algae: cyanophytes, rhodophytes, chrysophytes, xanthophytes, bacillariophytes, phaeophytes, pyrrhophytes (cryptophytes and dinophytes), euglenophytes, and chlorophytes. Later, many new groups were discovered (e.g., Bolidophyceae), and others were splintered from older groups: charophytes and glaucophytes (from chlorophytes), many heterokontophytes (e.g., synurophytes from chrysophytes, or eustigmatophytes from xanthophytes), haptophytes (from chrysophytes), and chlorarachniophytes (from xanthophytes).\n\nWith the abandonment of plant-animal dichotomous classification, most groups of algae (sometimes all) were included in Protista, later also abandoned in favour of Eukaryota. However, as a legacy of the older plant life scheme, some groups that were also treated as protozoans in the past still have duplicated classifications (see ambiregnal protists).\n\nSome parasitic algae (e.g., the green algae \"Prototheca\" and \"Helicosporidium\", parasites of metazoans, or \"Cephaleuros\", parasites of plants) were originally classified as fungi, sporozoans, or protistans of \"incertae sedis\", while others (e.g., the green algae \"Phyllosiphon\" and \"Rhodochytrium\", parasites of plants, or the red algae \"Pterocladiophila\" and \"Gelidiocolax mammillatus\", parasites of other red algae, or the dinoflagellates \"Oodinium\", parasites of fish) had their relationship with algae conjectured early. In other cases, some groups were originally characterized as parasitic algae (e.g., \"Chlorochytrium\"), but later were seen as endophytic algae. Some filamentous bacteria (e.g., \"Beggiatoa\") were originally seen as algae. Furthermore, groups like the apicomplexans are also parasites derived from ancestors that possessed plastids, but are not included in any group traditionally seen as algae.\n\nThe first land plants probably evolved from shallow freshwater charophyte algae much like \"Chara\" almost 500 million years ago. These probably had an isomorphic alternation of generations and were probably filamentous. Fossils of isolated land plant spores suggest land plants may have been around as long as 475 million years ago.\n\nA range of algal morphologies is exhibited, and convergence of features in unrelated groups is common. The only groups to exhibit three-dimensional multicellular thalli are the reds and browns, and some chlorophytes. Apical growth is constrained to subsets of these groups: the florideophyte reds, various browns, and the charophytes. The form of charophytes is quite different from those of reds and browns, because they have distinct nodes, separated by internode 'stems'; whorls of branches reminiscent of the horsetails occur at the nodes. Conceptacles are another polyphyletic trait; they appear in the coralline algae and the Hildenbrandiales, as well as the browns.\n\nMost of the simpler algae are unicellular flagellates or amoeboids, but colonial and nonmotile forms have developed independently among several of the groups. Some of the more common organizational levels, more than one of which may occur in the lifecycle of a species, are\n\nIn three lines, even higher levels of organization have been reached, with full tissue differentiation. These are the brown algae,—some of which may reach 50 m in length (kelps)—the red algae, and the green algae. The most complex forms are found among the green algae (see Charales and Charophyta), in a lineage that eventually led to the higher land plants. The point where these nonalgal plants begin and algae stop is usually taken to be the presence of reproductive organs with protective cell layers, a characteristic not found in the other algal groups.\n\nMany algae, particularly members of the Characeae, have served as model experimental organisms to understand the mechanisms of the water permeability of membranes, osmoregulation, turgor regulation, salt tolerance, cytoplasmic streaming, and the generation of action potentials.\n\nPhytohormones are found not only in higher plants, but in algae, too.\n\nSome species of algae form symbiotic relationships with other organisms. In these symbioses, the algae supply photosynthates (organic substances) to the host organism providing protection to the algal cells. The host organism derives some or all of its energy requirements from the algae. Examples are:\n\nLichens are defined by the International Association for Lichenology to be \"an association of a fungus and a photosynthetic symbiont resulting in a stable vegetative body having a specific structure.\" The fungi, or mycobionts, are mainly from the Ascomycota with a few from the Basidiomycota. They are not found alone in nature; but when they began to associate is not known. One mycobiont associates with the same phycobiont species, rarely two, from the green algae, except that alternatively, the mycobiont may associate with a species of cyanobacteria (hence \"photobiont\" is the more accurate term). A photobiont may be associated with many different mycobionts or may live independently; accordingly, lichens are named and classified as fungal species. The association is termed a morphogenesis because the lichen has a form and capabilities not possessed by the symbiont species alone (they can be experimentally isolated). The photobiont possibly triggers otherwise latent genes in the mycobiont.\n\n Coral reefs are accumulated from the calcareous exoskeletons of marine invertebrates of the order Scleractinia (stony corals). These animals metabolize sugar and oxygen to obtain energy for their cell-building processes, including secretion of the exoskeleton, with water and carbon dioxide as byproducts. Dinoflagellates (algal protists) are often endosymbionts in the cells of the coral-forming marine invertebrates, where they accelerate host-cell metabolism by generating immediately available sugar and oxygen through photosynthesis using incident light and the carbon dioxide produced by the host. Reef-building stony corals (hermatypic corals) require endosymbiotic algae from the genus \"Symbiodinium\" to be in a healthy condition. The loss of \"Symbiodinium\" from the host is known as coral bleaching, a condition which leads to the deterioration of a reef.\n\nGreen algae live close to the surface of some sponges, for example, breadcrumb sponge (\"Halichondria panicea\"). The alga is thus protected from predators; the sponge is provided with oxygen and sugars which can account for 50 to 80% of sponge growth in some species.\n\nRhodophyta, Chlorophyta, and Heterokontophyta, the three main algal divisions, have lifecycles which show considerable variation and complexity. In general, an asexual phase exists where the seaweed's cells are diploid, a sexual phase where the cells are haploid, followed by fusion of the male and female gametes. Asexual reproduction permits efficient population increases, but less variation is possible. Commonly, in sexual reproduction of unicellular and colonial algae, two specialized, sexually compatible, haploid gametes make physical contact and fuse to form a zygote. To ensure a successful mating, the development and release of gametes is highly synchronized and regulated; pheromones may play a key role in these processes. Sexual reproduction allows for more variation and provides the benefit of efficient recombinational repair of DNA damages during meiosis, a key stage of the sexual cycle. However, sexual reproduction is more costly than asexual reproduction. Meiosis has been shown to occur in many different species of algae.\n\nThe \"Algal Collection of the US National Herbarium\" (located in the National Museum of Natural History) consists of approximately 320,500 dried specimens, which, although not exhaustive (no exhaustive collection exists), gives an idea of the order of magnitude of the number of algal species (that number remains unknown). Estimates vary widely. For example, according to one standard textbook, in the British Isles the \"UK Biodiversity Steering Group Report\" estimated there to be 20,000 algal species in the UK. Another checklist reports only about 5,000 species. Regarding the difference of about 15,000 species, the text concludes: \"It will require many detailed field surveys before it is possible to provide a reliable estimate of the total number of species ...\"\n\nRegional and group estimates have been made, as well:\nand so on, but lacking any scientific basis or reliable sources, these numbers have no more credibility than the British ones mentioned above. Most estimates also omit microscopic algae, such as phytoplankton.\n\nThe most recent estimate suggests 72,500 algal species worldwide.\n\nThe distribution of algal species has been fairly well studied since the founding of phytogeography in the mid-19th century. Algae spread mainly by the dispersal of spores analogously to the dispersal of Plantae by seeds and spores. This dispersal can be accomplished by air, water, or other organisms. Due to this, spores can be found in a variety of environments: fresh and marine waters, air, soil, and in or on other organisms. Whether a spore is to grow into an organism depends on the combination of the species and the environmental conditions where the spore lands.\n\nThe spores of freshwater algae are dispersed mainly by running water and wind, as well as by living carriers. However, not all bodies of water can carry all species of algae, as the chemical composition of certain water bodies limits the algae that can survive within them. Marine spores are often spread by ocean currents. Ocean water presents many vastly different habitats based on temperature and nutrient availability, resulting in phytogeographic zones, regions, and provinces.\n\nTo some degree, the distribution of algae is subject to floristic discontinuities caused by geographical features, such as Antarctica, long distances of ocean or general land masses. It is, therefore, possible to identify species occurring by locality, such as \"Pacific algae\" or \"North Sea algae\". When they occur out of their localities, hypothesizing a transport mechanism is usually possible, such as the hulls of ships. For example, \"Ulva reticulata\" and \"U. fasciata\" travelled from the mainland to Hawaii in this manner.\n\nMapping is possible for select species only: \"there are many valid examples of confined distribution patterns.\" For example, \"Clathromorphum\" is an arctic genus and is not mapped far south of there. However, scientists regard the overall data as insufficient due to the \"difficulties of undertaking such studies.\"\n\nAlgae are prominent in bodies of water, common in terrestrial environments, and are found in unusual environments, such as on snow and ice. Seaweeds grow mostly in shallow marine waters, under deep; however, some such as Navicula pennata have been recorded to a depth of .\n\nThe various sorts of algae play significant roles in aquatic ecology. Microscopic forms that live suspended in the water column (phytoplankton) provide the food base for most marine food chains. In very high densities (algal blooms), these algae may discolor the water and outcompete, poison, or asphyxiate other life forms.\n\nAlgae can be used as indicator organisms to monitor pollution in various aquatic systems. In many cases, algal metabolism is sensitive to various pollutants. Due to this, the species composition of algal populations may shift in the presence of chemical pollutants. To detect these changes, algae can be sampled from the environment and maintained in laboratories with relative ease.\n\nOn the basis of their habitat, algae can be categorized as: aquatic (planktonic, benthic, marine, freshwater, lentic, lotic), terrestrial, aerial (subareial), lithophytic, halophytic (or euryhaline), psammon, thermophilic, cryophilic, epibiont (epiphytic, epizoic), endosymbiont (endophytic, endozoic), parasitic, calcifilic or lichenic (phycobiont).\n\nIn classical Chinese, the word is used both for \"algae\" and (in the modest tradition of the imperial scholars) for \"literary talent\". The third island in Kunming Lake beside the Summer Palace in Beijing is known as the Zaojian Tang Dao, which thus simultaneously means \"Island of the Algae-Viewing Hall\" and \"Island of the Hall for Reflecting on Literary Talent\".\n\nAgar, a gelatinous substance derived from red algae, has a number of commercial uses. It is a good medium on which to grow bacteria and fungi, as most microorganisms cannot digest agar.\n\nAlginic acid, or alginate, is extracted from brown algae. Its uses range from gelling agents in food, to medical dressings. Alginic acid also has been used in the field of biotechnology as a biocompatible medium for cell encapsulation and cell immobilization. Molecular cuisine is also a user of the substance for its gelling properties, by which it becomes a delivery vehicle for flavours.\n\nBetween 100,000 and 170,000 wet tons of \"Macrocystis\" are harvested annually in New Mexico for alginate extraction and abalone feed.\n\nTo be competitive and independent from fluctuating support from (local) policy on the long run, biofuels should equal or beat the cost level of fossil fuels. Here, algae-based fuels hold great promise, directly related to the potential to produce more biomass per unit area in a year than any other form of biomass. The break-even point for algae-based biofuels is estimated to occur by 2025.\n\nFor centuries, seaweed has been used as a fertilizer; George Owen of Henllys writing in the 16th century referring to drift weed in South Wales:This kind of ore they often gather and lay on great heapes, where it heteth and rotteth, and will have a strong and loathsome smell; when being so rotten they cast on the land, as they do their muck, and thereof springeth good corn, especially barley ... After spring-tydes or great rigs of the sea, they fetch it in sacks on horse backes, and carie the same three, four, or five miles, and cast it on the lande, which doth very much better the ground for corn and grass.\n\nToday, algae are used by humans in many ways; for example, as fertilizers, soil conditioners, and livestock feed. Aquatic and microscopic species are cultured in clear tanks or ponds and are either harvested or used to treat effluents pumped through the ponds. Algaculture on a large scale is an important type of aquaculture in some places. Maerl is commonly used as a soil conditioner.\n\nNaturally growing seaweeds are an important source of food, especially in Asia. They provide many vitamins including: A, B, B, B, niacin, and C, and are rich in iodine, potassium, iron, magnesium, and calcium. In addition, commercially cultivated microalgae, including both algae and cyanobacteria, are marketed as nutritional supplements, such as spirulina, \"Chlorella\" and the vitamin-C supplement from \"Dunaliella\", high in beta-carotene.\n\nAlgae are national foods of many nations: China consumes more than 70 species, including \"fat choy\", a cyanobacterium considered a vegetable; Japan, over 20 species; Ireland, dulse; Chile, cochayuyo. Laver is used to make \"laver bread\" in Wales, where it is known as \"bara lawr\"; in Korea, \"gim\"; in Japan, \"nori\" and \"aonori\". It is also used along the west coast of North America from California to British Columbia, in Hawaii and by the Māori of New Zealand. Sea lettuce and badderlocks are salad ingredients in Scotland, Ireland, Greenland, and Iceland.\n\nThe oils from some algae have high levels of unsaturated fatty acids. For example, \"Parietochloris incisa\" is very high in arachidonic acid, where it reaches up to 47% of the triglyceride pool. Some varieties of algae favored by vegetarianism and veganism contain the long-chain, essential omega-3 fatty acids, docosahexaenoic acid (DHA) and eicosapentaenoic acid (EPA). Fish oil contains the omega-3 fatty acids, but the original source is algae (microalgae in particular), which are eaten by marine life such as copepods and are passed up the food chain. Algae have emerged in recent years as a popular source of omega-3 fatty acids for vegetarians who cannot get long-chain EPA and DHA from other vegetarian sources such as flaxseed oil, which only contains the short-chain alpha-linolenic acid (ALA).\n\n\nAgricultural Research Service scientists found that 60–90% of nitrogen runoff and 70–100% of phosphorus runoff can be captured from manure effluents using a horizontal algae scrubber, also called an algal turf scrubber (ATS). Scientists developed the ATS, which consists of shallow, 100-foot raceways of nylon netting where algae colonies can form, and studied its efficacy for three years. They found that algae can readily be used to reduce the nutrient runoff from agricultural fields and increase the quality of water flowing into rivers, streams, and oceans. Researchers collected and dried the nutrient-rich algae from the ATS and studied its potential as an organic fertilizer. They found that cucumber and corn seedlings grew just as well using ATS organic fertilizer as they did with commercial fertilizers. Algae scrubbers, using bubbling upflow or vertical waterfall versions, are now also being used to filter aquaria and ponds.\n\nThe alga \"Stichococcus bacillaris\" has been seen to colonize silicone resins used at archaeological sites; biodegrading the synthetic substance.\n\nThe natural pigments (carotenoids and chlorophylls) produced by algae can be used as alternatives to chemical dyes and coloring agents.\nThe presence of some individual algal pigments, together with specific pigment concentration ratios, are taxon-specific: analysis of their concentrations with various analytical methods, particularly high-performance liquid chromatography, can therefore offer deep insight into the taxonomic composition and relative abundance of natural alga populations in sea water samples.\n\nCarrageenan, from the red alga \"Chondrus crispus\", is used as a stabilizer in milk products.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "634", "url": "https://en.wikipedia.org/wiki?curid=634", "title": "Analysis of variance", "text": "Analysis of variance\n\nAnalysis of variance (ANOVA) is a collection of statistical models used to analyze the differences among group means and their associated procedures (such as \"variation\" among and between groups), developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the \"t\"-test to more than two groups. ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is more conservative (results in less type I error) and is therefore suited to a wide range of practical problems.\nWhile the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler. These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. Laplace was performing hypothesis testing in the 1770s. The development of least-squares methods by Laplace and Gauss circa 1800 provided an improved method of combining observations (over the existing practices then used in astronomy and geodesy). It also initiated much study of the contributions to sums of squares. Laplace soon knew how to estimate a variance from a residual (rather than a total) sum of squares. By 1827 Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides. Before 1800 astronomers had isolated observational errors resulting \nfrom reaction times (the \"personal equation\") and had developed methods of reducing the errors. The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added. An eloquent non-mathematical explanation of the additive effects model was\navailable in 1885.\n\nRonald Fisher introduced the term variance and proposed its formal analysis in a 1918 article \"The Correlation Between Relatives on the Supposition of Mendelian Inheritance\". His first application of the analysis of variance was published in 1921. Analysis of variance became widely known after being included in Fisher's 1925 book \"Statistical Methods for Research Workers\".\n\nRandomization models were developed by several researchers. The first was published in Polish by Neyman in 1923.\n\nOne of the attributes of ANOVA which ensured its early popularity was computational elegance. The structure of the additive model allows solution for the additive coefficients by simple algebra rather than by matrix calculations. In the era of mechanical calculators this simplicity was critical. The determination of statistical significance also required access to tables of the F function which were supplied by early statistics texts.\n\nThe analysis of variance can be used as an exploratory tool to explain observations. A dog show provides an example. A dog show is not a random sampling of the breed: it is typically limited to dogs that are adult, pure-bred, and exemplary. A histogram of dog weights from a show might plausibly be rather complex, like the yellow-orange distribution shown in the illustrations. Suppose we wanted to predict the weight of a dog based on a certain set of characteristics of each dog. Before we could do that, we would need to \"explain\" the distribution of weights by dividing the dog population into groups based on those characteristics. A successful grouping will split dogs such that (a) each group has a low variance of dog weights (meaning the group is relatively homogeneous) and (b) the mean of each group is distinct (if two groups have the same mean, then it isn't reasonable to conclude that the groups are, in fact, separate in any meaningful way).\nIn the illustrations to the right, each group is identified as \"X\", \"X\", etc. In the first illustration, we divide the dogs according to the product (interaction) of two binary groupings: young vs old, and short-haired vs long-haired (thus, group 1 is young, short-haired dogs, group 2 is young, long-haired dogs, etc.). Since the distributions of dog weight within each of the groups (shown in blue) has a large variance, and since the means are very close across groups, grouping dogs by these characteristics does not produce an effective way to explain the variation in dog weights: knowing which group a dog is in does not allow us to make any reasonable statements as to what that dog's weight is likely to be. Thus, this grouping fails to \"fit\" the distribution we are trying to explain (yellow-orange).\n\nAn attempt to explain the weight distribution by grouping dogs as (pet vs working breed) and (less athletic vs more athletic) would probably be somewhat more successful (fair fit). The heaviest show dogs are likely to be big strong working breeds, while breeds kept as pets tend to be smaller and thus lighter. As shown by the second illustration, the distributions have variances that are considerably smaller than in the first case, and the means are more reasonably distinguishable. However, the significant overlap of distributions, for example, means that we cannot reliably say that \"X\" and \"X\" are truly distinct (i.e., it is perhaps reasonably likely that splitting dogs according to the flip of a coin—by pure chance—might produce distributions that look similar).\n\nAn attempt to explain weight by breed is likely to produce a very good fit. All Chihuahuas are light and all St Bernards are heavy. The difference in weights between Setters and Pointers does not justify separate breeds. The analysis of variance provides the formal tools to justify these intuitive judgments. A common use of the method is the analysis of experimental data or the development of models. The method has some advantages over correlation: not all of the data must be numeric and one result of the method is a judgment in the confidence in an explanatory relationship.\n\nANOVA is a particular form of statistical hypothesis testing heavily used in the analysis of experimental data. A test result (calculated from the null hypothesis and the sample) is called statistically significant if it is deemed unlikely to have occurred by chance, \"assuming the truth of the null hypothesis\". A statistically significant result, when a probability (p-value) is less than a threshold (significance level), justifies the rejection of the null hypothesis, but only if the priori probability of the null hypothesis is not high.\n\nIn the typical application of ANOVA, the null hypothesis is that all groups are simply random samples of the same population. For example, when studying the effect of different treatments on similar samples of patients, the null hypothesis would be that all treatments have the same effect (perhaps none). Rejecting the null hypothesis would imply that different treatments result in altered effects.\n\nBy construction, hypothesis testing limits the rate of Type I errors (false positives) to a significance level. Experimenters also wish to limit Type II errors (false negatives). \nThe rate of Type II errors depends largely on sample size (the rate will increase for small numbers of samples), significance \nlevel (when the standard of proof is high, the chances of overlooking \na discovery are also high) and effect size (a smaller effect size is more prone to Type II error).\n\nThe terminology of ANOVA is largely from the statistical \ndesign of experiments. The experimenter adjusts factors and \nmeasures responses in an attempt to determine an effect. Factors are \nassigned to experimental units by a combination of randomization and \nblocking to ensure the validity of the results. Blinding keeps the\nweighing impartial. Responses show a variability that is partially \nthe result of the effect and is partially random error.\n\nANOVA is the synthesis of several ideas and it is used for multiple \npurposes. As a consequence, it is difficult to define concisely or precisely.\n\n\"Classical ANOVA for balanced data does three things at once:\n\nIn short, ANOVA is a statistical tool used in several ways to develop and confirm an explanation for the observed data.\n\nAdditionally:\n\nAs a result:\nANOVA \"has long enjoyed the status of being the most used (some would \nsay abused) statistical technique in psychological research.\"\nANOVA \"is probably the most useful technique in the field of \nstatistical inference.\"\n\nANOVA is difficult to teach, particularly for complex experiments, with split-plot designs being notorious. In some cases the proper \napplication of the method is best determined by problem pattern recognition \nfollowed by the consultation of a classic authoritative test.\n\n(Condensed from the NIST Engineering Statistics handbook: Section 5.7. A \nGlossary of DOE Terminology.)\n\n\nThere are three classes of models used in the analysis of variance, and these are outlined here.\n\nThe fixed-effects model (class I) of analysis of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see whether the response variable values change. This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole.\n\nRandom effects model (class II) is used when the treatments are not fixed. This occurs when the various factor levels are sampled from a larger population. Because the levels themselves are random variables, some assumptions and the method of contrasting the treatments (a multi-variable generalization of simple differences) differ from the fixed-effects model.\n\nA mixed-effects model (class III) contains experimental factors of both fixed and random-effects types, with appropriately different interpretations and analysis for the two types.\n\nExample:\nTeaching experiments could be performed by a college or university department \nto find a good introductory textbook, with each text considered a \ntreatment. The fixed-effects model would compare a list of candidate \ntexts. The random-effects model would determine whether important \ndifferences exist among a list of randomly selected texts. The \nmixed-effects model would compare the (fixed) incumbent texts to \nrandomly selected alternatives.\n\nDefining fixed and random effects has proven elusive, with competing \ndefinitions arguably leading toward a linguistic quagmire.\n\nThe analysis of variance has been studied from several approaches, the most common of which uses a linear model that relates the response to the treatments and blocks. Note that the model is linear in parameters but may be nonlinear across factor levels. Interpretation is easy when data is balanced across factors but much deeper understanding is needed for unbalanced data.\n\nThe analysis of variance can be presented in terms of a linear model, which makes the following assumptions about the probability distribution of the responses:\n\nThe separate assumptions of the textbook model imply that the errors are independently, identically, and normally distributed for fixed effects models, that is, that the errors (formula_1) are independent and\n\nIn a randomized controlled experiment, the treatments are randomly assigned to experimental units, following the experimental protocol. This randomization is objective and declared before the experiment is carried out. The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of C. S. Peirce and Ronald Fisher. This design-based analysis was discussed and developed by Francis J. Anscombe at Rothamsted Experimental Station and by Oscar Kempthorne at Iowa State University. Kempthorne and his students make an assumption of \"unit treatment additivity\", which is discussed in the books of Kempthorne and David R. Cox.\n\nIn its simplest form, the assumption of unit-treatment additivity states that the observed response formula_3 from experimental unit formula_4 when receiving treatment formula_5 can be written as the sum of the unit's response formula_6 and the treatment-effect formula_7, that is \nThe assumption of unit-treatment additivity implies that, for every treatment formula_5, the formula_5th treatment has exactly the same effect formula_11 on every experiment unit.\n\nThe assumption of unit treatment additivity usually cannot be directly falsified, according to Cox and Kempthorne. However, many \"consequences\" of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of unit-treatment additivity \"implies\" that the variance is constant for all treatments. Therefore, by contraposition, a necessary condition for unit-treatment additivity is that the variance is constant.\n\nThe use of unit treatment additivity and randomization is similar to the design-based inference that is standard in finite-population survey sampling.\n\nKempthorne uses the randomization-distribution and the assumption of \"unit treatment additivity\" to produce a \"derived linear model\", very similar to the textbook model discussed previously. The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies. However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations. In the randomization-based analysis, there is \"no assumption\" of a \"normal\" distribution and certainly \"no assumption\" of \"independence\". On the contrary, \"the observations are dependent\"!\n\nThe randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time. Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments.\n\nHowever, when applied to data from non-randomized experiments or observational studies, model-based analysis lacks the warrant of randomization. For observational data, the derivation of confidence intervals must use \"subjective\" models, as emphasized by Ronald Fisher and his followers. In practice, the estimates of treatment-effects from observational studies generally are often inconsistent. In practice, \"statistical models\" and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public.\n\nThe normal-model based ANOVA analysis assumes the independence, normality and \nhomogeneity of the variances of the residuals. The \nrandomization-based analysis assumes only the homogeneity of the \nvariances of the residuals (as a consequence of unit-treatment \nadditivity) and uses the randomization procedure of the experiment. \nBoth these analyses require homoscedasticity, as an assumption for the normal-model analysis and as a consequence of randomization and additivity for the randomization-based analysis.\n\nHowever, studies of processes that \nchange variances rather than means (called dispersion effects) have \nbeen successfully conducted using ANOVA. There are\n\"no\" necessary assumptions for ANOVA in its full generality, but the\nF-test used for ANOVA hypothesis testing has assumptions and practical \nlimitations which are of continuing interest.\n\nProblems which do not satisfy the assumptions of ANOVA can often be transformed to satisfy the assumptions. \nThe property of unit-treatment additivity is not invariant under a \"change of scale\", so statisticians often use transformations to achieve unit-treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be transformed to stabilize the variance. Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model.\nAccording to Cauchy's functional equation theorem, the logarithm is the only continuous transformation that transforms real multiplication to addition.\n\nANOVA is used in the analysis of comparative experiments, those in \nwhich only the difference in outcomes is of interest. The statistical\nsignificance of the experiment is determined by a ratio of two \nvariances. This ratio is independent of several possible alterations\nto the experimental observations: Adding a constant to all \nobservations does not alter significance. Multiplying all \nobservations by a constant does not alter significance. So ANOVA \nstatistical significance result is independent of constant bias and \nscaling errors as well as the units used in expressing observations. \nIn the era of mechanical calculation it was common to \nsubtract a constant from all observations (when equivalent to \ndropping leading digits) to simplify data entry. This is an example of data\ncoding.\n\nThe calculations of ANOVA can be characterized as computing a number\nof means and variances, dividing two variances and comparing the ratio \nto a handbook value to determine statistical significance. Calculating \na treatment effect is then trivial, \"the effect of any treatment is \nestimated by taking the difference between the mean of the \nobservations which receive the treatment and the general mean\".\n\nANOVA uses traditional standardized terminology. The definitional \nequation of sample variance is\nformula_12, where the \ndivisor is called the degrees of freedom (DF), the summation is called \nthe sum of squares (SS), the result is called the mean square (MS) and \nthe squared terms are deviations from the sample mean. ANOVA \nestimates 3 sample variances: a total variance based on all the \nobservation deviations from the grand mean, an error variance based on \nall the observation deviations from their appropriate \ntreatment means, and a treatment variance. The treatment variance is\nbased on the deviations of treatment means from the grand mean, the \nresult being multiplied by the number of observations in each \ntreatment to account for the difference between the variance of \nobservations and the variance of means.\n\nThe fundamental technique is a partitioning of the total sum of squares \"SS\" into components related to the effects used in the model. For example, the model for a simplified ANOVA with one type of treatment at different levels.\n\nThe number of degrees of freedom \"DF\" can be partitioned in a similar way: one of these components (that for error) specifies a chi-squared distribution which describes the associated sum of squares, while the same is true for \"treatments\" if there is no treatment effect.\n\nSee also Lack-of-fit sum of squares.\n\nThe F-test is used for comparing the factors of the total deviation. For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic\n\nwhere \"MS\" is mean square, formula_17 = number of treatments and \nformula_18 = total number of cases\n\nto the F-distribution with formula_19, formula_20 degrees of freedom. Using the F-distribution is a natural candidate because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled chi-squared distribution.\n\nThe expected value of F is formula_21 (where n is the treatment sample size)\nwhich is 1 for no treatment effect. As values of F increase above 1, the evidence is increasingly inconsistent with the null hypothesis. Two apparent experimental methods of increasing F are increasing the sample size and reducing the error variance by tight experimental controls.\n\nThere are two methods of concluding the ANOVA hypothesis test, both of which produce the same result:\nThe ANOVA F-test is known to be nearly optimal in the sense of minimizing false negative errors for a fixed rate of false positive errors (i.e. maximizing power for a fixed significance level). For example, to test the hypothesis that various medical treatments have exactly the same effect, the F-test's p-values closely approximate the permutation test's p-values: The approximation is particularly close when the design is balanced. Such permutation tests characterize tests with maximum power against all alternative hypotheses, as observed by Rosenbaum. The ANOVA F–test (of the null-hypothesis that all treatments have exactly the same effect) is recommended as a practical test, because of its robustness against many alternative distributions.\n\nANOVA consists of separable parts; partitioning sources of variance \nand hypothesis testing can be used individually. ANOVA is used to \nsupport other statistical tools. Regression is first used to fit more \ncomplex models to data, then ANOVA is used to compare models with the \nobjective of selecting simple(r) models that adequately describe the \ndata. \"Such models could be fit without any reference to ANOVA, but \nANOVA tools could then be used to make some sense of the fitted models, \nand to test hypotheses about batches of coefficients.\" \n\"[W]e think of the analysis of variance as a way of understanding and structuring \nmultilevel models—not as an alternative to regression but as a tool \nfor summarizing complex high-dimensional inferences ...\"\n\nThe simplest experiment suitable for ANOVA analysis is the completely \nrandomized experiment with a single factor. More complex experiments \nwith a single factor involve constraints on randomization and include \ncompletely randomized blocks and Latin squares (and variants: \nGraeco-Latin squares, etc.). The more complex experiments share many \nof the complexities of multiple factors. A relatively complete \ndiscussion of the analysis (models, data summaries, ANOVA table) of \nthe completely randomized experiment is \navailable.\n\nANOVA generalizes to the study of the effects of multiple factors. \nWhen the experiment includes observations at all combinations of \nlevels of each factor, it is termed factorial. \nFactorial experiments \nare more efficient than a series of single factor experiments and the \nefficiency grows as the number of factors increases. Consequently, factorial designs are heavily used.\n\nThe use of ANOVA to study the effects of multiple factors has a complication. In a 3-way ANOVA with factors x, y and z, the ANOVA model includes terms for the main effects (x, y, z) and terms for interactions (xy, xz, yz, xyz). \nAll terms require hypothesis tests. The proliferation of interaction terms increases the risk that some hypothesis test will produce a false positive by chance. Fortunately, experience says that high order interactions are rare. \nThe ability to detect interactions is a major advantage of multiple \nfactor ANOVA. Testing one factor at a time hides interactions, but \nproduces apparently inconsistent experimental results.\n\nCaution is advised when encountering interactions; Test \ninteraction terms first and expand the analysis beyond ANOVA if \ninteractions are found. Texts vary in their recommendations regarding \nthe continuation of the ANOVA procedure after encountering an \ninteraction. Interactions complicate the interpretation of \nexperimental data. Neither the calculations of significance nor the \nestimated treatment effects can be taken at face value. \"A \nsignificant interaction will often mask the significance of main effects.\" Graphical methods are recommended\nto enhance understanding. Regression is often useful. A lengthy discussion of interactions is available in Cox (1958). Some interactions can be removed (by transformations) while others cannot.\n\nA variety of techniques are used with multiple factor ANOVA to reduce expense. One technique used in factorial designs is to minimize replication (possibly no replication with support of analytical trickery) and to combine groups when effects are found to be statistically (or practically) insignificant. An experiment with many insignificant factors may collapse into one with a few factors supported by many replications.\n\nSeveral fully worked numerical examples are available. A \nsimple case uses one-way (a single factor) analysis. A more complex case uses two-way (two-factor) analysis.\n\nSome analysis is required in support of the \"design\" of the experiment while other analysis is performed after changes in the factors are formally found to produce statistically significant changes in the responses. Because experimentation is iterative, the results of one experiment alter plans for following experiments.\n\nIn the design of an experiment, the number of experimental units is planned to satisfy the goals of the experiment. Experimentation is often sequential.\n\nEarly experiments are often designed to provide mean-unbiased estimates of treatment effects and of experimental error. Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude; in this case, the number of experimental units is chosen so that the experiment is within budget and has adequate power, among other goals.\n\nReporting sample size analysis is generally required in psychology. \"Provide information on sample size and the process that led to sample size decisions.\" The analysis, which is written in the experimental protocol before the experiment is conducted, is examined in grant applications and administrative review boards.\n\nBesides the power analysis, there are less formal methods for selecting the number of experimental units. These include graphical methods based on limiting\nthe probability of false negative errors, graphical methods based on an expected variation increase (above the residuals) and methods based on achieving a desired confident interval.\n\nPower analysis is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and significance level. Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true.\n\nSeveral standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor(s) and the dependent variable (e.g., η, ω, or ƒ) or the overall standardized difference (Ψ) of the complete model. Standardized effect-size estimates facilitate comparison of findings across studies and disciplines. However, while standardized effect sizes are commonly used in much of the professional literature, a non-standardized measure of effect size that has immediately \"meaningful\" units may be preferable for reporting purposes.\n\nIt is always appropriate to carefully consider outliers. They have a disproportionate impact on statistical conclusions and are often the result of errors.\n\nIt is prudent to verify that the assumptions of ANOVA have been met. Residuals are examined or analyzed to confirm homoscedasticity and gross normality. Residuals should have the appearance of (zero mean normal distribution) noise when plotted as a function of anything including time and \nmodeled data values. Trends hint at interactions among factors or among observations. One rule of thumb: \"If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results \nwill still be approximately correct.\"\n\nA statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses. Follow-up tests are often distinguished in terms of whether they are planned (a priori) or post hoc. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.\n\nOften one of the \"treatments\" is none, so the treatment group can act as a control. Dunnett's test (a modification of the t-test) tests whether each of the other treatment groups has the same \nmean as the control.\n\nPost hoc tests such as Tukey's range test most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors. Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of groups means where one set has two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.\n\nFollowing ANOVA with pair-wise multiple-comparison tests has been criticized on several grounds. There are many such tests (10 in one table) and recommendations regarding their use are vague or conflicting.\n\nThere are several types of ANOVA. Many statisticians base ANOVA on the design of the experiment, especially on the protocol that specifies the random assignment of treatments to subjects; the protocol's description of the assignment mechanism should include a specification of the structure of the treatments and of any blocking. It is also common to apply ANOVA to observational data using an appropriate statistical model.\n\nSome popular designs use the following types of ANOVA:\n\nBalanced experiments (those with an equal sample size for each treatment) are relatively easy to interpret; Unbalanced \nexperiments offer more complexity. For single factor (one way) ANOVA, the adjustment for unbalanced data is easy, but the unbalanced analysis lacks both robustness and power. For more complex designs the lack of balance leads to further complications. \"The orthogonality property of main effects and interactions present in balanced data does not carry over to the unbalanced case. This means that the usual analysis of variance techniques do not apply. \nConsequently, the analysis of unbalanced factorials is much more difficult than that for balanced designs.\" In the general case, \"The analysis of variance can also be applied to unbalanced data, but then the sums of squares, mean squares, and F-ratios will depend on the order in which the sources of variation \nare considered.\" The simplest techniques for handling unbalanced data restore balance by either throwing out data or by synthesizing missing data. More complex techniques use regression.\n\nANOVA is (in part) a significance test. The American Psychological Association holds the view that simply reporting significance is insufficient and that reporting confidence bounds is preferred.\n\nWhile ANOVA is conservative (in maintaining a significance level) against multiple comparisons in one dimension, it is not conservative against comparisons in multiple dimensions.\n\nANOVA is considered to be a special case of linear regression which in turn is a special case of the general linear model. All consider the observations to be the sum of a model (fit) and a residual (error) to be minimized.\n\nThe Kruskal–Wallis test and the Friedman test are nonparametric tests, which do not rely on an assumption of normality.\n\nBelow we make clear the connection between multi-way ANOVA and linear regression. Linearly re-order the data so that formula_22 observation is associated with a response formula_23 and factors formula_24 where formula_25 denotes the different factors and formula_26 is the total number of factors. In one-way ANOVA formula_27 and in two-way ANOVA formula_28. Furthermore, we assume the formula_29 factor has formula_30 levels. Now, we can one-hot encode the factors into the formula_31 dimensional vector formula_32.\n\nThe one-hot encoding function formula_33 is defined such that the formula_34 entry of formula_35 is\nformula_36\nThe vector formula_32 is the concatenation of all of the above vectors for all formula_38. Thus, formula_39. In order to obtain a fully general formula_26-way interaction ANOVA we must also concatenate every additional interaction term in the vector formula_32 and then add an intercept term. Let that vector be formula_42.\n\nWith this notation in place, we now have the exact connection with linear regression. We simply regress response formula_23 against the vector formula_44. However, there is a concern about identifiability. In order to overcome such issues we assume that the sum of the parameters within each set of interactions is equal to zero. From here, one can use F-statistics or other methods to determine the relevance of the individual factors.\n\nWe can consider the 2-way interaction example where we assume that the first factor has 2 levels and the second factor has 3 levels.\n\nDefine formula_45 if formula_46 and formula_47 if formula_48, i.e. formula_49 is the one-hot encoding of the first factor and formula_38 is the one-hot encoding of the second factor.\n\nWith that,\nformula_51\nwhere the last term is an intercept term. For a more concrete example suppose that\nformula_52\nThen,\nformula_53\n\n\n\n"}
{"id": "639", "url": "https://en.wikipedia.org/wiki?curid=639", "title": "Alkane", "text": "Alkane\n\nIn organic chemistry, an alkane, or paraffin (a historical name that also has other meanings), is an acyclic saturated hydrocarbon. In other words, an alkane consists of hydrogen and carbon atoms arranged in a tree structure in which all the carbon-carbon bonds are single. Alkanes have the general chemical formula . The alkanes range in complexity from the simplest case of methane, CH where \"n\" = 1 (sometimes called the parent molecule), to arbitrarily large molecules. \n\nBesides this standard definition by the International Union of Pure and Applied Chemistry, in some authors' usage the term \"alkane\" is applied to any saturated hydrocarbon, including those that are either monocyclic (i.e. the cycloalkanes) or polycyclic.\n\nIn an alkane, each carbon atom has 4 bonds (either C-C or C-H), and each hydrogen atom is joined to one of the carbon atoms (so in a C-H bond). The longest series of linked carbon atoms in a molecule is known as its carbon skeleton or carbon backbone. The number of carbon atoms may be thought of as the size of the alkane.\n\nOne group of the higher alkanes are waxes, solids at standard ambient temperature and pressure (SATP), for which the number of carbons in the carbon backbone is greater than about 17.\n\nWith their repeated -CH- units, the alkanes constitute a homologous series of organic compounds in which the members differ in molecular mass by multiples of 14.03 u (the total mass of each such methylene-bridge unit, which comprises a single carbon atom of mass 12.01 u and two hydrogen atoms of mass ~1.01 u each).\n\nAlkanes are not very reactive and have little biological activity. They can be viewed as molecular trees upon which can be hung the more active/reactive functional groups of biological molecules.\n\nThe alkanes have two main commercial sources: petroleum (crude oil) and natural gas.\n\nAn alkyl group, generally abbreviated with the symbol R, is a functional group that, like an alkane, consists solely of single-bonded carbon and hydrogen atoms connected acyclically—for example, a methyl or ethyl group.\n\nSaturated hydrocarbons are hydrocarbons having only single covalent bonds between their carbons. They can be:\n\nAccording to the definition by IUPAC, the former two are alkanes, whereas the third group is called cycloalkanes. Saturated hydrocarbons can also combine any of the linear, cyclic (e.g., polycyclic) and branching structures; the general formula is , where \"k\" is the number of independent loops. Alkanes are the acyclic (loopless) ones, corresponding to \"k\" = 0.\n\nAlkanes with more than three carbon atoms can be arranged in various different ways, forming structural isomers. The simplest isomer of an alkane is the one in which the carbon atoms are arranged in a single chain with no branches. This isomer is sometimes called the \"n\"-isomer (\"n\" for \"normal\", although it is not necessarily the most common). However the chain of carbon atoms may also be branched at one or more points. The number of possible isomers increases rapidly with the number of carbon atoms. For example:\n\nBranched alkanes can be chiral. For example, 3-methylhexane and its higher homologues are chiral due to their stereogenic center at carbon atom number 3. In addition to the alkane isomers, the chain of carbon atoms may form one or more loops. Such compounds are called cycloalkanes.\n\nThe IUPAC nomenclature (systematic way of naming compounds) for alkanes is based on identifying hydrocarbon chains. Unbranched, saturated hydrocarbon chains are named systematically with a Greek numerical prefix denoting the number of carbons and the suffix \"-ane\".\n\nIn 1866, August Wilhelm von Hofmann suggested systematizing nomenclature by using the whole sequence of vowels a, e, i, o and u to create suffixes -ane, -ene, -ine (or -yne), -one, -une, for the hydrocarbons CH, CH, CH, CH, CH. Now, the first three name hydrocarbons with single, double and triple bonds; \"-one\" represents a ketone; \"-ol\" represents an alcohol or OH group; \"-oxy-\" means an ether and refers to oxygen between two carbons, so that methoxymethane is the IUPAC name for dimethyl ether.\n\nIt is difficult or impossible to find compounds with more than one IUPAC name. This is because shorter chains attached to longer chains are prefixes and the convention includes brackets. Numbers in the name, referring to which carbon a group is attached to, should be as low as possible so that 1- is implied and usually omitted from names of organic compounds with only one side-group. Symmetric compounds will have two ways of arriving at the same name.\n\nStraight-chain alkanes are sometimes indicated by the prefix \"\"n\"-\" (for \"normal\") where a non-linear isomer exists. Although this is not strictly necessary, the usage is still common in cases where there is an important difference in properties between the straight-chain and branched-chain isomers, e.g., \"n\"-hexane or 2- or 3-methylpentane. Alternative names for this group are: linear paraffins or \"n\"-paraffins.\n\nThe members of the series (in terms of number of carbon atoms) are named as follows:\n\nThe first four names were derived from methanol, ether, propionic acid and butyric acid, respectively (hexadecane is also sometimes referred to as cetane). Alkanes with five or more carbon atoms are named by adding the suffix -ane to the appropriate numerical multiplier prefix with elision of any terminal vowel (\"-a\" or \"-o\") from the basic numerical term. Hence, pentane, CH; hexane, CH; heptane, CH; octane, CH; etc. The prefix is generally Greek, however alkanes with a carbon atom count ending in nine, for example nonane, use the Latin prefix non-. For a more complete list, see List of alkanes.\n\nSimple branched alkanes often have a common name using a prefix to distinguish them from linear alkanes, for example \"n\"-pentane, isopentane, and neopentane.\n\nIUPAC naming conventions can be used to produce a systematic name.\n\nThe key steps in the naming of more complicated branched alkanes are as follows:\n\nThough technically distinct from the alkanes, this class of hydrocarbons is referred to by some as the \"cyclic alkanes.\" As their description implies, they contain one or more rings.\n\nSimple cycloalkanes have a prefix \"cyclo-\" to distinguish them from alkanes. Cycloalkanes are named as per their acyclic counterparts with respect to the number of carbon atoms in their backbones, e.g., cyclopentane (CH) is a cycloalkane with 5 carbon atoms just like pentane (CH), but they are joined up in a five-membered ring. In a similar manner, propane and cyclopropane, butane and cyclobutane, etc.\n\nSubstituted cycloalkanes are named similarly to substituted alkanes — the cycloalkane ring is stated, and the substituents are according to their position on the ring, with the numbering decided by the Cahn–Ingold–Prelog priority rules.\n\nThe trivial (non-systematic) name for alkanes is \"paraffins\". Together, alkanes are known as the \"paraffin series\". Trivial names for compounds are usually historical artifacts. They were coined before the development of systematic names, and have been retained due to familiar usage in industry. Cycloalkanes are also called naphthenes.\n\nIt is almost certain that the term \"paraffin\" stems from the petrochemical industry. Branched-chain alkanes are called \"isoparaffins\". The use of the term \"paraffin\" is a general term and often does not distinguish between pure compounds and mixtures of isomers, i.e., compounds of the same chemical formula, e.g., pentane and isopentane.\n\nThe following trivial names are retained in the IUPAC system:\n\nAll alkanes are colourless and odourless.\n\nAlkanes experience intermolecular van der Waals forces. Stronger intermolecular van der Waals forces give rise to greater boiling points of alkanes.\n\nThere are two determinants for the strength of the van der Waals forces:\n\nUnder standard conditions, from CH to CH alkanes are gaseous; from CH to CH they are liquids; and after CH they are solids. As the boiling point of alkanes is primarily determined by weight, it should not be a surprise that the boiling point has almost a linear relationship with the size (molecular weight) of the molecule. As a rule of thumb, the boiling point rises 20–30 °C for each carbon added to the chain; this rule applies to other homologous series.\n\nA straight-chain alkane will have a boiling point higher than a branched-chain alkane due to the greater surface area in contact, thus the greater van der Waals forces, between adjacent molecules. For example, compare isobutane (2-methylpropane) and n-butane (butane), which boil at −12 and 0 °C, and 2,2-dimethylbutane and 2,3-dimethylbutane which boil at 50 and 58 °C, respectively. For the latter case, two molecules 2,3-dimethylbutane can \"lock\" into each other better than the cross-shaped 2,2-dimethylbutane, hence the greater van der Waals forces.\n\nOn the other hand, cycloalkanes tend to have higher boiling points than their linear counterparts due to the locked conformations of the molecules, which give a plane of intermolecular contact.\n\nThe melting points of the alkanes follow a similar trend to boiling points for the same reason as outlined above. That is, (all other things being equal) the larger the molecule the higher the melting point. There is one significant difference between boiling points and melting points. Solids have more rigid and fixed structure than liquids. This rigid structure requires energy to break down. Thus the better put together solid structures will require more energy to break apart. For alkanes, this can be seen from the graph above (i.e., the blue line). The odd-numbered alkanes have a lower trend in melting points than even numbered alkanes. This is because even numbered alkanes pack well in the solid phase, forming a well-organized structure, which requires more energy to break apart. The odd-numbered alkanes pack less well and so the \"looser\" organized solid packing structure requires less energy to break apart.\n\nThe melting points of branched-chain alkanes can be either higher or lower than those of the corresponding straight-chain alkanes, again depending on the ability of the alkane in question to pack well in the solid phase: This is particularly true for isoalkanes (2-methyl isomers), which often have melting points higher than those of the linear analogues.\n\nAlkanes do not conduct electricity, nor are they substantially polarized by an electric field. For this reason, they do not form hydrogen bonds and are insoluble in polar solvents such as water. Since the hydrogen bonds between individual water molecules are aligned away from an alkane molecule, the coexistence of an alkane and water leads to an increase in molecular order (a reduction in entropy). As there is no significant bonding between water molecules and alkane molecules, the second law of thermodynamics suggests that this reduction in entropy should be minimized by minimizing the contact between alkane and water: Alkanes are said to be hydrophobic in that they repel water.\n\nTheir solubility in nonpolar solvents is relatively good, a property that is called lipophilicity. Different alkanes are, for example, miscible in all proportions among themselves.\n\nThe density of the alkanes usually increases with the number of carbon atoms but remains less than that of water. Hence, alkanes form the upper layer in an alkane–water mixture.\n\nThe molecular structure of the alkanes directly affects their physical and chemical characteristics. It is derived from the electron configuration of carbon, which has four valence electrons. The carbon atoms in alkanes are always sp hybridized, that is to say that the valence electrons are said to be in four equivalent orbitals derived from the combination of the 2s orbital and the three 2p orbitals. These orbitals, which have identical energies, are arranged spatially in the form of a tetrahedron, the angle of cos(−) ≈ 109.47° between them.\n\nAn alkane molecule has only C–H and C–C single bonds. The former result from the overlap of an sp orbital of carbon with the 1s orbital of a hydrogen; the latter by the overlap of two sp orbitals on different carbon atoms. The bond lengths amount to 1.09 × 10 m for a C–H bond and 1.54 × 10 m for a C–C bond.\nThe spatial arrangement of the bonds is similar to that of the four sp orbitals—they are tetrahedrally arranged, with an angle of 109.47° between them. Structural formulae that represent the bonds as being at right angles to one another, while both common and useful, do not correspond with the reality.\n\nThe structural formula and the bond angles are not usually sufficient to completely describe the geometry of a molecule. There is a further degree of freedom for each carbon–carbon bond: the torsion angle between the atoms or groups bound to the atoms at each end of the bond. The spatial arrangement described by the torsion angles of the molecule is known as its conformation.\n\nEthane forms the simplest case for studying the conformation of alkanes, as there is only one C–C bond. If one looks down the axis of the C–C bond, one will see the so-called Newman projection. The hydrogen atoms on both the front and rear carbon atoms have an angle of 120° between them, resulting from the projection of the base of the tetrahedron onto a flat plane. However, the torsion angle between a given hydrogen atom attached to the front carbon and a given hydrogen atom attached to the rear carbon can vary freely between 0° and 360°. This is a consequence of the free rotation about a carbon–carbon single bond. Despite this apparent freedom, only two limiting conformations are important: eclipsed conformation and staggered conformation.\n\nThe two conformations, also known as rotamers, differ in energy: The staggered conformation is 12.6 kJ/mol lower in energy (more stable) than the eclipsed conformation (the least stable).\n\nThis difference in energy between the two conformations, known as the torsion energy, is low compared to the thermal energy of an ethane molecule at ambient temperature. There is constant rotation about the C–C bond. The time taken for an ethane molecule to pass from one staggered conformation to the next, equivalent to the rotation of one CH group by 120° relative to the other, is of the order of 10 seconds.\n\nThe case of higher alkanes is more complex but based on similar principles, with the antiperiplanar conformation always being the most favored around each carbon–carbon bond. For this reason, alkanes are usually shown in a zigzag arrangement in diagrams or in models. The actual structure will always differ somewhat from these idealized forms, as the differences in energy between the conformations are small compared to the thermal energy of the molecules: Alkane molecules have no fixed structural form, whatever the models may suggest.\n\nVirtually all organic compounds contain carbon–carbon, and carbon–hydrogen bonds, and so show some of the features of alkanes in their spectra. Alkanes are notable for having no other groups, and therefore for the \"absence\" of other characteristic spectroscopic features of a different functional group like –OH, –CHO, –COOH etc.\n\nThe carbon–hydrogen stretching mode gives a strong absorption between 2850 and 2960 cm, while the carbon–carbon stretching mode absorbs between 800 and 1300 cm. The carbon–hydrogen bending modes depend on the nature of the group: methyl groups show bands at 1450 cm and 1375 cm, while methylene groups show bands at 1465 cm and 1450 cm. Carbon chains with more than four carbon atoms show a weak absorption at around 725 cm.\n\nThe proton resonances of alkanes are usually found at \"δ\" = 0.5–1.5. The carbon-13 resonances depend on the number of hydrogen atoms attached to the carbon: \"δ\" = 8–30 (primary, methyl, –CH), 15–55 (secondary, methylene, –CH–), 20–60 (tertiary, methyne, C–H) and quaternary. The carbon-13 resonance of quaternary carbon atoms is characteristically weak, due to the lack of nuclear Overhauser effect and the long relaxation time, and can be missed in weak samples, or samples that have not been run for a sufficiently long time.\n\nAlkanes have a high ionization energy, and the molecular ion is usually weak. The fragmentation pattern can be difficult to interpret, but, in the case of branched chain alkanes, the carbon chain is preferentially cleaved at tertiary or quaternary carbons due to the relative stability of the resulting free radicals. The fragment resulting from the loss of a single methyl group (\"M\" − 15) is often absent, and other fragments are often spaced by intervals of fourteen mass units, corresponding to sequential loss of CH groups.\n\nAlkanes are only weakly reactive with ionic and other polar substances. The acid dissociation constant (pK) values of all alkanes are above 60, hence they are practically inert to acids and bases (see: carbon acids). This inertness is the source of the term \"paraffins\" (with the meaning here of \"lacking affinity\"). In crude oil the alkane molecules have remained chemically unchanged for millions of years.\n\nHowever redox reactions of alkanes, in particular with oxygen and the halogens, are possible as the carbon atoms are in a strongly reduced condition; in the case of methane, the lowest possible oxidation state for carbon (−4) is reached. Reaction with oxygen (\"if\" present in sufficient quantity to satisfy the reaction stoichiometry) leads to combustion without any smoke, producing carbon dioxide and water. Free radical halogenation reactions occur with halogens, leading to the production of haloalkanes. In addition, alkanes have been shown to interact with, and bind to, certain transition metal complexes in C–H bond activation.\n\nFree radicals, molecules with unpaired electrons, play a large role in most reactions of alkanes, such as cracking and reformation where long-chain alkanes are converted into shorter-chain alkanes and straight-chain alkanes into branched-chain isomers.\n\nIn highly branched alkanes, the bond angle may differ significantly from the optimal value (109.5°) in order to allow the different groups sufficient space. This causes a tension in the molecule, known as steric hindrance, and can substantially increase the reactivity.\n\nAll alkanes react with oxygen in a combustion reaction, although they become increasingly difficult to ignite as the number of carbon atoms increases. The general equation for complete combustion is:\n\nIn the absence of sufficient oxygen, carbon monoxide or even soot can be formed, as shown below:\n\nFor example, methane:\n\nSee the alkane heat of formation table for detailed data.\nThe standard enthalpy change of combustion, Δ\"H\", for alkanes increases by about 650 kJ/mol per CH group. Branched-chain alkanes have lower values of Δ\"H\" than straight-chain alkanes of the same number of carbon atoms, and so can be seen to be somewhat more stable.\n\nAlkanes react with halogens in a so-called \"free radical halogenation\" reaction. The hydrogen atoms of the alkane are progressively replaced by halogen atoms. Free radicals are the reactive species that participate in the reaction, which usually leads to a mixture of products. The reaction is highly exothermic, and can lead to an explosion.\n\nThese reactions are an important industrial route to halogenated hydrocarbons. There are three steps:\n\nExperiments have shown that all halogenation produces a mixture of all possible isomers, indicating that all hydrogen atoms are susceptible to reaction. The mixture produced, however, is not a statistical mixture: Secondary and tertiary hydrogen atoms are preferentially replaced due to the greater stability of secondary and tertiary free-radicals. An example can be seen in the monobromination of propane:\n\nCracking breaks larger molecules into smaller ones. This can be done with a thermal or catalytic method. The thermal cracking process follows a homolytic mechanism with formation of free-radicals. The catalytic cracking process involves the presence of acid catalysts (usually solid acids such as silica-alumina and zeolites), which promote a heterolytic (asymmetric) breakage of bonds yielding pairs of ions of opposite charges, usually a carbocation and the very unstable hydride anion. Carbon-localized free radicals and cations are both highly unstable and undergo processes of chain rearrangement, C–C scission in position beta (i.e., cracking) and intra- and intermolecular hydrogen transfer or hydride transfer. In both types of processes, the corresponding reactive intermediates (radicals, ions) are permanently regenerated, and thus they proceed by a self-propagating chain mechanism. The chain of reactions is eventually terminated by radical or ion recombination.\n\nDragan and his colleague were the first to report about isomerization in alkanes. Isomerization and reformation are processes in which straight-chain alkanes are heated in the presence of a platinum catalyst. In isomerization, the alkanes become branched-chain isomers. In other words, it does not lose any carbons or hydrogens, keeping the same molecular weight. In reformation, the alkanes become cycloalkanes or aromatic hydrocarbons, giving off hydrogen as a by-product. Both of these processes raise the octane number of the substance. Butane is the most common alkane that is put under the process of isomerization, as it makes many branched alkanes with high octane numbers.\n\nAlkanes will react with steam in the presence of a nickel catalyst to give hydrogen. Alkanes can be chlorosulfonated and nitrated, although both reactions require special conditions. The fermentation of alkanes to carboxylic acids is of some technical importance. In the Reed reaction, sulfur dioxide, chlorine and light convert hydrocarbons to sulfonyl chlorides. Nucleophilic Abstraction can be used to separate an alkane from a metal. Alkyl groups can be transferred from one compound to another by transmetalation reactions.\n\nAlkanes form a small portion of the atmospheres of the outer gas planets such as Jupiter (0.1% methane, 2 ppm ethane), Saturn (0.2% methane, 5 ppm ethane), Uranus (1.99% methane, 2.5 ppm ethane) and Neptune (1.5% methane, 1.5 ppm ethane). Titan (1.6% methane), a satellite of Saturn, was examined by the \"Huygens\" probe, which indicated that Titan's atmosphere periodically rains liquid methane onto the moon's surface. Also on Titan the Cassini mission has imaged seasonal methane/ethane lakes near the polar regions of Titan. Methane and ethane have also been detected in the tail of the comet Hyakutake. Chemical analysis showed that the abundances of ethane and methane were roughly equal, which is thought to imply that its ices formed in interstellar space, away from the Sun, which would have evaporated these volatile molecules. Alkanes have also been detected in meteorites such as carbonaceous chondrites.\n\nTraces of methane gas (about 0.0002% or 1745 ppb) occur in the Earth's atmosphere, produced primarily by methanogenic microorganisms, such as Archaea in the gut of ruminants.\n\nThe most important commercial sources for alkanes are natural gas and oil. Natural gas contains primarily methane and ethane, with some propane and butane: oil is a mixture of liquid alkanes and other hydrocarbons. These hydrocarbons were formed when marine animals and plants (zooplankton and phytoplankton) died and sank to the bottom of ancient seas and were covered with sediments in an anoxic environment and converted over many millions of years at high temperatures and high pressure to their current form. Natural gas resulted thereby for example from the following reaction:\n\nThese hydrocarbon deposits, collected in porous rocks trapped beneath impermeable cap rocks, comprise commercial oil fields. They have formed over millions of years and once exhausted cannot be readily replaced. The depletion of these hydrocarbons reserves is the basis for what is known as the energy crisis.\n\nMethane is also present in what is called biogas, produced by animals and decaying matter, which is a possible renewable energy source.\n\nAlkanes have a low solubility in water, so the content in the oceans is negligible; however, at high pressures and low temperatures (such as at the bottom of the oceans), methane can co-crystallize with water to form a solid methane clathrate (methane hydrate). Although this cannot be commercially exploited at the present time, the amount of combustible energy of the known methane clathrate fields exceeds the energy content of all the natural gas and oil deposits put together. Methane extracted from methane clathrate is, therefore, a candidate for future fuels.\n\nAcyclic alkanes occur in nature in various ways.\n\n\nCertain types of bacteria can metabolize alkanes: they prefer even-numbered carbon chains as they are easier to degrade than odd-numbered chains.\n\nOn the other hand, certain archaea, the methanogens, produce large quantities of methane by the metabolism of carbon dioxide or other oxidized organic compounds. The energy is released by the oxidation of hydrogen:\n\nMethanogens are also the producers of marsh gas in wetlands, and release about two billion tonnes of methane per year—the atmospheric content of this gas is produced nearly exclusively by them. The methane output of cattle and other herbivores, which can release up to 150 liters per day, and of termites, is also due to methanogens. They also produce this simplest of all alkanes in the intestines of humans. Methanogenic archaea are, hence, at the end of the carbon cycle, with carbon being released back into the atmosphere after having been fixed by photosynthesis. It is probable that our current deposits of natural gas were formed in a similar way.\n\nAlkanes also play a role, if a minor role, in the biology of the three eukaryotic groups of organisms: fungi, plants and animals. Some specialized yeasts, e.g., \"Candida tropicale\", \"Pichia\" sp., \"Rhodotorula\" sp., can use alkanes as a source of carbon or energy. The fungus \"Amorphotheca resinae\" prefers the longer-chain alkanes in aviation fuel, and can cause serious problems for aircraft in tropical regions.\n\nIn plants, the solid long-chain alkanes are found in the plant cuticle and epicuticular wax of many species, but are only rarely major constituents. They protect the plant against water loss, prevent the leaching of important minerals by the rain, and protect against bacteria, fungi, and harmful insects. The carbon chains in plant alkanes are usually odd-numbered, between 27 and 33 carbon atoms in length and are made by the plants by decarboxylation of even-numbered fatty acids. The exact composition of the layer of wax is not only species-dependent but changes also with the season and such environmental factors as lighting conditions, temperature or humidity.\n\nMore volatile short-chain alkanes are also produced by and found in plant tissues. The Jeffrey pine is noted for producing exceptionally high levels of \"n\"-heptane in its resin, for which reason its distillate was designated as the zero point for one octane rating. Floral scents have also long been known to contain volatile alkane components, and \"n\"-nonane is a significant component in the scent of some roses. Emission of gaseous and volatile alkanes such as ethane, pentane, and hexane by plants has also been documented at low levels, though they are not generally considered to be a major component of biogenic air pollution.\n\nEdible vegetable oils also typically contain small fractions of biogenic alkanes with a wide spectrum of carbon numbers, mainly 8 to 35, usually peaking in the low to upper 20s, with concentrations up to dozens of milligrams per kilogram (parts per million by weight) and sometimes over a hundred for the total alkane fraction.\n\nAlkanes are found in animal products, although they are less important than unsaturated hydrocarbons. One example is the shark liver oil, which is approximately 14% pristane (2,6,10,14-tetramethylpentadecane, CH). They are important as pheromones, chemical messenger materials, on which insects depend for communication. In some species, e.g. the support beetle \"Xylotrechus colonus\", pentacosane (CH), 3-methylpentaicosane (CH) and 9-methylpentaicosane (CH) are transferred by body contact. With others like the tsetse fly \"Glossina morsitans morsitans\", the pheromone contains the four alkanes 2-methylheptadecane (CH), 17,21-dimethylheptatriacontane (CH), 15,19-dimethylheptatriacontane (CH) and 15,19,23-trimethylheptatriacontane (CH), and acts by smell over longer distances. Waggle-dancing honey bees produce and release two alkanes, tricosane and pentacosane.\n\nOne example, in which both plant and animal alkanes play a role, is the ecological relationship between the sand bee (\"Andrena nigroaenea\") and the early spider orchid (\"Ophrys sphegodes\"); the latter is dependent for pollination on the former. Sand bees use pheromones in order to identify a mate; in the case of \"A. nigroaenea\", the females emit a mixture of tricosane (CH), pentacosane (CH) and heptacosane (CH) in the ratio 3:3:1, and males are attracted by specifically this odor. The orchid takes advantage of this mating arrangement to get the male bee to collect and disseminate its pollen; parts of its flower not only resemble the appearance of sand bees but also produce large quantities of the three alkanes in the same ratio as female sand bees. As a result, numerous males are lured to the blooms and attempt to copulate with their imaginary partner: although this endeavor is not crowned with success for the bee, it allows the orchid to transfer its pollen,\nwhich will be dispersed after the departure of the frustrated male to different blooms.\n\nAs stated earlier, the most important source of alkanes is natural gas and crude oil. Alkanes are separated in an oil refinery by fractional distillation and processed into many different products.\n\nThe Fischer–Tropsch process is a method to synthesize liquid hydrocarbons, including alkanes, from carbon monoxide and hydrogen. This method is used to produce substitutes for petroleum distillates.\n\nThere is usually little need for alkanes to be synthesized in the laboratory, since they are usually commercially available. Also, alkanes are generally unreactive chemically or biologically, and do not undergo functional group interconversions cleanly. When alkanes are produced in the laboratory, it is often a side-product of a reaction. For example, the use of \"n\"-butyllithium as a strong base gives the conjugate acid, \"n\"-butane as a side-product:\n\nHowever, at times it may be desirable to make a section of a molecule into an alkane-like functionality (alkyl group) using the above or similar methods. For example, an ethyl group is an alkyl group; when this is attached to a hydroxy group, it gives ethanol, which is not an alkane. To do so, the best-known methods are hydrogenation of alkenes:\n\nAlkanes or alkyl groups can also be prepared directly from alkyl halides in the Corey–House–Posner–Whitesides reaction. The Barton–McCombie deoxygenation removes hydroxyl groups from alcohols e.g.\n\nand the Clemmensen reduction removes carbonyl groups from aldehydes and ketones to form alkanes or alkyl-substituted compounds e.g.:\n\nThe applications of alkanes depend on the number of carbon atoms. The first four alkanes are used mainly for heating and cooking purposes, and in some countries for electricity generation. Methane and ethane are the main components of natural gas; they are normally stored as gases under pressure. It is, however, easier to transport them as liquids: This requires both compression and cooling of the gas.\n\nPropane and butane are gases at atmospheric pressure that can be liquefied at fairly low pressures and are commonly known as liquified petroleum gas (LPG). Propane is used in propane gas burners and as a fuel for road vehicles, butane in space heaters and disposable cigarette lighters. Both are used as propellants in aerosol sprays.\n\nFrom pentane to octane the alkanes are highly volatile liquids. They are used as fuels in internal combustion engines, as they vaporize easily on entry into the combustion chamber without forming droplets, which would impair the uniformity of the combustion. Branched-chain alkanes are preferred as they are much less prone to premature ignition, which causes knocking, than their straight-chain homologues. This propensity to premature ignition is measured by the octane rating of the fuel, where 2,2,4-trimethylpentane (\"isooctane\") has an arbitrary value of 100, and heptane has a value of zero. Apart from their use as fuels, the middle alkanes are also good solvents for nonpolar substances.\n\nAlkanes from nonane to, for instance, hexadecane (an alkane with sixteen carbon atoms) are liquids of higher viscosity, less and less suitable for use in gasoline. They form instead the major part of diesel and aviation fuel. Diesel fuels are characterized by their cetane number, cetane being an old name for hexadecane. However, the higher melting points of these alkanes can cause problems at low temperatures and in polar regions, where the fuel becomes too thick to flow correctly.\n\nAlkanes from hexadecane upwards form the most important components of fuel oil and lubricating oil. In the latter function, they work at the same time as anti-corrosive agents, as their hydrophobic nature means that water cannot reach the metal surface. Many solid alkanes find use as paraffin wax, for example, in candles. This should not be confused however with true wax, which consists primarily of esters.\n\nAlkanes with a chain length of approximately 35 or more carbon atoms are found in bitumen, used, for example, in road surfacing. However, the higher alkanes have little value and are usually split into lower alkanes by cracking.\n\nSome synthetic polymers such as polyethylene and polypropylene are alkanes with chains containing hundreds of thousands of carbon atoms. These materials are used in innumerable applications, and billions of kilograms of these materials are made and used each year.\n\nAlkanes are chemically very inert apolar molecules which are not very reactive as organic compounds. This inertness yields serious ecological issues if they are released into the environment. Due to their lack of functional groups and low water solubility alkanes show poor bioavailability for microorganisms.\n\nThere are however some microorganisms possessing the metabolic capacity to utilize n-alkanes as both carbon and energy sources. Some bacterial species are highly specialised in degrading alkanes, these are referred to as hydrocarbonoclastic bacteria. \n\nMethane is explosive when mixed with air (1–8% CH) and other lower alkanes can also form explosive mixtures with air. The lighter liquid alkanes are highly flammable, although this risk decreases with the length of the carbon chain. Pentane, hexane, heptane, and octane are classed as \"dangerous for the environment\" and \"harmful\".\n\nConsiderations for detection or risk control:\n\n\n"}
{"id": "640", "url": "https://en.wikipedia.org/wiki?curid=640", "title": "Appellate procedure in the United States", "text": "Appellate procedure in the United States\n\nUnited States appellate procedure involves the rules and regulations for filing appeals in state courts and federal courts. The nature of an appeal can vary greatly depending on the type of case and the rules of the court in the jurisdiction where the case was prosecuted. There are many types of standard of review for appeals, such as \"de novo\" and abuse of discretion. However, most appeals begin when a party files a petition for review to a higher court for the purpose of overturning the lower court's decision.\n\nAn appellate court is a court that hears cases on appeal from another court. Depending on the particular legal rules that apply to each circumstance, a party to a court case who is unhappy with the result might be able to challenge that result in an appellate court on specific grounds. These grounds typically could include errors of law, fact, procedure or due process. In different jurisdictions, appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts.\n\nThe specific procedures for appealing, including even whether there is a right of appeal from a particular type of decision, can vary greatly from state to state. The right to file an appeal can also vary from state to state; for example, the New Jersey Constitution vests judicial power in a Supreme Court, a Superior Court, and other courts of limited jurisdiction, with an appellate court being part of the Superior Court.\n\nA party who files an appeal is called an \"appellant\", \"plaintiff in error\", \"petitioner\" or \"pursuer\", and a party on the other side is called a \"appellee\". A \"cross-appeal\" is an appeal brought by the respondent. For example, suppose at trial the judge found for the plaintiff and ordered the defendant to pay $50,000. If the defendant files an appeal arguing that he should not have to pay any money, then the plaintiff might file a cross-appeal arguing that the defendant should have to pay $200,000 instead of $50,000.\n\nThe appellant is the party who, having lost part or all their claim in a lower court decision, is appealing to a higher court to have their case reconsidered. This is usually done on the basis that the lower court judge erred in the application of law, but it may also be possible to appeal on the basis of court misconduct, or that a finding of fact was entirely unreasonable to make on the evidence.\n\nThe appellant in the new case can be either the plaintiff (or claimant), defendant, third-party intervenor, or respondent (appellee) from the lower case, depending on who was the losing party. The winning party from the lower court, however, is now the respondent. In unusual cases the appellant can be the victor in the court below, but still appeal.\n\nAn appellee is the party to an appeal in which the lower court judgment was in its favor. The appellee is required to respond to the petition, oral arguments, and legal briefs of the appellant. In general, the appellee takes the procedural posture that the lower court's decision should be affirmed.\n\nAn appeal \"as of right\" is one that is guaranteed by statute or some underlying constitutional or legal principle. The appellate court cannot refuse to listen to the appeal. An appeal \"by leave\" or \"permission\" requires the appellant to obtain leave to appeal; in such a situation either or both of the lower court and the court may have the discretion to grant or refuse the appellant's demand to appeal the lower court's decision. In the Supreme Court, review in most cases is available only if the Court exercises its discretion and grants a writ of certiorari.\n\nIn tort, equity, or other civil matters either party to a previous case may file an appeal. In criminal matters, however, the state or prosecution generally has no appeal \"as of right\". And due to the double jeopardy principle, the state or prosecution may never appeal a jury or bench verdict of acquittal. But in some jurisdictions, the state or prosecution may appeal \"as of right\" from a trial court's dismissal of an indictment in whole or in part or from a trial court's granting of a defendant's suppression motion. Likewise, in some jurisdictions, the state or prosecution may appeal an issue of law \"by leave\" from the trial court or the appellate court. The ability of the prosecution to appeal a decision in favor of a defendant varies significantly internationally. All parties must present grounds to appeal, or it will not be heard.\n\nBy convention in some law reports, the appellant is named first. This can mean that where it is the defendant who appeals, the name of the case in the law reports reverses (in some cases twice) as the appeals work their way up the court hierarchy. This is not always true, however. In the federal courts, the parties' names always stay in the same order as the lower court when an appeal is taken to the circuit courts of appeals, and are re-ordered only if the appeal reaches the Supreme Court.\n\nMany jurisdictions recognize two types of appeals, particularly in the criminal context. The first is the traditional \"direct\" appeal in which the appellant files an appeal with the next higher court of review. The second is the collateral appeal or post-conviction petition, in which the petitioner-appellant files the appeal in a court of first instance—usually the court that tried the case.\n\nThe key distinguishing factor between direct and collateral appeals is that the former occurs in state courts, and the latter in federal courts.\n\nRelief in post-conviction is rare and is most often found in capital or violent felony cases. The typical scenario involves an incarcerated defendant locating DNA evidence demonstrating the defendant's actual innocence.\n\n\"Appellate review\" is the general term for the process by which courts with appellate jurisdiction take jurisdiction of matters decided by lower courts. It is distinguished from judicial review, which refers to the court's overriding constitutional or statutory right to determine if a legislative act or administrative decision is defective for jurisdictional or other reasons (which may vary by jurisdiction).\n\nIn most jurisdictions the normal and preferred way of seeking appellate review is by filing an appeal of the final judgment. Generally, an appeal of the judgment will also allow appeal of all other orders or rulings made by the trial court in the course of the case. This is because such orders cannot be appealed \"as of right\". However, certain critical interlocutory court orders, such as the denial of a request for an interim injunction, or an order holding a person in contempt of court, can be appealed immediately although the case may otherwise not have been fully disposed of.\n\nThere are two distinct forms of appellate review, \"direct\" and \"collateral\". For example, a criminal defendant may be convicted in state court, and lose on \"direct appeal\" to higher state appellate courts, and if unsuccessful, mount a \"collateral\" action such as filing for a writ of habeas corpus in the federal courts. Generally speaking, \"[d]irect appeal statutes afford defendants the opportunity to challenge the merits of a judgment and allege errors of law or fact. ... [Collateral review], on the other hand, provide[s] an independent and civil inquiry into the validity of a conviction and sentence, and as such are generally limited to challenges to constitutional, jurisdictional, or other fundamental violations that occurred at trial.\" \"Graham v. Borgen\", 483 F 3d. 475 (7th Cir. 2007) (no. 04-4103) (slip op. at 7) (citation omitted).\n\nIn Anglo-American common law courts, appellate review of lower court decisions may also be obtained by filing a petition for review by prerogative writ in certain cases. There is no corresponding right to a writ in any pure or continental civil law legal systems, though some mixed systems such as Quebec recognize these prerogative writs.\n\nAfter exhausting the first appeal as of right, defendants usually petition the highest state court to review the decision. This appeal is known as a direct appeal. The highest state court, generally known as the Supreme Court, exercises discretion over whether it will review the case. On direct appeal, a prisoner challenges the grounds of the conviction based on an error that occurred at trial or some other stage in the adjudicative process.\n\nAn appellant's claim(s) must usually be preserved at trial. This means that the defendant had to object to the error when it occurred in the trial. Because constitutional claims are of great magnitude, appellate courts might be more lenient to review the claim even if it was not preserved. For example, Connecticut applies the following standard to review unpreserved claims: 1.the record is adequate to review the alleged claim of error; 2. the claim is of constitutional magnitude alleging the violation of a fundamental right; 3. the alleged constitutional violation clearly exists and clearly deprived the defendant of a fair trial; 4. if subject to harmless error analysis, the state has failed to demonstrate harmlessness of the alleged constitutional violation beyond a reasonable doubt.\n\nAll States have a post-conviction relief process. Similar to federal post-conviction relief, an appellant can petition the court to correct alleged fundamental errors that were not corrected on direct review. Typical claims might include ineffective assistance of counsel and actual innocence based on new evidence. These proceedings are normally separate from the direct appeal, however some states allow for collateral relief to be sought on direct appeal. After direct appeal, the conviction is considered final. An appeal from the post conviction court proceeds just as a direct appeal. That is, it goes to the intermediate appellate court, followed by the highest court. If the petition is granted the appellant could be released from incarceration, the sentence could be modified, or a new trial could be ordered.\n\nA \"notice of appeal\" is a form or document that in many cases is required to begin an appeal. The form is completed by the appellant or by the appellant's legal representative. The nature of this form can vary greatly from country to country and from court to court within a country.\n\nThe specific rules of the legal system will dictate exactly how the appeal is officially begun. For example, the appellant might have to file the notice of appeal with the appellate court, or with the court from which the appeal is taken, or both.\n\nSome courts have samples of a notice of appeal on the court's own web site. In New Jersey, for example, the Administrative Office of the Court has promulgated a form of notice of appeal for use by appellants, though using this exact form is not mandatory and the failure to use it is not a jurisdictional defect provided that all pertinent information is set forth in whatever form of notice of appeal is used.\n\nThe deadline for beginning an appeal can often be very short: traditionally, it is measured in days, not months. This can vary from country to country, as well as within a country, depending on the specific rules in force. In the U.S. federal court system, criminal defendants must file a notice of appeal within 10 days of the entry of either the judgment or the order being appealed, or the right to appeal is forfeited.\n\nGenerally speaking the appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether that decision was legally sound or not. The appellate court will typically be deferential to the lower court's findings of fact (such as whether a defendant committed a particular act), unless clearly erroneous, and so will focus on the court's application of the law to those facts (such as whether the act found by the court to have occurred fits a legal definition at issue).\n\nIf the appellate court finds no defect, it \"affirms\" the judgment. If the appellate court does find a legal defect in the decision \"below\" (i.e., in the lower court), it may \"modify\" the ruling to correct the defect, or it may nullify (\"reverse\" or \"vacate\") the whole decision or any part of it. It may, in addition, send the case back (\"remand\" or \"remit\") to the lower court for further proceedings to remedy the defect.\n\nIn some cases, an appellate court may review a lower court decision \"de novo\" (or completely), challenging even the lower court's findings of fact. This might be the proper standard of review, for example, if the lower court resolved the case by granting a pre-trial motion to dismiss or motion for summary judgment which is usually based only upon written submissions to the trial court and not on any trial testimony.\n\nAnother situation is where appeal is by way of \"re-hearing\". Certain jurisdictions permit certain appeals to cause the trial to be heard afresh in the appellate court.\n\nSometimes, the appellate court finds a defect in the procedure the parties used in filing the appeal and dismisses the appeal without considering its merits, which has the same effect as affirming the judgment below. (This would happen, for example, if the appellant waited too long, under the appellate court's rules, to file the appeal.)\n\nGenerally, there is no trial in an appellate court, only consideration of the record of the evidence presented to the trial court and all the pre-trial and trial court proceedings are reviewed—unless the appeal is by way of re-hearing, new evidence will usually only be considered on appeal in \"very\" rare instances, for example if that material evidence was unavailable to a party for some very significant reason such as prosecutorial misconduct.\n\nIn some systems, an appellate court will only consider the written decision of the lower court, together with any written evidence that was before that court and is relevant to the appeal. In other systems, the appellate court will normally consider the record of the lower court. In those cases the record will first be certified by the lower court.\n\nThe appellant has the opportunity to present arguments for the granting of the appeal and the appellee (or respondent) can present arguments against it. Arguments of the parties to the appeal are presented through their appellate lawyers, if represented, or \"pro se\" if the party has not engaged legal representation. Those arguments are presented in written briefs and sometimes in oral argument to the court at a hearing. At such hearings each party is allowed a brief presentation at which the appellate judges ask questions based on their review of the record below and the submitted briefs.\n\nIn an adversarial system, appellate courts do not have the power to review lower court decisions unless a party appeals it. Therefore, if a lower court has ruled in an improper manner, or against legal precedent, that judgment will stand if not appealed – even if it might have been overturned on appeal.\n\nThe United States legal system generally recognizes two types of appeals: a trial \"de novo\" or an appeal on the record.\n\nA trial de novo is usually available for review of informal proceedings conducted by some minor judicial tribunals in proceedings that do not provide all the procedural attributes of a formal judicial trial. If unchallenged, these decisions have the power to settle more minor legal disputes once and for all. If a party is dissatisfied with the finding of such a tribunal, one generally has the power to request a trial \"de novo\" by a court of record. In such a proceeding, all issues and evidence may be developed newly, as though never heard before, and one is not restricted to the evidence heard in the lower proceeding. Sometimes, however, the decision of the lower proceeding is itself admissible as evidence, thus helping to curb frivolous appeals.\n\nIn some cases, an application for \"trial de novo\" effectively erases the prior trial as if it had never taken place. The Supreme Court of Virginia has stated that '\"This Court has repeatedly held that the effect of an appeal to circuit court is to \"annul the judgment of the inferior tribunal as completely as if there had been no previous trial.\"' The only exception to this is that if a defendant appeals a conviction for a crime having multiple levels of offenses, where they are convicted on a lesser offense, the appeal is of the lesser offense; the conviction represents an acquittal of the more serious offenses. \"[A] trial on the same charges in the circuit court does not violate double jeopardy principles, . . . subject only to the limitation that conviction in [the] district court for an offense lesser included in the one charged constitutes an acquittal of the greater offense,\npermitting trial de novo in the circuit court only for the lesser-included offense.\"\n\nIn an appeal on the record from a decision in a judicial proceeding, both appellant and respondent are bound to base their arguments wholly on the proceedings and body of evidence as they were presented in the lower tribunal. Each seeks to prove to the higher court that the result they desired was the just result. Precedent and case law figure prominently in the arguments. In order for the appeal to succeed, the appellant must prove that the lower court committed reversible error, that is, an impermissible action by the court acted to cause a result that was unjust, and which would not have resulted had the court acted properly. Some examples of reversible error would be erroneously instructing the jury on the law applicable to the case, permitting seriously improper argument by an attorney, admitting or excluding evidence improperly, acting outside the court's jurisdiction, injecting bias into the proceeding or appearing to do so, juror misconduct, etc. The failure to formally object at the time, to what one views as improper action in the lower court, may result in the affirmance of the lower court's judgment on the grounds that one did not \"preserve the issue for appeal\" by objecting.\n\nIn cases where a judge rather than a jury decided issues of fact, an appellate court will apply an \"abuse of discretion\" standard of review. Under this standard, the appellate court gives deference to the lower court's view of the evidence, and reverses its decision only if it were a clear abuse of discretion. This is usually defined as a decision outside the bounds of reasonableness. On the other hand, the appellate court normally gives less deference to a lower court's decision on issues of law, and may reverse if it finds that the lower court applied the wrong legal standard.\n\nIn some cases, an appellant may successfully argue that the law under which the lower decision was rendered was unconstitutional or otherwise invalid, or may convince the higher court to order a new trial on the basis that evidence earlier sought was concealed or only recently discovered. In the case of new evidence, there must be a high probability that its presence or absence would have made a material difference in the trial. Another issue suitable for appeal in criminal cases is effective assistance of counsel. If a defendant has been convicted and can prove that his lawyer did not adequately handle his case and that there is a reasonable probability that the result of the trial would have been different had the lawyer given competent representation, he is entitled to a new trial.\n\nA lawyer traditionally starts an oral argument to any appellate court with the words \"May it please the court.\"\n\nAfter an appeal is heard, the \"mandate\" is a formal notice of a decision by a court of appeal; this notice is transmitted to the trial court and, when filed by the clerk of the trial court, constitutes the final judgment on the case, unless the appeal court has directed further proceedings in the trial court. The mandate is distinguished from the appeal court's opinion, which sets out the legal reasoning for its decision. In some jurisdictions the mandate is known as the \"remittitur\".\n\nThe result of an appeal can be:\n\nThere can be multiple outcomes, so that the reviewing court can affirm some rulings, reverse others and remand the case all at the same time. Remand is not required where there is nothing left to do in the case. \"Generally speaking, an appellate court's judgment provides 'the final directive of the appeals courts as to the matter appealed, setting out with specificity the court's determination that the action appealed from should be affirmed, reversed, remanded or modified'\".\n\nSome reviewing courts who have discretionary review may send a case back without comment other than \"review improvidently granted\". In other words, after looking at the case, they chose not to say anything. The result for the case of \"review improvidently granted\" is effectively the same as affirmed, but without that extra higher court stamp of approval.\n\n"}
{"id": "642", "url": "https://en.wikipedia.org/wiki?curid=642", "title": "Answer (law)", "text": "Answer (law)\n\nIn law, an Answer was originally a solemn assertion in opposition to someone or something, and thus generally any counter-statement or defense, a reply to a question or response, or objection, or a correct solution of a problem.\n\nIn the common law, an Answer is the first pleading by a defendant, usually filed and served upon the plaintiff within a certain strict time limit after a civil complaint or criminal information or indictment has been served upon the defendant. It may have been preceded by an \"optional\" \"pre-answer\" motion to dismiss or demurrer; if such a motion is unsuccessful, the defendant \"must\" file an answer to the complaint or risk an adverse default judgment.\n\nIn a criminal case, there is usually an arraignment or some other kind of appearance before the defendant comes to court. The pleading in the criminal case, which is entered on the record in open court, is usually either guilty or not guilty. Generally speaking in private, civil cases there is no plea entered of guilt or innocence. There is only a judgment that grants money damages or some other kind of equitable remedy such as restitution or a permanent injunction. Criminal cases may lead to fines or other punishment, such as imprisonment.\n\nThe famous Latin \"Responsa Prudentium\" (\"answers of the learned ones\") were the accumulated views of many successive generations of Roman lawyers, a body of legal opinion which gradually became authoritative.\n\nDuring debates of a contentious nature, deflection, colloquially known as 'changing the topic', has been widely observed, and is often seen as a failure to answer a question.\n\n"}
{"id": "643", "url": "https://en.wikipedia.org/wiki?curid=643", "title": "Appellate court", "text": "Appellate court\n\nAn appellate court, commonly called an appeals court, court of appeals (American English), appeal court (British English), court of second instance or second instance court, is any court of law that is empowered to hear an appeal of a trial court or other lower tribunal. In most jurisdictions, the court system is divided into at least three levels: the trial court, which initially hears cases and reviews evidence and testimony to determine the facts of the case; at least one intermediate appellate court; and a supreme court (or court of last resort) which primarily reviews the decisions of the intermediate courts. A jurisdiction's supreme court is that jurisdiction's highest appellate court. Appellate courts nationwide can operate under varying rules.\n\nThe authority of appellate courts to review the decisions of lower courts varies widely from one jurisdiction to another. In some areas the appellate court has limited powers of review. Generally, an appellate court's judgment provides the final directive of the appeals courts as to the matter appealed, setting out with specificity the court's determination that the action appealed from should be affirmed, reversed, remanded or modified.\n\nThe Court of Appeal of New Zealand, located in Wellington, is New Zealand's principal intermediate appellate court. In practice, most appeals are resolved at this intermediate appellate level, rather than in the Supreme Court.\n\nThe Court of Appeal of Sri Lanka is located in Colombo, is the second most senior court in the Sri Lankan legal system.\n\nIn the United States, both state and federal appellate courts are usually restricted to examining whether the lower court made the correct legal determinations, rather than hearing direct evidence and determining what the facts of the case were. Furthermore, U.S. appellate courts are usually restricted to hearing appeals based on matters that were originally brought up before the trial court. Hence, such an appellate court will not consider an appellant's argument if it is based on a theory that is raised for the first time in the appeal.\n\nIn most U.S. states, and in U.S. federal courts, parties before the court are allowed one appeal as of right. This means that a party who is unsatisfied with the outcome of a trial may bring an appeal to contest that outcome. However, appeals may be costly, and the appellate court must find an error on the part of the court below that justifies upsetting the verdict. Therefore, only a small proportion of trial court decisions result in appeals. Some appellate courts, particularly supreme courts, have the power of discretionary review, meaning that they can decide whether they will hear an appeal brought in a particular case.\n\nMany U.S. jurisdictions title their appellate court a court of appeal or court of appeals. Historically, others have titled their appellate court a court of errors (or court of errors and appeals), on the premise that it was intended to correct errors made by lower courts. Examples of such courts include the New Jersey Court of Errors and Appeals (which existed from 1844 to 1947), the Connecticut Supreme Court of Errors (which has been renamed the Connecticut Supreme Court), the Kentucky Court of Errors (renamed the Kentucky Supreme Court), and the Mississippi High Court of Errors and Appeals (since renamed the Supreme Court of Mississippi). In some jurisdictions, courts able to hear appeals are known as an appellate division.\n\nThe phrase \"court of appeals\" most often refers to intermediate appellate courts. However, the New York system is different: the \"New York Court of Appeals\" is the highest appellate court; and the phrase \"New York Supreme Court\" applies to the trial court of general jurisdiction.\n\nDepending on the system, certain courts may serve as both trial courts and appellate courts, hearing appeals of decisions made by courts with more limited jurisdiction. Some jurisdictions have specialized appellate courts, such as the Texas Court of Criminal Appeals, which only hears appeals raised in criminal cases, and the United States Court of Appeals for the Federal Circuit, which has general jurisdiction but derives most of its caseload from patent cases, on one hand, and appeals from the Court of Federal Claims on the other.\n\n\n"}
{"id": "649", "url": "https://en.wikipedia.org/wiki?curid=649", "title": "Arraignment", "text": "Arraignment\n\nArraignment is a formal reading of a criminal charging document in the presence of the defendant to inform the defendant of the charges against him or her. In response to arraignment, the accused is expected to enter a plea. Acceptable pleas vary among jurisdiction but they generally include \"guilty\", \"not guilty\", and the peremptory pleas (or pleas in bar) setting out reasons why a trial cannot proceed. Pleas of \"nolo contendere\" (no contest) and the \"\"Alford\" plea\" are allowed in some circumstances.\n\nIn Australia, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment. The judge will testify during the indictment process.\n\nIn every province in Canada except British Columbia, defendants are arraigned on the day of their trial. In British Columbia, arraignment takes places in one of the first few court appearances by the defendant or their lawyer. The defendant is asked whether he or she pleads guilty or not guilty to each charge.\n\nIn France, the general rule is that one cannot remain in police custody for more than 24 hours from the time of the arrest. However, police custody can last another 24 hours in specific circumstances, especially if the offence is punishable by at least one year's imprisonment, or if the investigation is deemed to require the extra time, and can last up to 96 hours in certain cases involving terrorism, drug trafficking or organised crime. The police needs to have the consent of the prosecutor (in the vast majority of cases, the prosecutor will consent).\n\nIn Germany, if one has been arrested and taken into custody by the police one must be brought before a judge as soon as possible and at the latest on the day after the arrest.\n\nAt the first appearance, the accused is read the charges and asked for a plea. The available pleas are, guilty, not guilty, and no plea. No plea allows the defendant to get legal advice on the plea, which must be made on the second appearance.\n\nIn South Africa, arraignment is defined as the calling upon the accused to appear, the informing of the accused of the crime charged against him, the demanding of the accused whether he be guilty or not guilty, and the entering of his plea. His plea having been entered he is said to stand arraigned.\n\nIn England, Wales, and Northern Ireland, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment.\n\nIn England and Wales, the police cannot legally detain anyone for more than 24 hours without charging them unless an officer with the rank of superintendent (or above) authorises detention for a further 12 hours (36 hours total), or a judge (who will be a magistrate) authorises detention by the police before charge for up to a maximum of 96 hours, but for terrorism-related offences people can be held by the police for up to 28 days before charge. If they are not released after being charged, they should be brought before a court as soon as practicable.\n\nUnder the United States Federal Rules of Criminal Procedure, \"arraignment shall [consist of an] open...reading [of] the indictment...to the defendant...and call[] on him to plead thereto. He/she shall be given a copy of the indictment...before he/she is called upon to plead.\"\n\nIn federal courts, arraignment takes place in two stages. The first is called the initial arraignment and must take place within 48 hours of an individual's arrest, 72 hours if the individual was arrested on the weekend and not able to go before a judge until Monday. During this arraignment the defendant is informed of the pending legal charges and is informed of his or her right to retain counsel. The presiding judge also decides at what amount, if any, to set bail. During the second arraignment, a post-indictment arraignment or PIA, the defendant is allowed to enter a plea.\n\nIn New York, most people arrested must be released if they are not arraigned within 24 hours.\n\nIn California, arraignments must be conducted without unnecessary delay and, in any event, within 48 hours of arrest, excluding weekends and holidays. Thus, an individual arrested without a warrant, in some cases, may be held for as long as 168 hours (7 days) without arraignment or charge.\n\nThe wording of the arraignment varies from jurisdiction to jurisdiction. However, it generally conforms with the following principles:\n\nVideo arraignment is the act of conducting the arraignment process using some form of videoconferencing technology. Use of video arraignment system allows the courts to conduct the requisite arraignment process without the need to transport the defendant to the courtroom by using an audio-visual link between the location where the defendant is being held and the courtroom.\n\nUse of the video arraignment process addresses the problems associated with having to transport defendants. The transportation of defendants requires time, puts additional demands on the public safety organizations to provide for the safety of the public, court personnel and for the security of the population held in detention. It also addresses the rising costs of transportation.\n\nIf the defendant pleads guilty, an evidentiary hearing usually follows. The court is not required to accept a guilty plea. During the hearing, the judge assesses the offense, the mitigating factors, and the defendant's character, and passes sentence.\n\nIf the defendant pleads not guilty, a date is set for a preliminary hearing or a trial.\n\nIn the past, a defendant who refused to plead (or \"stood mute\") was subject to peine forte et dure (Law French for \"strong and hard punishment\"). Today in common-law jurisdictions, the court enters a plea of not guilty for a defendant who refuses to enter a plea. The rationale for this is the defendant's right to silence.\n\nThis is also often the stage at which arguments for or against pre-trial release and bail may be made, depending on the alleged crime and jurisdiction.\n\n"}
{"id": "651", "url": "https://en.wikipedia.org/wiki?curid=651", "title": "America the Beautiful", "text": "America the Beautiful\n\n\"America the Beautiful\" is an American patriotic song. The lyrics were written by Katharine Lee Bates, and the music was composed by church organist and choirmaster Samuel A. Ward at Grace Episcopal Church in Newark, New Jersey.\n\nBates originally wrote the words as a poem, \"Pikes Peak\", first published in the Fourth of July edition of the church periodical \"The Congregationalist\" in 1895. At that time, the poem was titled \"America\" for publication.\n\nWard had originally written the music, \"Materna\", for the hymn \"O Mother dear, Jerusalem\" in 1882, though it was not first published until 1892. Ward's music combined with the Bates poem was first published in 1910 and titled \"America the Beautiful\".\n\nThe song is one of the most popular of the many American patriotic songs.\n\nIn 1893, at the age of 33, Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach a short summer school session at Colorado College. Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the \"White City\" with its promise of the future contained within its alabaster buildings; the wheat fields of America's heartland Kansas, through which her train was riding on July 16; and the majestic view of the Great Plains from high atop Pikes Peak.\n\nOn the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel. The poem was initially published two years later in \"The Congregationalist\" to commemorate the Fourth of July. It quickly caught the public's fancy. Amended versions were published in 1904 and 1911.\n\nSeveral existing pieces of music were adapted to the poem. A hymn tune composed by Samuel A. Ward was generally considered the best music as early as 1910 and is still the popular tune today. Just as Bates had been inspired to write her poem, Ward, too, was inspired to compose his tune. The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City, after a leisurely summer day in 1882, and he immediately wrote it down. Supposedly, he was so anxious to capture the tune in his head, he asked fellow passenger friend Harry Martin for his shirt cuff to write the tune on. He composed the tune for the old hymn \"O Mother Dear, Jerusalem\", retitling the work \"Materna\". Ward's music combined with Bates's poem were first published together in 1910 and titled \"America the Beautiful\".\n\nWard died in 1903, not knowing the national stature his music would attain since the music was only first applied to the song in 1904. Bates was more fortunate since the song's popularity was well established by the time of her death in 1929.\n\nAt various times in the more than 100 years that have elapsed since the song was written, particularly during the John F. Kennedy administration, there have been efforts to give \"America the Beautiful\" legal status either as a national hymn or as a national anthem equal to, or in place of, \"The Star-Spangled Banner\", but so far this has not succeeded. Proponents prefer \"America the Beautiful\" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as \"The Star-Spangled Banner\". Some prefer \"America the Beautiful\" over \"The Star-Spangled Banner\" due to the latter's war-oriented imagery. Others prefer \"The Star-Spangled Banner\" for the same reason. While that national dichotomy has stymied any effort at changing the tradition of the national anthem, \"America the Beautiful\" continues to be held in high esteem by a large number of Americans.\n\nThis song was used as the background music of the television broadcast of the Tiangong-1 launch.\n\nThe song is often included in songbooks in a wide variety of religious congregations in the United States.\nIn 1976, while the United States celebrated its bicentennial, a soulful version popularized by Ray Charles peaked at number 98 on the US R&B Charts, and is included on the soundtrack for the movie \"The Sandlot\".\n\nThree different renditions of the song have entered the Hot Country Songs charts. The first was by Charlie Rich, which went to number 22 in 1976. A second, by Mickey Newbury, peaked at number 82 in 1980. An all-star version of \"America the Beautiful\" performed by country singers Trace Adkins, Sherrié Austin, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Lyle Lovett, Lila McCann, Lorrie Morgan, Jamie O'Neal, The Oak Ridge Boys, Collin Raye, Kenny Rogers, Keith Urban and Phil Vassar reached number 58 in July 2001. The song re-entered the chart following the September 11 attacks.\n\nA punk rock adaptation of the song was recorded in 1976 by New York band The Dictators, and released on their album Every Day is Saturday.\n\nPopularity of the song increased greatly following the September 11 attacks; at some sporting events it was sung in addition to the traditional singing of the national anthem. During the first taping of the \"Late Show with David Letterman\" following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse.\n\nFor Super Bowl XLVIII, The Coca-Cola Company aired a multilingual version of the song, sung in several different languages. The commercial received some criticism on social media sites, such as Twitter and Facebook, and from some conservatives, such as Glenn Beck. Despite the controversies, Coca-Cola later reused the Super Bowl ad during Super Bowl LI, the opening ceremonies of the 2014 Winter Olympics and 2016 Summer Olympics and in patriotic holidays.\n\n\"From sea to shining sea\", originally used in the charters of some of the English Colonies in North America, is an American idiom meaning \"from the Atlantic Ocean to the Pacific Ocean\" (or vice versa). Other songs that have used this phrase include the American patriotic song \"God Bless the USA\" and Schoolhouse Rock's \"Elbow Room\". The phrase and the song are also the namesake of the Shining Sea Bikeway, a bike path in Bates's hometown of Falmouth, Massachusetts. The phrase is similar to the Latin phrase \"\"\"\" (\"From sea to sea\"), which serves as the official motto of Canada.\n\n\"Purple mountain majesties\" refers to the shade of the Pikes Peak in Colorado Springs, Colorado, which inspired Bates to write the poem.\n\nLynn Sherr's 2001 book \"America the Beautiful\" discusses the origins of the song and the backgrounds of its authors in depth. The book points out that the poem has the same meter as that of \"Auld Lang Syne\"; the songs can be sung interchangeably. Additionally, Sherr discusses the evolution of the lyrics, for instance, changes to the original third verse written by Bates. The song appears in Ellen Raskin's \"The Westing Game\".\n\n\n"}
{"id": "653", "url": "https://en.wikipedia.org/wiki?curid=653", "title": "Assistive technology", "text": "Assistive technology\n\nAssistive technology is an umbrella term that includes assistive, adaptive, and rehabilitative devices for people with disabilities and also includes the process used in selecting, locating, and using them. Assistive technology promotes greater independence by enabling people to perform tasks that they were formerly unable to accomplish, or had great difficulty accomplishing, by providing enhancements to, or changing methods of interacting with, the technology needed to accomplish such tasks.\n\nThe term adaptive technology is often used as the synonym for assistive technology; however, they are different terms. Assistive technology refers to \"any item, piece of equipment, or product system, whether acquired commercially, modified, or customized, that is used to increase, maintain, or improve functional capabilities of individuals with disabilities\", while adaptive technology covers items that are specifically designed for persons with disabilities and would seldom be used by non-disabled persons. In other words, \"assistive technology is any object or system that increases or maintains the capabilities of people with disabilities,\" while adaptive technology is \"any object or system that is specifically designed for the purpose of increasing or maintaining the capabilities of people with disabilities.\" Consequently, adaptive technology is a subset of assistive technology. Adaptive technology often refers specifically to electronic and information technology access.\n\nWheelchairs are devices that can be manually propelled or electrically propelled and that include a seating system and are designed to be a substitute for the normal mobility that most people enjoy. Wheelchairs and other mobility devices allow people to perform mobility related activities of daily living which include feeding, toileting, dressing, grooming, and bathing. The devices comes in a number of variations where they can be propelled either by hand or by motors where the occupant uses electrical controls to manage motors and seating control actuators through a joystick, sip-and-puff control, or other input devices. Often there are handles behind the seat for someone else to do the pushing or input devices for caregivers. Wheelchairs are used by people for whom walking is difficult or impossible due to illness, injury, or disability. People with both sitting and walking disability often need to use a wheelchair or walker.\n\nPatient transfer devices generally allow patients with impaired mobility to be moved by caregivers between beds, wheelchairs, commodes, toilets, chairs, stretchers, shower benches, automobiles, swimming pools, and other patient support systems (i.e., radiology, surgical, or examining tables). The most common devices are Patient lifts (for vertical transfer), Transfer benches, stretcher or convertible chairs (for lateral, supine transfer), sit-to-stand lifts (for moving patients from one seated position to another i.e., from wheelchairs to commodes), air bearing inflatable mattresses (for supine transfer i.e., transfer from a gurney to an operating room table), and sliding boards (usually used for transfer from a bed to a wheelchair). Highly dependent patients who cannot assist their caregiver in moving them often require a Patient lift (a floor or ceiling-suspended sling lift) which though invented in 1955 and in common use since the early 1960s is still considered the state-of-the-art transfer device by OSHA and the American Nursing Association.\n\nA walker or walking frame or Rollator is a tool for disabled people who need additional support to maintain balance or stability while walking. It consists of a frame that is about waist high, approximately twelve inches deep and slightly wider than the user. Walkers are also available in other sizes, such as for children, or for heavy people. Modern walkers are height-adjustable. The front two legs of the walker may or may not have wheels attached depending on the strength and abilities of the person using it. It is also common to see caster wheels or glides on the back legs of a walker with wheels on the front.\n\nA prosthesis, prosthetic, or prosthetic limb is a device that replaces a missing body part. It is part of the field of biomechatronics, the science of using mechanical devices with human muscle, skeleton, and nervous systems to assist or enhance motor control lost by trauma, disease, or defect. Prostheses are typically used to replace parts lost by injury (traumatic) or missing from birth (congenital) or to supplement defective body parts. Inside the body, artificial heart valves are in common use with artificial hearts and lungs seeing less common use but under active technology development. Other medical devices and aids that can be considered prosthetics include hearing aids, artificial eyes, palatal obturator, gastric bands, and dentures.\n\nProstheses are specifically \"not\" orthoses, although given certain circumstances a prosthesis might end up performing some or all of the same functionary benefits as an orthosis. Prostheses are technically the complete finished item. For instance, a C-Leg knee alone is \"not\" a prosthesis, but only a prosthetic \"component\". The complete prosthesis would consist of the attachment system  to the residual limb — usually a \"socket\", and all the attachment hardware components all the way down to and including the terminal device. Keep this in mind as nomenclature is often interchanged.\n\nThe terms \"prosthetic\" and \"orthotic\" are adjectives used to describe devices such as a prosthetic knee. The terms \"prosthetics\" and \"orthotics\" are used to describe the respective allied health fields. The devices themselves are properly referred to as \"prostheses\" and \"orthoses\" in the plural and \"prosthesis\" and \"orthosis\" in the singular.\n\nMany people with serious visual impairments live independently, using a wide range of tools and techniques. Examples of assistive technology for visually impairment include screen readers, screen magnifiers, Braille embossers, desktop video magnifiers, and voice recorders.\n\nScreen readers allow the visually impaired to easily access electronic information. These software programs connect to a computer to read the text displayed out loud. There are a variety of platforms and applications available for a variety of costs.\n\nBraille is a system of raised dots formed into units called braille cells. A full braille cell is made up of six dots, with two parallel rows of three dots, but other combinations and quantities of dots represent other letters, numbers, punctuation marks, or words. People can then use their fingers to read the code of raised dots.\n\nA braille embosser is, simply put, a printer for braille. Instead of a standard printer adding ink onto a page, the braille embosser imprints the raised dots of braille onto a page. Some braille embossers combine both braille and ink so the documents can be read with either sight or touch.\n\nDesktop video magnifiers are electronic devices that use a camera and a display screen to perform digital magnification of printed materials. They enlarge printed pages for those with low vision. A camera connects to a monitor that displays real time images, and the user can control settings such as magnification, focus, contrast, underlining, highlighting, and other screen preferences. They come in a variety of sizes and styles; some are small and portable with handheld cameras, while others are much larger and mounted on a fixed stand.\n\nA screen magnifier is software that interfaces with a computer's graphical output to present enlarged screen content. It allows users to enlarge the texts and graphics on their computer screens for easier viewing. Similar to desktop video magnifiers, this technology assists people with low vision. After the user loads the software into their computer's memory, it serves as a kind of \"computer magnifying glass.\" Wherever the computer cursor moves, it enlarges the area around it. This allows greater computer accessibility for a wide range of visual abilities.\n\nA large-print keyboard has large letters printed on the keys. On the keyboard shown, the round buttons at the top control software which can magnify the screen (zoom in), change the background color of the screen, or make the mouse cursor on the screen larger. The \"bump dots\" on the keys, installed in this case by the organization using the keyboards, help the user find the right keys in a tactile way.\n\nAssistive technology for navigation has exploded on the IEEE Xplore database since 2000, with over 7,500 engineering articles written on assistive technologies and visual impairment in the past 25 years, and over 1,300 articles on solving the problem of navigation for people who are blind or visually impaired. As well, over 600 articles on augmented reality and visual impairment have appeared in the engineering literature since 2000. Most of these articles were published within the past 5 years, and the number of articles in this area is increasing every year. GPS, accelerometers, gyroscopes, and cameras can pinpoint the exact location of the user and provide information on what’s in the immediate vicinity, and assistance in getting to a destination.\n\nSome navigation assistants (for both indoors and outdoors) include:\n\nPersonal emergency response systems (PERS), or Telecare (UK term), are a particular sort of assistive technology that use electronic sensors connected to an alarm system to help caregivers manage risk and help vulnerable people stay independent at home longer. An example would be the systems being put in place for senior people such as fall detectors, thermometers (for hypothermia risk), flooding and unlit gas sensors (for people with mild dementia). Notably, these alerts can be customized to the particular person's risks. When the alert is triggered, a message is sent to a caregiver or contact center who can respond appropriately.\n\nIn human–computer interaction, computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability or severity of impairment, examples include web accessibility guidelines. Another approach is for the user to present a token to the computer terminal, such as a smart card, that has configuration information to adjust the computer speed, text size, etc. to their particular needs. This is useful where users want to access public computer based terminals in Libraries, ATM, Information kiosks etc. The concept is encompassed by the CEN EN 1332-4 Identification Card Systems - Man-Machine Interface. This development of this standard has been supported in Europe by SNAPI and has been successfully incorporated into the Lasseo specifications, but with limited success due to the lack of interest from public computer terminal suppliers.\n\nThe deaf or hard of hearing community has a difficult time to communicate and perceive information as compared to hearing individuals. Thus, these individuals often rely on visual and tactile mediums for receiving and communicating information. The use of assistive technology and devices provides this community with various solutions to their problems by providing higher sound (for those who are hard of hearing), tactile feedback, visual cues and improved technology access. Individuals who are deaf or hard of hearing utilize a variety of assistive technologies that provide them with improved accessibility to information in numerous environments. Most devices either provide amplified sound or alternate ways to access information through vision and/or vibration. These technologies can be grouped into three general categories: Hearing Technology, alerting devices, and communication support.\n\nA hearing aid or deaf aid is an electroacoustic device which is designed to amplify sound for the wearer, usually with the aim of making speech more intelligible, and to correct impaired hearing as measured by audiometry. This type of assistive technology helps people with hearing loss participate more fully in their communities by allowing them to hear more clearly. They amplify any and all sound waves through use of a microphone, amplifier, and speaker. There is a wide variety of hearing aids available, including digital, in-the-ear, in-the-canal, behind-the-ear, and on-the-body aids.\n\nAssistive listening devices include FM, infrared, and loop assistive listening devices. This type of technology allows people with hearing difficulties to focus on a speaker or subject by getting rid of extra background noises and distractions, making places like auditoriums, classrooms, and meetings much easier to participate in. The assistive listening device usually uses a microphone to capture an audio source near to its origin and broadcast it wirelessly over an FM (Frequency Modulation) transmission, IR (Infra Red) transmission, IL (Induction Loop) transmission, or other transmission method. The person who is listening may use an FM/IR/IL Receiver to tune into the signal and listen at his/her preferred volume.\n\nThis type of assistive technology allows users to amplify the volume and clarity of their phone calls so that they can easily partake in this medium of communication. There are also options to adjust the frequency and tone of a call to suit their individual hearing needs. Additionally, there is a wide variety of amplified telephones to choose from, with different degrees of amplification. For example, a phone with 26 to 40 decibel is generally sufficient for mild hearing loss, while a phone with 71 to 90 decibel is better for more severe hearing loss.\n\nAugmentative and alternative communication (AAC) is an umbrella term that encompasses methods of communication for those with impairments or restrictions on the production or comprehension of spoken or written language. AAC systems are extremely diverse and depend on the capabilities of the user. They may be as basic as pictures on a board that are used to request food, drink, or other care; or they can be advanced speech generating devices, based on speech synthesis, that are capable of storing hundreds of phrases and words.\n\nAssistive Technology for Cognition (ATC) is the use of technology (usually high tech) to augment and assist cognitive processes such as attention, memory, self-regulation, navigation, emotion recognition and management, planning, and sequencing activity. Systematic reviews of the field have found that the number of ATC are growing rapidly, but have focused on memory and planning, that there is emerging evidence for efficacy, that a lot of scope exists to develop new ATC. Examples of ATC include: NeuroPage which prompts users about meetings, Wakamaru, which provides companionship and reminds users to take medicine and calls for help if something is wrong, and telephone Reassurance systems.\n\nMemory aids are any type of assistive technology that helps a user learn and remember certain information. Many memory aids are used for cognitive impairments such as reading, writing, or organizational difficulties. For example, a Smartpen records handwritten notes by creating both a digital copy and an audio recording of the text. Users simply tap certain parts of their notes and the pen saves it and reads it back to them. From there, the user can also download their notes onto a computer for increased accessibility. Digital voice recorders are also used to record \"in the moment\" information for fast and easy recall at a later time.\n\nEducational software is software that assists people with reading, learning, comprehension, and organizational difficulties. Any accommodation software such as text readers, notetakers, text enlargers, organization tools, word predictions, and talking word processors falls under the category of educational software.\n\nAssistive technology in sport is an area of technology design that is growing. Assistive technology is the array of new devices created to enable sports enthusiasts who have disabilities to play. Assistive technology may be used in adaptive sports, where an existing sport is modified to enable players with a disability to participate; or, assistive technology may be used to invent completely new sports with athletes with disabilities exclusively in mind.\n\nAn increasing number of people with disabilities are participating in sports, leading to the development of new assistive technology. Assistive technology devices can be simple, or \"low-tech\", or they may use highly advanced technology, with some even using computers. Assistive technology for sports may also be simple, or advanced. Accordingly, assistive technology can be found in sports ranging from local community recreation to the elite Paralympic Games. More complex assistive technology devices have been developed over time, and as a result, sports for people with disabilities \"have changed from being a clinical therapeutic tool to an increasingly competition-oriented activity\".\n\nIn the United States there are two major pieces of legislation that govern the use of assistive technology within the school system. The first is Section 504 of the Rehabilitation Act of 1973 and the second being the Individuals with Disabilities Education Act (IDEA) which was first enacted in 1975 under the name The Education for All Handicapped Children Act. In 2004, during the reauthorization period for IDEA, the National Instructional Material Access Center (NIMAC) was created which provided a repository of accessible text including publisher's textbooks to students with a qualifying disability. Files provided are in XML format and used as a starting platform for braille readers, screen readers, and other digital text software. IDEA defines assistive technology as follows: \"any item, piece of equipment, or product system, whether acquired commercially off the shelf, modified, or customized, that is used to increase, maintain, or improve functional capabilities of a child with a disability. (B) Exception.--The term does not include a medical device that is surgically implanted, or the replacement of such device.\" \n\nAssistive technology in this area is broken down into low, mid, and high tech categories. Low tech encompasses equipment that is often low cost and does not include batteries or requires charging. Examples include adapted paper and pencil grips for writing or masks and color overlays for reading. Mid tech supports used in the school setting include the use of handheld spelling dictionaries and portable word processors used to keyboard writing. High tech supports involve the use of tablet devices and computers with accompanying software. Software supports for writing include the use of auditory feedback while keyboarding, word prediction for spelling, and speech to text. Supports for reading include the use of text to speech (TTS) software and font modification via access to digital text. Limited supports are available for math instruction and mostly consist of grid based software to allow younger students to keyboard equations and auditory feedback of more complex equations using MathML and Daisy.\n\nOne of the largest problems that affect people with disabilities is discomfort with prostheses. An experiment performed in Massachusetts utilized 20 people with various sensors attached to their arms. The subjects tried different arm exercises, and the sensors recorded their movements. All of the data helped engineers develop new engineering concepts for prosthetics.\n\nAssistive technology may attempt to improve the ergonomics of the devices themselves such as Dvorak and other alternative keyboard layouts, which offer more ergonomic layouts of the keys.\nAssistive technology devices have been created to enable people with disabilities to use modern touch screen mobile computers such as the iPad, iPhone and iPod touch. The Pererro is a plug and play adapter for iOS devices which uses the built in Apple VoiceOver feature in combination with a basic switch. This brings touch screen technology to those who were previously unable to use it. Apple, with the release of iOS 7 had introduced the ability to navigate apps using switch control. Switch access could be activated either through an external bluetooth connected switch, single touch of the screen, or use of right and left head turns using the device's camera. Additional accessibility features include the use of Assistive Touch which allows a user to access multi-touch gestures through pre-programmed onscreen buttons.\n\nFor users with physical disabilities a large variety of switches are available and customizable to the user's needs varying in size, shape, or amount of pressure required for activation. Switch access may be placed near any area of the body which has consistent and reliable mobility and less subject to fatigue. Common sites include the hands, head, and feet. Eye gaze and head mouse systems can also be used as an alternative mouse navigation. A user may utilize single or multiple switch sites and the process often involves a scanning through items on a screen and activating the switch once the desired object is highlighted.\n\nThe form of home automation called assistive domotics focuses on making it possible for elderly and disabled people to live independently. Home automation is becoming a viable option for the elderly and disabled who would prefer to stay in their own homes rather than move to a healthcare facility. This field uses much of the same technology and equipment as home automation for security, entertainment, and energy conservation but tailors it towards elderly and disabled users. For example, automated prompts and reminders utilize motion sensors and pre-recorded audio messages; an automated prompt in the kitchen may remind the resident to turn off the oven, and one by the front door may remind the resident to lock the door.\n\nOverall, assistive technology aims to allow people with disabilities to \"participate more fully in all aspects of life (home, school, and community)\" and increases their opportunities for \"education, social interactions, and potential for meaningful employment.\" It creates greater independence and control for disabled individuals. For example, in one study of 1,342 infants, toddlers and preschoolers, all with some kind of developmental, physical, sensory, or cognitive disability, the use of assistive technology created improvements in child development. These included improvements in \"cognitive, social, communication, literacy, motor, adaptive, and increases in engagement in learning activities.\" It has been found to lighten caregiver load.\n\n\n\n"}
{"id": "655", "url": "https://en.wikipedia.org/wiki?curid=655", "title": "Abacus", "text": "Abacus\n\nThe abacus (\"plural\" abaci or abacuses), also called a counting frame, is a calculating tool that was in use in Europe, China and Russia, centuries before the adoption of the written Hindu–Arabic numeral system. The exact origin of the abacus is still unknown. Today, abaci are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves in sand or on tablets of wood, stone, or metal.\n\nAbaci come in different designs. Some designs, like the bead frame consisting of beads divided into tens, are used mainly to teach arithmetic, although they remain popular in the post-Soviet states as a tool. Other designs, such as the Japanese soroban, have been used for practical calculations even involving several digits. For any particular abacus design, there usually are numerous different methods to perform a certain type of calculation, which may include basic operations like addition and multiplication, or even more complex ones, such as calculating square roots. Some of these methods may work with non-natural numbers (numbers such as and ).\n\nAlthough today many use calculators and computers instead of abaci to calculate, abaci still remain in common use in some countries. Merchants, traders and clerks in some parts of Eastern Europe, Russia, China and Africa use abaci, and they are still used to teach arithmetic to children. Some people who are unable to use a calculator because of visual impairment may use an abacus.\n\nThe use of the word \"abacus\" dates before 1387 AD, when a Middle English work borrowed the word from Latin to describe a sandboard abacus. The Latin word came from Greek ἄβαξ \"abax\" which means something without base, and improperly, any piece of rectangular board or plank. \nAlternatively, without reference to ancient texts on etymology, it has been suggested that it means \"a square tablet strewn with dust\", or \"drawing-board covered with dust (for the use of mathematics)\" (the exact shape of the Latin perhaps reflects the genitive form of the Greek word, ἄβακoς \"abakos\"). Whereas the table strewn with dust definition is popular, there are those that do not place credence in this at all and in fact state that it is not proven. Greek ἄβαξ itself is probably a borrowing of a Northwest Semitic, perhaps Phoenician, word akin to Hebrew \"ʾābāq\" (אבק), \"dust\" (or in post-Biblical sense meaning \"sand used as a writing surface\").\n\nThe preferred plural of \"abacus\" is a subject of disagreement, with both \"abacuses\" and \"abaci\" in use. The user of an abacus is called an \"abacist\".\n\nThe period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.\n\nSome scholars point to a character from the Babylonian cuneiform which may have been derived from a representation of the abacus. It is the belief of Old Babylonian scholars such as Carruccio that Old Babylonians \"may have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations\".\n\nThe use of the abacus in Ancient Egypt is mentioned by the Greek historian Herodotus, who writes that the Egyptians manipulated the pebbles from right to left, opposite in direction to the Greek left-to-right method. Archaeologists have found ancient disks of various sizes that are thought to have been used as counters. However, wall depictions of this instrument have not been discovered.\n\nDuring the Achaemenid Empire, around 600 BC the Persians first began to use the abacus. Under the Parthian, Sassanian and Iranian empires, scholars concentrated on exchanging knowledge and inventions with the countries around them – India, China, and the Roman Empire, when it is thought to have been exported to other countries.\n\nThe earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC. Also Demosthenes (384 BC–322 BC) talked of the need to use pebbles for calculations too difficult for your head. A play by Alexis from the 4th century BC mentions an abacus and pebbles for accounting, and both Diogenes and Polybius mention men that sometimes stood for more and sometimes for less, like the pebbles on an abacus. The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations. This Greek abacus saw use in Achaemenid Persia, the Etruscan civilization, Ancient Rome and, until the French Revolution, the Western Christian world.\n\nA tablet found on the Greek island Salamis in 1846 AD (the Salamis Tablet), dates back to 300 BC, making it the oldest counting board discovered so far. It is a slab of white marble long, wide, and thick, on which are 5 groups of markings. In the center of the tablet is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line. Below these lines is a wide space with a horizontal crack dividing it. Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line. Also from this time frame the \"Darius Vase\" was unearthed in 1851. It was covered with pictures including a \"treasurer\" holding a wax tablet in one hand while manipulating counters on a table with the other.\n\nThe earliest known written documentation of the Chinese abacus dates to the 2nd century BC.\n\nThe Chinese abacus, known as the suanpan (, lit. \"Counting tray\", Mandarin \"suàn pán\", Cantonese \"syun pun\"), is typically tall and comes in various widths depending on the operator. It usually has more than seven rods. There are two beads on each rod in the upper deck and five beads each in the bottom. The beads are usually rounded and made of a hardwood. The beads are counted by moving them up or down towards the beam. If you move them toward the beam, you count their value. If you move away, you don't count their value. The suanpan can be reset to the starting position instantly by a quick movement along the horizontal axis to spin all the beads away from the horizontal beam at the center.\n\nSuanpans can be used for functions other than counting. Unlike the simple counting board used in elementary schools, very efficient suanpan techniques have been developed to do multiplication, division, addition, subtraction, square root and cube root operations at high speed. There are currently schools teaching students how to use it.\n\nIn the long scroll \"Along the River During the Qingming Festival\" painted by Zhang Zeduan (1085–1145 AD) during the Song dynasty (960–1297 AD), a suanpan is clearly seen lying beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao).\n\nThe similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, as there is some evidence of a trade relationship between the Roman Empire and China. However, no direct connection can be demonstrated, and the similarity of the abaci may be coincidental, both ultimately arising from counting with five fingers per hand. Where the Roman model (like most modern Korean and Japanese) has 4 plus 1 bead per decimal place, the standard suanpan has 5 plus 2. (Incidentally, this allows use with a hexadecimal numeral system, which was used for traditional Chinese measures of weight.) Instead of running on wires as in the Chinese, Korean, and Japanese models, the beads of Roman model run in grooves, presumably making arithmetic calculations much slower.\n\nAnother possible source of the suanpan is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a place holder. The zero was probably introduced to the Chinese in the Tang dynasty (618-907 AD) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.\n\nThe normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table. Originally pebbles (\"calculi\") were used. Later, and in medieval Europe, jetons were manufactured. Marked lines indicated units, fives, tens etc. as in the Roman numeral system. This system of 'counter casting' continued into the late Roman empire and in medieval Europe, and persisted in limited use into the nineteenth century. Due to Pope Sylvester II's reintroduction of the abacus with very useful modifications, it became widely used in Europe once again during the 11th century This abacus used beads on wires, unlike the traditional Roman counting boards, which meant the abacus could be used much faster.\n\nWriting in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus.\n\nOne example of archaeological evidence of the Roman abacus, shown here in reconstruction, dates to the 1st century AD. It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each. The groove marked I indicates units, X tens, and so on up to millions. The beads in the shorter grooves denote fives –five units, five tens etc., essentially in a bi-quinary coded decimal system, obviously related to the Roman numerals. The short grooves on the right may have been used for marking Roman \"ounces\" (i.e. fractions).\n\nThere is no clear evidence for use of the abacus in India. The decimal number system invented in India replaced the abacus in Western Europe.\n\nThe \"Abhidharmakośabhāṣya\" of Vasubandhu (316-396), a Sanskrit work on Buddhist philosophy, says that the second-century CE philosopher Vasumitra said that \"placing a wick (Sanskrit \"vartikā\") on the number one (\"ekāṅka\") means it is a one, while placing the wick on the number hundred means it is called a hundred, and on the number one thousand means it is a thousand\". It is unclear exactly what this arrangement may have been. Around the 5th century, Indian clerks were already finding new ways of recording the contents of the Abacus. Hindu texts used the term \"śūnya\" (zero) to indicate the empty column on the abacus.\n\nIn Japanese, the abacus is called \"soroban\" (, lit. \"Counting tray\"), imported from China in the 14th century. It was probably in use by the working class a century or more before the ruling class started, as the class structure did not allow for devices used by the lower class to be adopted or used by the ruling class. The 1/4 abacus, which is suited to decimal calculation, appeared circa 1930, and became widespread as the Japanese abandoned hexadecimal weight calculation which was still common in China. The abacus is still manufactured in Japan today even with the proliferation, practicality, and affordability of pocket electronic calculators. The use of the soroban is still taught in Japanese primary schools as part of mathematics, primarily as an aid to faster mental calculation. Using visual imagery of a soroban, one can arrive at the answer in the same time as, or even faster than, is possible with a physical instrument.\n\nThe Chinese abacus migrated from China to Korea around 1400 AD. Koreans call it \"jupan\" (주판), \"supan\" (수판) or \"jusan\" (주산).\n\nSome sources mention the use of an abacus called a \"nepohualtzintzin\" in ancient Aztec culture. This Mesoamerican abacus used a 5-digit base-20 system.\nThe word Nepōhualtzintzin comes from Nahuatl and it is formed by the roots; \"Ne\" – personal -; \"pōhual\" or \"pōhualli\" – the account -; and \"tzintzin\" – small similar elements. Its complete meaning was taken as: counting with small similar elements by somebody. Its use was taught in the Calmecac to the \"temalpouhqueh\" , who were students dedicated to take the accounts of skies, from childhood.\n\nThe Nepōhualtzintzin was divided in two main parts separated by a bar or intermediate cord. In the left part there were four beads, which in the first row have unitary values (1, 2, 3, and 4), and in the right side there are three beads with values of 5, 10, and 15 respectively. In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding account in the first row.\n\nAltogether, there were 13 rows with 7 beads in each one, which made up 91 beads in each Nepōhualtzintzin. This was a basic number to understand, 7 times 13, a close relation conceived between natural phenomena, the underworld and the cycles of the heavens. One Nepōhualtzintzin (91) represented the number of days that a season of the year lasts, two Nepōhualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepōhualtzintzin (273) is the number of days of a baby's gestation, and four Nepōhualtzintzin (364) completed a cycle and approximate a year (1 days short). When translated into modern computer arithmetic, the Nepōhualtzintzin amounted to the rank from 10 to the 18 in floating point, which calculated stellar as well as infinitesimal amounts with absolute precision, meant that no round off was allowed.\n\nThe rediscovery of the Nepōhualtzintzin was due to the Mexican engineer David Esparza Hidalgo, who in his wanderings throughout Mexico found diverse engravings and paintings of this instrument and reconstructed several of them made in gold, jade, encrustations of shell, etc. There have also been found very old Nepōhualtzintzin attributed to the Olmec culture, and even some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures.\n\nGeorge I. Sanchez, \"Arithmetic in Maya\", Austin-Texas, 1961 found another base 5, base 4 abacus in the Yucatán peninsula that also computed calendar data. This was a finger abacus, on one hand 0, 1, 2, 3, and 4 were used; and on the other hand 0, 1, 2 and 3 were used. Note the use of zero at the beginning and end of the two cycles. Sanchez worked with Sylvanus Morley, a noted Mayanist.\n\nThe quipu of the Incas was a system of colored knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations. Calculations were carried out using a yupana (Quechua for \"counting tool\"; see figure) which was still in use after the conquest of Peru. The working principle of a yupana is unknown, but in 2001 an explanation of the mathematical basis of these instruments was proposed by Italian mathematician Nicolino De Pasquale. By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20 and 40 as place values for the different fields in the instrument. Using the Fibonacci sequence would keep the number of grains within any one field at a minimum.\n\nThe Russian abacus, the \"schoty\" (счёты), usually has a single slanted deck, with ten beads on each wire (except one wire, usually positioned near the user, with four beads for quarter-ruble fractions). Older models have another 4-bead wire for quarter-kopeks, which were minted until 1916. The Russian abacus is often used vertically, with wires from left to right in the manner of a book. The wires are usually bowed to bulge upward in the center, to keep the beads pinned to either of the two sides. It is cleared when all the beads are moved to the right. During manipulation, beads are moved to the left. For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different color from the other eight beads. Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color.\n\nAs a simple, cheap and reliable device, the Russian abacus was in use in all shops and markets throughout the former Soviet Union, and the usage of it was taught in most schools until the 1990s. Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia and likewise the mass production of Felix arithmometers since 1924 did not significantly reduce their use in the Soviet Union. The Russian abacus began to lose popularity only after the mass production of microcalculators had started in the Soviet Union in 1974. Today it is regarded as an archaism and replaced by the handheld calculator.\n\nThe Russian abacus was brought to France around 1820 by the mathematician Jean-Victor Poncelet, who served in Napoleon's army and had been a prisoner of war in Russia. The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods. To Poncelet's French contemporaries, it was something new. Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid. The Turks and the Armenian people also used abaci similar to the Russian schoty. It was named a \"coulba\" by the Turks and a \"choreb\" by the Armenians.\n\nAround the world, abaci have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic.\n\nIn Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame has been common (see image). It is still often seen as a plastic or wooden toy.\n\nThe wire frame may be used either with positional notation like other abaci (thus the 10-wire version may represent numbers up to 9,999,999,999), or each bead may represent one unit (so that e.g. 74 can be represented by shifting all beads on 7 wires and 4 beads on the 8th wire, so numbers up to 100 may be represented). In the bead frame shown, the gap between the 5th and 6th wire, corresponding to the color change between the 5th and the 6th bead on each wire, suggests the latter use.\n\nThe red-and-white abacus is used in contemporary primary schools for a wide range of number-related lessons. The twenty bead version, referred to by its Dutch name \"rekenrek\" (\"calculating frame\"), is often used, sometimes on a string of beads, sometimes on a rigid framework.\n\nAn adapted abacus, invented by Tim Cranmer, called a Cranmer abacus is still commonly used by individuals who are blind. A piece of soft fabric or rubber is placed behind the beads so that they do not move inadvertently. This keeps the beads in place while the users feel or manipulate them. They use an abacus to perform the mathematical functions multiplication, division, addition, subtraction, square root and cube root.\n\nAlthough blind students have benefited from talking calculators, the abacus is still very often taught to these students in early grades, both in public schools and state schools for the blind. The abacus teaches mathematical skills that can never be replaced with talking calculators and is an important learning tool for blind students. Blind students also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems can be long and difficult. The abacus gives blind and visually impaired students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper. Many blind people find this number machine a very useful tool throughout life.\n\nThe binary abacus is used to explain how computers manipulate numbers. The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII. The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an \"on\" or \"off\" position.\n\n\n\n\n"}
{"id": "656", "url": "https://en.wikipedia.org/wiki?curid=656", "title": "Acid", "text": "Acid\n\nAn acid is a molecule or ion capable of donating a hydron (proton or hydrogen ion H), or, alternatively, capable of forming a covalent bond with an electron pair (a Lewis acid).\n\nThe first category of acids is the proton donors or Brønsted acids. In the special case of aqueous solutions, proton donors form the hydronium ion HO and are known as Arrhenius acids. Brønsted and Lowry generalized the Arrhenius theory to include non-aqueous solvents. A Brønsted or Arrhenius acid usually contains a hydrogen atom bonded to a chemical structure that is still energetically favorable after loss of H.\n\nAqueous Arrhenius acids have characteristic properties which provide a practical description of an acid. Acids form aqueous solutions with a sour taste, can turn blue litmus red, and react with bases and certain metals (like calcium) to form salts. The word \"acid\" is derived from the Latin \"acidus/acēre\" meaning \"sour\". An aqueous solution of an acid has a pH less than 7 and is colloquially also referred to as 'acid' (as in 'dissolved in acid'), while the strict definition refers only to the solute. A lower pH means a higher acidity, and thus a higher concentration of positive hydrogen ions in the solution. Chemicals or substances having the property of an acid are said to be acidic.\n\nCommon aqueous acids include hydrochloric acid (a solution of hydrogen chloride which is found in gastric acid in the stomach and activates digestive enzymes), acetic acid (vinegar is a dilute aqueous solution of this liquid), sulfuric acid (used in car batteries), and citric acid (found in citrus fruits). As these examples show, acids (in the colloquial sense) can be solutions or pure substances, and can be derived from acids (in the strict sense) that are solids, liquids, or gases. Strong acids and some concentrated weak acids are corrosive, but there are exceptions such as carboranes and boric acid.\n\nThe second category of acids are Lewis acids, which form a covalent bond with an electron pair. An example is boron trifluoride (BF), whose boron atom has a vacant orbital which can form a covalent bond by sharing a lone pair of electrons on an atom in a base, for example the nitrogen atom in ammonia (NH). Lewis considered this as a generalization of the Brønsted definition, so that an acid is a chemical species that accepts electron pairs either directly \"or\" by releasing protons (H) into the solution, which then accept electron pairs. However, hydrogen chloride, acetic acid, and most other Brønsted-Lowry acids cannot form a covalent bond with an electron pair and are therefore not Lewis acids. Conversely, many Lewis acids are not Arrhenius or Brønsted-Lowry acids. In modern terminology, an \"acid\" is implicitly a Brønsted acid and not a Lewis acid, since chemists almost always refer to a Lewis acid explicitly as \"a Lewis acid\".\n\nModern definitions are concerned with the fundamental chemical reactions common to all acids.\n\nMost acids encountered in everyday life are aqueous solutions, or can be dissolved in water, so the Arrhenius and Brønsted-Lowry definitions are the most relevant.\n\nThe Brønsted-Lowry definition is the most widely used definition; unless otherwise specified, acid-base reactions are assumed to involve the transfer of a proton (H) from an acid to a base.\n\nHydronium ions are acids according to all three definitions. Interestingly, although alcohols and amines can be Brønsted-Lowry acids, they can also function as Lewis bases due to the lone pairs of electrons on their oxygen and nitrogen atoms.\n\nThe Swedish chemist Svante Arrhenius attributed the properties of acidity to hydrogen ions (H) or protons in 1884. An Arrhenius acid is a substance that, when added to water, increases the concentration of H ions in the water. Note that chemists often write H(\"aq\") and refer to the hydrogen ion when describing acid-base reactions but the free hydrogen nucleus, a proton, does not exist alone in water, it exists as the hydronium ion, HO. Thus, an Arrhenius acid can also be described as a substance that increases the concentration of hydronium ions when added to water. Examples include molecular substances such as HCl and acetic acid.\n\nAn Arrhenius base, on the other hand, is a substance which increases the concentration of hydroxide (OH) ions when dissolved in water. This decreases the concentration of hydronium because the ions react to form HO molecules:\n\nHO + OH ⇌ HO + HO\n\nDue to this equilibrium, any increase in the concentration of hydronium is accompanied by a decrease in the concentration of hydroxide. Thus, an Arrhenius acid could also be said to be one that decreases hydroxide concentration, while an Arrhenius base increases it.\n\nIn an acidic solution, the concentration of hydronium ions is greater than 10 moles per liter. Since pH is defined as the negative logarithm of the concentration of hydronium ions, acidic solutions thus have a pH of less than 7.\n\nWhile the Arrhenius concept is useful for describing many reactions, it is also quite limited in its scope. In 1923 chemists Johannes Nicolaus Brønsted and Thomas Martin Lowry independently recognized that acid-base reactions involve the transfer of a proton. A Brønsted-Lowry acid (or simply Brønsted acid) is a species that donates a proton to a Brønsted-Lowry base. Brønsted-Lowry acid-base theory has several advantages over Arrhenius theory. Consider the following reactions of acetic acid (CHCOOH), the organic acid that gives vinegar its characteristic taste:\n\nBoth theories easily describe the first reaction: CHCOOH acts as an Arrhenius acid because it acts as a source of HO when dissolved in water, and it acts as a Brønsted acid by donating a proton to water. In the second example CHCOOH undergoes the same transformation, in this case donating a proton to ammonia (NH), but does not relate to the Arrhenius definition of an acid because the reaction does not produce hydronium. Nevertheless, CHCOOH is both an Arrhenius and a Brønsted-Lowry acid.\n\nBrønsted-Lowry theory can be used to describe reactions of molecular compounds in nonaqueous solution or the gas phase. Hydrogen chloride (HCl) and ammonia combine under several different conditions to form ammonium chloride, NHCl. In aqueous solution HCl behaves as hydrochloric acid and exists as hydronium and chloride ions. The following reactions illustrate the limitations of Arrhenius's definition:\n\nAs with the acetic acid reactions, both definitions work for the first example, where water is the solvent and hydronium ion is formed by the HCl solute. The next two reactions do not involve the formation of ions but are still proton-transfer reactions. In the second reaction hydrogen chloride and ammonia (dissolved in benzene) react to form solid ammonium chloride in a benzene solvent and in the third gaseous HCl and NH combine to form the solid.\n\nA third, only marginally related concept was proposed in 1923 by Gilbert N. Lewis, which includes reactions with acid-base characteristics that do not involve a proton transfer. A Lewis acid is a species that accepts a pair of electrons from another species; in other words, it is an electron pair acceptor. Brønsted acid-base reactions are proton transfer reactions while Lewis acid-base reactions are electron pair transfers. Many Lewis acids are not Brønsted-Lowry acids. Contrast how the following reactions are described in terms of acid-base chemistry:\nIn the first reaction a fluoride ion, F, gives up an electron pair to boron trifluoride to form the product tetrafluoroborate. Fluoride \"loses\" a pair of valence electrons because the electrons shared in the B—F bond are located in the region of space between the two atomic nuclei and are therefore more distant from the fluoride nucleus than they are in the lone fluoride ion. BF is a Lewis acid because it accepts the electron pair from fluoride. This reaction cannot be described in terms of Brønsted theory because there is no proton transfer. The second reaction can be described using either theory. A proton is transferred from an unspecified Brønsted acid to ammonia, a Brønsted base; alternatively, ammonia acts as a Lewis base and transfers a lone pair of electrons to form a bond with a hydrogen ion. The species that gains the electron pair is the Lewis acid; for example, the oxygen atom in HO gains a pair of electrons when one of the H—O bonds is broken and the electrons shared in the bond become localized on oxygen. Depending on the context, a Lewis acid may also be described as an oxidizer or an electrophile. Organic Brønsted acids, such as acetic, citric, or oxalic acid, are not Lewis acids. They dissociate in water to produce a Lewis acid, H, but at the same time also yield an equal amount of a Lewis base (acetate, citrate, or oxalate, respectively, for the acids mentioned). Few, if any, of the acids discussed in the following are Lewis acids.\n\nReactions of acids are often generalized in the form HA H + A, where HA represents the acid and A is the conjugate base. This reaction is referred to as protolysis. The protonated form (HA) of an acid is also sometimes referred to as the free acid.\n\nAcid-base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively). Note that the acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as HA H + A. In solution there exists an equilibrium between the acid and its conjugate base. The equilibrium constant \"K\" is an expression of the equilibrium concentrations of the molecules or the ions in solution. Brackets indicate concentration, such that [HO] means \"the concentration of HO\". The acid dissociation constant \"K\" is generally used in the context of acid-base reactions. The numerical value of \"K\" is equal to the product of the concentrations of the products divided by the concentration of the reactants, where the reactant is the acid (HA) and the products are the conjugate base and H.\nThe stronger of two acids will have a higher \"K\" than the weaker acid; the ratio of hydrogen ions to acid will be higher for the stronger acid as the stronger acid has a greater tendency to lose its proton. Because the range of possible values for \"K\" spans many orders of magnitude, a more manageable constant, p\"K\" is more frequently used, where p\"K\" = −log \"K\". Stronger acids have a smaller p\"K\" than weaker acids. Experimentally determined p\"K\" at 25 °C in aqueous solution are often quoted in textbooks and reference material.\n\nIn the classical naming system, acids are named according to their anions. That ionic suffix is dropped and replaced with a new suffix (and sometimes prefix), according to the table below.\nFor example, HCl has chloride as its anion, so the -ide suffix makes it take the form hydrochloric acid. In the IUPAC naming system, \"aqueous\" is simply added to the name of the ionic compound. Thus, for hydrogen chloride, the IUPAC name would be aqueous hydrogen chloride. The prefix \"hydro-\" is added only if the acid is made up of just hydrogen and one other element.\n\nClassical naming system:\nThe strength of an acid refers to its ability or tendency to lose a proton. A strong acid is one that completely dissociates in water; in other words, one mole of a strong acid HA dissolves in water yielding one mole of H and one mole of the conjugate base, A, and none of the protonated acid HA. In contrast, a weak acid only partially dissociates and at equilibrium both the acid and the conjugate base are in solution. Examples of strong acids are hydrochloric acid (HCl), hydroiodic acid (HI), hydrobromic acid (HBr), perchloric acid (HClO), nitric acid (HNO) and sulfuric acid (HSO). In water each of these essentially ionizes 100%. The stronger an acid is, the more easily it loses a proton, H. Two key factors that contribute to the ease of deprotonation are the polarity of the H—A bond and the size of atom A, which determines the strength of the H—A bond. Acid strengths are also often discussed in terms of the stability of the conjugate base.\n\nStronger acids have a larger \"K\" and a more negative p\"K\" than weaker acids.\n\nSulfonic acids, which are organic oxyacids, are a class of strong acids. A common example is toluenesulfonic acid (tosylic acid). Unlike sulfuric acid itself, sulfonic acids can be solids. In fact, polystyrene functionalized into polystyrene sulfonate is a solid strongly acidic plastic that is filterable.\n\nSuperacids are acids stronger than 100% sulfuric acid. Examples of superacids are fluoroantimonic acid, magic acid and perchloric acid. Superacids can permanently protonate water to give ionic, crystalline hydronium \"salts\". They can also quantitatively stabilize carbocations.\n\nWhile \"K\" measures the strength of an acid compound, the strength of an aqueous acid solution is measured by pH, which is an indication of the concentration of hydronium in the solution. The pH of a simple solution of an acid compound in water is determined by the dilution of the compound and the compound's \"K\".\n\nMonoprotic acids are those acids that are able to donate one proton per molecule during the process of dissociation (sometimes called ionization) as shown below (symbolized by HA):\n\nCommon examples of monoprotic acids in mineral acids include hydrochloric acid (HCl) and nitric acid (HNO). On the other hand, for organic acids the term mainly indicates the presence of one carboxylic acid group and sometimes these acids are known as monocarboxylic acid. Examples in organic acids include formic acid (HCOOH), acetic acid (CHCOOH) and benzoic acid (CHCOOH).\nPolyprotic acids, also known as polybasic acids, are able to donate more than one proton per acid molecule, in contrast to monoprotic acids that only donate one proton per molecule. Specific types of polyprotic acids have more specific names, such as diprotic acid (two potential protons to donate) and triprotic acid (three potential protons to donate).\n\nA diprotic acid (here symbolized by HA) can undergo one or two dissociations depending on the pH. Each dissociation has its own dissociation constant, K and K.\n\nThe first dissociation constant is typically greater than the second; i.e., \"K\" > \"K\". For example, sulfuric acid (HSO) can donate one proton to form the bisulfate anion (HSO), for which \"K\" is very large; then it can donate a second proton to form the sulfate anion (SO), wherein the \"K\" is intermediate strength. The large \"K\" for the first dissociation makes sulfuric a strong acid. In a similar manner, the weak unstable carbonic acid (HCO) can lose one proton to form bicarbonate anion (HCO) and lose a second to form carbonate anion (CO). Both \"K\" values are small, but \"K\" > \"K\" .\n\nA triprotic acid (HA) can undergo one, two, or three dissociations and has three dissociation constants, where \"K\" > \"K\" > \"K\".\n\nAn inorganic example of a triprotic acid is orthophosphoric acid (HPO), usually just called phosphoric acid. All three protons can be successively lost to yield HPO, then HPO, and finally PO, the orthophosphate ion, usually just called phosphate. Even though the positions of the three protons on the original phosphoric acid molecule are equivalent, the successive \"K\" values differ since it is energetically less favorable to lose a proton if the conjugate base is more negatively charged. An organic example of a triprotic acid is citric acid, which can successively lose three protons to finally form the citrate ion.\n\nAlthough the subsequent loss of each hydrogen ion is less favorable, all of the conjugate bases are present in solution. The fractional concentration, \"α\" (alpha), for each species can be calculated. For example, a generic diprotic acid will generate 3 species in solution: HA, HA, and A. The fractional concentrations can be calculated as below when given either the pH (which can be converted to the [H]) or the concentrations of the acid with all its conjugate bases:\nA plot of these fractional concentrations against pH, for given \"K\" and \"K\", is known as a Bjerrum plot. A pattern is observed in the above equations and can be expanded to the general \"n\" -protic acid that has been deprotonated \"i\" -times:\n\\alpha_{\\ce H_{n-i} A^{i-} }= \n"}
{"id": "657", "url": "https://en.wikipedia.org/wiki?curid=657", "title": "Asphalt", "text": "Asphalt\n\nAsphalt , also known as bitumen , is a sticky, black, and highly viscous liquid or semi-solid form of petroleum. It may be found in natural deposits or may be a refined product, and is classed as a pitch. Before the 20th century, the term asphaltum was also used. The word is derived from the Ancient Greek ἄσφαλτος \"ásphaltos\".\n\nThe primary use (70%) of asphalt is in road construction, where it is used as the glue or binder mixed with aggregate particles to create asphalt concrete. Its other main uses are for bituminous waterproofing products, including production of roofing felt and for sealing flat roofs.\n\nThe terms \"asphalt\" and \"bitumen\" are often used interchangeably to mean both natural and manufactured forms of the substance. In American English, \"asphalt\" (or \"asphalt cement\") is commonly used for a refined residue from the distillation process of selected crude oils. Outside the United States, the product is often called \"bitumen\", and geologists worldwide often prefer the term for the naturally occurring variety. Common colloquial usage often refers to various forms of asphalt as \"tar\", as in the name of the La Brea Tar Pits.\n\nNaturally occurring asphalt is sometimes specified by the term \"crude bitumen\". Its viscosity is similar to that of cold molasses while the material obtained from the fractional distillation of crude oil boiling at is sometimes referred to as \"refined bitumen\". The Canadian province of Alberta has most of the world's reserves of natural asphalt, covering , an area larger than England.\n\nThe word \"asphalt\" is derived from the late Middle English, in turn from French \"asphalte\", based on Late Latin \"asphalton\", \"asphaltum\", which is the latinisation of the Greek ἄσφαλτος (\"ásphaltos\", \"ásphalton\"), a word meaning \"asphalt/bitumen/pitch\", which perhaps derives from ἀ-, \"without\" and σφάλλω (\"sfallō\"), \"make fall\". The first use of asphalt by the ancients was in the nature of a cement for securing or joining together various objects, and it thus seems likely that the name itself was expressive of this application. Specifically, Herodotus mentioned that bitumen was brought to Babylon to build its gigantic fortification wall. From the Greek, the word passed into late Latin, and thence into French (\"asphalte\") and English (\"asphaltum\" and \"asphalt\"). In French, the term \"asphalte\" is used for naturally occurring asphalt-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the \"asphaltic concrete\" used to pave roads. \n\nThe expression \"bitumen\" originated in the Sanskrit words \"jatu\", meaning \"pitch,\" and \"jatu-krit\", meaning \"pitch creating\" or \"pitch producing\" (referring to coniferous or resinous trees). The Latin equivalent is claimed by some to be originally \"gwitu-men\" (pertaining to pitch), and by others, \"pixtumens\" (exuding or bubbling pitch), which was subsequently shortened to \"bitumen\", thence passing via French into English. From the same root is derived the Anglo-Saxon word \"cwidu\" (mastix), the German word \"Kitt\" (cement or mastic) and the old Norse word \"kvada\".\n\nIn British English, \"bitumen\" is used instead of \"asphalt.\" The word \"asphalt\" is instead used to refer to asphalt concrete, a mixture of construction aggregate and asphalt itself (also called \"tarmac\" in common parlance). Bitumen mixed with clay was usually called \"asphaltum\", but the term is less commonly used today. \n\nIn Australian English, \"bitumen\" is often used as the generic term for road surfaces. \n\nIn American English, \"asphalt\" is equivalent to the British \"bitumen\". However, \"asphalt\" is also commonly used as a shortened form of \"asphalt concrete\" (therefore equivalent to the British \"asphalt\" or \"tarmac\"). \n\nIn Canadian English, the word \"bitumen\" is used to refer to the vast Canadian deposits of extremely heavy crude oil, while \"asphalt\" is used for the oil refinery product. Diluted bitumen (diluted with naphtha to make it flow in pipelines) is known as \"dilbit\" in the Canadian petroleum industry, while bitumen \"upgraded\" to synthetic crude oil is known as \"syncrude\", and syncrude blended with bitumen is called \"synbit\".\n\n\"Bitumen\" is still the preferred geological term for naturally occurring deposits of the solid or semi-solid form of petroleum. \"Bituminous rock\" is a form of sandstone impregnated with bitumen. The tar sands of Alberta, Canada are a similar material.\n\nNeither of the terms \"asphalt\" or \"bitumen\" should be confused with tar or coal tars.\n\nThe components of asphalt include four main classes of compounds: \n\nThe naphthene aromatics and polar aromatics are typically the majority components. Most natural bitumens also contain organosulfur compounds, resulting in an overall sulfur content of up to 4%. Nickel and vanadium are found at <10 parts per million, as is typical of some petroleum.\n\nThe substance is soluble in carbon disulfide. It is commonly modelled as a colloid, with asphaltenes as the dispersed phase and maltenes as the continuous phase. \"It is almost impossible to separate and identify all the different molecules of asphalt, because the number of molecules with different chemical structure is extremely large\".\n\nAsphalt may be confused with coal tar, which is a visually similar black, thermoplastic material produced by the destructive distillation of coal. During the early and mid-20th century, when town gas was produced, coal tar was a readily available byproduct and extensively used as the binder for road aggregates. The addition of coal tar to macadam roads led to the word \"tarmac\", which is now used in common parlance to refer to road-making materials. However, since the 1970s, when natural gas succeeded town gas, asphalt has completely overtaken the use of coal tar in these applications. Other examples of this confusion include the La Brea Tar Pits and the Canadian oil sands, both of which actually contain natural bitumen rather than tar. \"Pitch\" is another term sometimes informally used at times to refer to asphalt, as in Pitch Lake.\n\nThe majority of asphalt used commercially is obtained from petroleum. Nonetheless, large amounts of asphalt occur in concentrated form in nature. Naturally occurring deposits of bitumen are formed from the remains of ancient, microscopic algae (diatoms) and other once-living things. These remains were deposited in the mud on the bottom of the ocean or lake where the organisms lived. Under the heat (above 50 °C) and pressure of burial deep in the earth, the remains were transformed into materials such as bitumen, kerogen, or petroleum.\n\nNatural deposits of bitumen include lakes such as the Pitch Lake in Trinidad and Tobago and Lake Bermudez in Venezuela. Natural seeps occur in the La Brea Tar Pits and in the Dead Sea.\n\nBitumen also occurs in unconsolidated sandstones known as \"oil sands\" in Alberta, Canada, and the similar \"tar sands\" in Utah, US.\nThe Canadian province of Alberta has most of the world's reserves, in three huge deposits covering , an area larger than England or New York state. These bituminous sands contain of commercially established oil reserves, giving Canada the third largest oil reserves in the world. Although historically it was used without refining to pave roads, nearly all of the output is now used as raw material for oil refineries in Canada and the United States.\n\nThe world's largest deposit of natural bitumen, known as the Athabasca oil sands, is located in the McMurray Formation of Northern Alberta. This formation is from the early Cretaceous, and is composed of numerous lenses of oil-bearing sand with up to 20% oil. Isotopic studies show the oil deposits to be about 110 million years old. Two smaller but still very large formations occur in the Peace River oil sands and the Cold Lake oil sands, to the west and southeast of the Athabasca oil sands, respectively. Of the Alberta deposits, only parts of the Athabasca oil sands are shallow enough to be suitable for surface mining. The other 80% has to be produced by oil wells using enhanced oil recovery techniques like steam-assisted gravity drainage.\n\nMuch smaller heavy oil or bitumen deposits also occur in the Uinta Basin in Utah, US. The Tar Sand Triangle deposit, for example, is roughly 6% bitumen.\n\nBitumen may occur in hydrothermal veins. An example of this is within the Uinta Basin of Utah, in the US, where there is a swarm of laterally and vertically extensive veins composed of a solid hydrocarbon termed Gilsonite. These veins formed by the polymerization and solidification of hydrocarbons that were mobilized from the deeper oil shales of the Green River Formation during burial and diagenesis.\n\nBitumen is similar to the organic matter in carbonaceous meteorites. However, detailed studies have shown these materials to be distinct. The vast Alberta bitumen resources are considered to have started out as living material from marine plants and animals, mainly algae, that died millions of years ago when an ancient ocean covered Alberta. They were covered by mud, buried deeply over time, and gently cooked into oil by geothermal heat at a temperature of . Due to pressure from the rising of the Rocky Mountains in southwestern Alberta, 80 to 55 million years ago, the oil was driven northeast hundreds of kilometres and trapped into underground sand deposits left behind by ancient river beds and ocean beaches, thus forming the oil sands.\n\nThe use of asphalt for waterproofing and as an adhesive dates at least to the fifth millennium BC in the early Indus valley sites like Mehrgarh, where it was used to line the baskets in which crops were gathered.\n\nIn the ancient Middle East, the Sumerians used natural bitumen deposits for mortar between bricks and stones, to cement parts of carvings, such as eyes, into place, for ship caulking, and for waterproofing. The Greek historian Herodotus said hot bitumen was used as mortar in the walls of Babylon. \n\nThe long Euphrates Tunnel beneath the river Euphrates at Babylon in the time of Queen Semiramis (ca. 800 BC) was reportedly constructed of burnt bricks covered with bitumen as a waterproofing agent.\n\nBitumen was used by ancient Egyptians to embalm mummies. The Persian word for asphalt is \"moom\", which is related to the English word mummy. The Egyptians' primary source of bitumen was the Dead Sea, which the Romans knew as \"Palus Asphaltites\" (Asphalt Lake).\n\nApproximately 40 AD, Dioscorides described the Dead Sea material as \"Judaicum bitumen\", and noted other places in the region where it could be found.\nThe Sidon bitumen is thought to refer to materiall found at Hasbeya. Pliny refers also to bitumen being found in Epirus. It was a valuable strategic resource, the object of the first known battle for a hydrocarbon deposit—between the Seleucids and the Nabateans in 312 BC.\n\nIn the ancient Far East, natural bitumen was slowly boiled to get rid of the higher fractions, leaving a thermoplastic material of higher molecular weight that when layered on objects became quite hard upon cooling. This was used to cover objects that needed waterproofing, such as scabbards and other items. Statuettes of household deities were also cast with this type of material in Japan, and probably also in China.\n\nIn North America, archaeological recovery has indicated bitumen was sometimes used to adhere stone projectile points to wooden shafts. In Canada, aboriginal people used bitumen seeping out of the banks of the Athabasca and other rivers to waterproof birch bark canoes, and also heated it in smudge pots to ward off mosquitoes in the summer.\n\nIn 1553, Pierre Belon described in his work \"Observations\" that \"pissasphalto\", a mixture of pitch and bitumen, was used in the Republic of Ragusa (now Dubrovnik, Croatia) for tarring of ships.\n\nAn 1838 edition of \"Mechanics Magazine\" cites an early use of asphalt in France. A pamphlet dated 1621, by \"a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel\", and that he proposed to use it in a variety of ways – \"principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusion of dirt and filth\", which at that time made the water unusable. \"He expatiates also on the excellence of this material for forming level and durable terraces\" in palaces, \"the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation\". \n\nBut the substance was generally neglected in France until the revolution of 1830. In the 1830s there was a surge of interest, and asphalt became widely used \"for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes\". Its rise in Europe was \"a sudden phenomenon\", after natural deposits were found \"in France at Osbann (Bas-Rhin), the Parc (Ain) and the Puy-de-la-Poix (Puy-de-Dôme)\", although it could also be made artificially. One of the earliest uses in France was the laying of about 24,000 square yards of Seyssel asphalt at the Place de la Concorde in 1835.\n\nAmong the earlier uses of bitumen in the United Kingdom was for etching. William Salmon's \"Polygraphice\" (1673) provides a recipe for varnish used in etching, consisting of three ounces of virgin wax, two ounces of mastic, and one ounce of asphaltum. By the fifth edition in 1685, he had included more asphaltum recipes from other sources.\n\nThe first British patent for the use of asphalt was \"Cassell's patent asphalte or bitumen\" in 1834. Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain. Dr T. Lamb Phipson writes that his father, Samuel Ryland Phipson, a friend of Claridge, was also \"instrumental in introducing the asphalte pavement (in 1836)\". Indeed, mastic pavements had been previously employed at Vauxhall by a competitor of Claridge, but without success.\n\nClaridge obtained a patent in Scotland on 27 March 1838, and obtained a patent in Ireland on 23 April 1838. In 1851, extensions for the 1837 patent and for both 1838 patents were sought by the trustees of a company previously formed by Claridge. \"Claridge's Patent Asphalte Company\"—formed in 1838 for the purpose of introducing to Britain \"Asphalte in its natural state from the mine at Pyrimont Seysell in France\",—\"laid one of the first asphalt pavements in Whitehall\". Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, \"and subsequently on the space at the bottom of the steps leading from Waterloo Place to St. James Park\". \"The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry\". \"By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production\", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving \"continue(d) in good order\".\n\nIn 1838, there was a flurry of entrepreneurial activity involving asphalt, which had uses beyond paving. For example, asphalt could also be used for flooring, damp proofing in buildings, and for waterproofing of various types of pools and baths, both of which were also proliferating in the 19th century. On the London stockmarket, there were various claims as to the exclusivity of asphalt quality from France, Germany and England. And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other. In England, \"Claridge's was the type most used in the 1840s and 50s\".\n\nIn 1914, Claridge's Company entered into a joint venture to produce tar-bound macadam, with materials manufactured through a subsidiary company called Clarmac Roads Ltd. Two products resulted, namely \"Clarmac\", and \"Clarphalte\", with the former being manufactured by Clarmac Roads and the latter by Claridge's Patent Asphalte Co., although \"Clarmac\" was more widely used. However, the First World War ruined the Clarmac Company, which entered into liquidation in 1915. The failure of Clarmac Roads Ltd had a flow-on effect to Claridge's Company, which was itself compulsorily wound up, ceasing operations in 1917, having invested a substantial amount of funds into the new venture, both at the outset and in a subsequent attempt to save the Clarmac Company.\n\nThe first use of bitumen in the New World was by indigenous peoples. On the west coast, as early as the 13th century, the Tongva, Luiseño and Chumash peoples collected the naturally occurring bitumen that seeped to the surface above underlying petroleum deposits. All three groups used the substance as an adhesive. It is found on many different artifacts of tools and ceremonial items. For example, it was used on rattles to adhere gourds or turtle shells to rattle handles. It was also used in decorations. Small round shell beads were often set in asphaltum to provide decorations. It was used as a sealant on baskets to make them watertight for carrying water, possibly poisoning those who drank the water. Asphalt was used also to seal the planks on ocean-going canoes.\n\nRoads in the US have been paved with materials that include asphalt since at least 1870, when a street in front of the Newark (New Jersey) City Hall was paved. In many cases, these early pavings were made from naturally occurring \"bituminous rock\", such as at Ritchie Mines in Macfarlan in Ritchie County, West Virginia from 1852 to 1873. In 1876, asphalt-based paving was used to pave Pennsylvania Avenue in Washington DC, in time for the celebration of the national centennial. Asphalt was also used for flooring, paving and waterproofing of baths and swimming pools during the early 20th century, following similar trends in Europe.\n\nCanada has the world's largest deposit of natural bitumen in the Athabasca oil sands, and Canadian First Nations along the Athabasca River had long used it to waterproof their canoes. In 1719, a Cree named Wa-Pa-Su brought a sample for trade to Henry Kelsey of the Hudson’s Bay Company, who was the first recorded European to see it. However, it wasn't until 1787 that fur trader and explorer Alexander MacKenzie saw the Athabasca oil sands and said, \"At about 24 miles from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of 20 feet long may be inserted without the least resistance.\"\n\nThe value of the deposit was obvious from the start, but the means of extracting the bitumen was not. The nearest town, Fort McMurray, Alberta, was a small fur trading post, other markets were far away, and transportation costs were too high to ship the raw bituminous sand for paving. In 1915, Sidney Ells of the Federal Mines Branch experimented with separation techniques and used the product to pave 600 feet of road in Edmonton, Alberta. Other roads in Alberta were paved with material extracted from oil sands, but it was generally not economic. During the 1920s Dr. Karl A. Clark of the Alberta Research Council patented a hot water oil separation process and entrepreneur Robert C. Fitzsimmons built the Bitumount oil separation plant, which between 1925 and 1958 produced up to per day of bitumen using Dr. Clark's method. Most of the bitumen was used for waterproofing roofs, but other uses included fuels, lubrication oils, printers ink, medicines, rust- and acid-proof paints, fireproof roofing, street paving, patent leather, and fence post preservatives. Eventually Fitzsimmons ran out of money and the plant was taken over by the Alberta government. Today the Bitumount plant is a Provincial Historic Site.\n\nBitumen was used in early photographic technology. In 1826 or 1827, it was used by French scientist Joseph Nicéphore Niépce to make the oldest surviving photograph from nature. The bitumen was thinly coated onto a pewter plate which was then exposed in a camera. Exposure to light hardened the bitumen and made it insoluble, so that when it was subsequently rinsed with a solvent only the sufficiently light-struck areas remained. Many hours of exposure in the camera were required, making bitumen impractical for ordinary photography, but from the 1850s to the 1920s it was in common use as a photoresist in the production of printing plates for various photomechanical printing processes.\n\nBitumen was the nemesis of many artists during the 19th century. Although widely used for a time, it ultimately proved unstable for use in oil painting, especially when mixed with the most common diluents, such as linseed oil, varnish and turpentine. Unless thoroughly diluted, bitumen never fully solidifies and will in time corrupt the other pigments with which it comes into contact. The use of bitumen as a glaze to set in shadow or mixed with other colors to render a darker tone resulted in the eventual deterioration of many paintings, for instance those of Delacroix. Perhaps the most famous example of the destructiveness of bitumen is Théodore Géricault's Raft of the Medusa (1818–1819), where his use of bitumen caused the brilliant colors to degenerate into dark greens and blacks and the paint and canvas to buckle.\n\nThe vast majority of refined asphalt is used in construction: primarily as a constituent of products used in paving and roofing applications. According to the requirements of the end use, asphalt is produced to specification. This is achieved either by refining or blending. It is estimated that the current world use of asphalt is approximately 102 million tonnes per year. Approximately 85% of all the asphalt produced is used as the binder in asphalt concrete for roads. It is also used in other paved areas such as airport runways, car parks and footways. Typically, the production of asphalt concrete involves mixing fine and coarse aggregates such as sand, gravel and crushed rock with asphalt, which acts as the binding agent. Other materials, such as recycled polymers (e.g., rubber tyres), may be added to the asphalt to modify its properties according to the application for which the asphalt is ultimately intended.\nA further 10% of global asphalt production is used in roofing applications, where its waterproofing qualities are invaluable.\nThe remaining 5% of asphalt is used mainly for sealing and insulating purposes in a variety of building materials, such as pipe coatings, carpet tile backing and paint.\nAsphalt is applied in the construction and maintenance of many structures, systems, and components, such as the following:\n\nThe largest use of asphalt is for making asphalt concrete for road surfaces; this accounts for approximately 85% of the asphalt consumed in the United States. Asphalt concrete pavement mixes are typically composed of 5% asphalt cement and 95% aggregates (stone, sand, and gravel). Due to its highly viscous nature, asphalt cement must be heated so it can be mixed with the aggregates at the asphalt mixing facility. The temperature required varies depending upon characteristics of the asphalt and the aggregates, but warm-mix asphalt technologies allow producers to reduce the temperature required. There are about 4,000 asphalt concrete mixing plants in the US, and a similar number in Europe.\n\nWhen maintenance is performed on asphalt pavements, such as milling to remove a worn or damaged surface, the removed material can be returned to a facility for processing into new pavement mixtures. The asphalt in the removed material can be reactivated and put back to use in new pavement mixes. With some 95% of paved roads being constructed of or surfaced with asphalt, a substantial amount of asphalt pavement material is reclaimed each year. According to industry surveys conducted annually by the Federal Highway Administration and the National Asphalt Pavement Association, more than 99% of the asphalt removed each year from road surfaces during widening and resurfacing projects is reused as part of new pavements, roadbeds, shoulders and embankments.\n\nAsphalt concrete paving is widely used in airports around the world. Due to the sturdiness and ability to be repaired quickly, it is widely used for runways.\n\nMastic asphalt is a type of asphalt that differs from dense graded asphalt (asphalt concrete) in that it has a higher asphalt (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt concrete, which has only around 5% asphalt. This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground. Mastic asphalt is heated to a temperature of and is spread in layers to form an impervious barrier about thick.\n\nA number of technologies allow asphalt to be mixed at much lower temperatures. These involve mixing with petroleum solvents to form \"cutbacks\" with reduced melting point or mixing with water to turn the asphalt into an emulsion. Asphalt emulsions contain up to 70% asphalt and typically less than 1.5% chemical additives. There are two main types of emulsions with different affinity for aggregates, cationic and anionic. Asphalt emulsions are used in a wide variety of applications. Chipseal involves spraying the road surface with asphalt emulsion followed by a layer of crushed rock, gravel or crushed slag. Slurry seal involves the creation of a mixture of asphalt emulsion and fine crushed aggregate that is spread on the surface of a road. Cold-mixed asphalt can also be made from asphalt emulsion to create pavements similar to hot-mixed asphalt, several inches in depth, and asphalt emulsions are also blended into recycled hot-mix asphalt to create low-cost pavements.\n\nSynthetic crude oil, also known as syncrude, is the output from a bitumen upgrader facility used in connection with oil sand production in Canada. Bituminous sands are mined using enormous (100 ton capacity) power shovels and loaded into even larger (400 ton capacity) dump trucks for movement to an upgrading facility. The process used to extract the bitumen from the sand is a hot water process originally developed by Dr. Karl Clark of the University of Alberta during the 1920s. After extraction from the sand, the bitumen is fed into a bitumen upgrader which converts it into a light crude oil equivalent. This synthetic substance is fluid enough to be transferred through conventional oil pipelines and can be fed into conventional oil refineries without any further treatment. By 2015 Canadian bitumen upgraders were producing over per day of synthetic crude oil, of which 75% was exported to oil refineries in the United States.\n\nIn Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery. A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.\n\nCanadian bitumen does not differ substantially from oils such as Venezuelan extra-heavy and Mexican heavy oil in chemical composition, and the real difficulty is moving the extremely viscous bitumen through oil pipelines to the refinery. Many modern oil refineries are extremely sophisticated and can process non-upgraded bitumen directly into products such as gasoline, diesel fuel, and refined asphalt without any preprocessing. This is particularly common in areas such as the US Gulf coast, where refineries were designed to process Venezuelan and Mexican oil, and in areas such as the US Midwest where refineries were rebuilt to process heavy oil as domestic light oil production declined. Given the choice, such heavy oil refineries usually prefer to buy bitumen rather than synthetic oil because the cost is lower, and in some cases because they prefer to produce more diesel fuel and less gasoline. By 2015 Canadian production and exports of non-upgraded bitumen exceeded that of synthetic crude oil at over per day, of which about 65% was exported to the United States.\n\nBecause of the difficulty of moving crude bitumen through pipelines, non-upgraded bitumen is usually diluted with natural-gas condensate in a form called dilbit or with synthetic crude oil, called synbit. However, to meet international competition, much non-upgraded bitumen is now sold as a blend of multiple grades of bitumen, conventional crude oil, synthetic crude oil, and condensate in a standardized benchmark product such as Western Canadian Select. This sour, heavy crude oil blend is designed to have uniform refining characteristics to compete with internationally marketed heavy oils such as Mexican Mayan or Arabian Dubai Crude.\n\nAsphalt was used starting in the 1960s as an hydrophobic matrix aiming to encapsulate radioactive waste such as medium-activity salts (mainly soluble sodium nitrate and sodium sulfate) produced by the reprocessing of spent nuclear fuels or radioactive sludges from sedimentation ponds. Bituminised radioactive waste containing highly radiotoxic alpha-emitting transuranic elements from nuclear reprocessing plants have been produced at industrial scale in France, Belgium and Japan, but this type of waste conditioning has been abandoned because operational safety issues (risks of fire, as occurred in a bituminisation plant at Tokai Works in Japan) and long-term stability problems related to their geological disposal in deep rock formations. One of the main problem is the swelling of asphalt exposed to radiation and to water. Asphalt swelling is first induced by radiation because of the presence of hydrogen gas bubbles generated by alpha and gamma radiolysis. A second mechanism is the matrix swelling when the encapsulated hygroscopic salts exposed to water or moisture start to rehydrate and to dissolve. The high concentration of salt in the pore solution inside the bituminised matrix is then responsible for osmotic effects inside the bituminised matrix. The water moves in the direction of the concentrated salts, the asphalt acting as a semi-permeable membrane. This also causes the matrix to swell. The swelling pressure due to osmotic effect under constant volume can be as high as 200 bar. If not properly managed, this high pressure can cause fractures in the near field of a disposal gallery of bituminised medium-level waste. When the bituminised matrix has been altered by swelling, encapsulated radionuclides are easily leached by the contact of ground water and released in the geosphere. The high ionic strength of the concentrated saline solution also favours the migration of radionuclides in clay host rocks. The presence of chemically reactive nitrate can also affect the redox conditions prevailing in the host rock by establishing oxidizing conditions, preventing the reduction of redox-sensitive radionuclides. Under their higher valences, radionuclides of elements such as selenium, technetium, uranium, neptunium and plutonium have an higher solubility and are also often present in water as non-retarded anions. This makes the disposal of medium-level bituminised waste very challenging.\n\nDifferent type of asphalt have been used: blown bitumen (partly oxidized with air oxygen at high temperature after distillation, and harder) and direct distillation bitumen (softer). Blown bitumens like Mexphalte, with a high content of saturated hydrocarbons, are more easily biodegraded by microorganisms than direct distillation bitumen, with a low content of saturated hydrocarbons and a high content of aromatic hydrocarbons.\n\nConcrete encapsulation of radwaste is presently considered a safer alternative by the nuclear industry and the waste management organisations.\n\nRoofing shingles account for most of the remaining asphalt consumption. Other uses include cattle sprays, fence-post treatments, and waterproofing for fabrics. Asphalt is used to make Japan black, a lacquer known especially for its use on iron and steel, and it is also used in paint and marker inks by some exterior paint supply companies to increase the weather resistance and permanence of the paint or ink, and to make the color darker. Asphalt is also used to seal some alkaline batteries during the manufacturing process.\n\nAbout 40,000,000 tons were produced in 1984. It is obtained as the \"heavy\" (i.e., difficult to distill) fraction. Material with a boiling point greater than around 500 °C is considered asphalt. Vacuum distillation separates it from the other components in crude oil (such as naphtha, gasoline and diesel). The resulting material is typically further treated to extract small but valuable amounts of lubricants and to adjust the properties of the material to suit applications. In a de-asphalting unit, the crude asphalt is treated with either propane or butane in a supercritical phase to extract the lighter molecules, which are then separated. Further processing is possible by \"blowing\" the product: namely reacting it with oxygen. This step makes the product harder and more viscous.\n\nAsphalt is typically stored and transported at temperatures around . Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture. This mixture is often called \"bitumen feedstock\", or BFS. Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm. The backs of tippers carrying asphalt, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release. Diesel oil is no longer used as a release agent due to environmental concerns.\n\nNaturally occurring crude bitumen impregnated in sedimentary rock is the prime feed stock for petroleum production from \"oil sands\", currently under development in Alberta, Canada. Canada has most of the world's supply of natural bitumen, covering 140,000 square kilometres (an area larger than England), giving it the second-largest proven oil reserves in the world. The Athabasca oil sands are the largest bitumen deposit in Canada and the only one accessible to surface mining, although recent technological breakthroughs have resulted in deeper deposits becoming producible by \"in situ\" methods. Because of oil price increases after 2003, producing bitumen became highly profitable, but as a result of the decline after 2014 it became uneconomic to build new plants again. By 2014, Canadian crude bitumen production averaged about per day and was projected to rise to per day by 2020. The total amount of crude bitumen in Alberta that could be extracted is estimated to be about , which at a rate of would last about 200 years.\n\nAlthough uncompetitive economically, asphalt can be made from nonpetroleum-based renewable resources such as sugar, molasses and rice, corn and potato starches. Asphalt can also be made from waste material by fractional distillation of used motor oil, which is sometimes otherwise disposed of by burning or dumping into landfills. Use of motor oil may cause premature cracking in colder climates, resulting in roads that need to be repaved more frequently.\n\nNonpetroleum-based asphalt binders can be made light-colored. Lighter-colored roads absorb less heat from solar radiation, reducing their contribution to the urban heat island effect. Parking lots that use asphalt alternatives are called green parking lots.\n\nSelenizza is a naturally occurring solid hydrocarbon bitumen found in native deposits in Selenice, in Albania, the only European asphalt mine still in use. The bitumen is found in the form of veins, filling cracks in a more or less horizontal direction. The bitumen content varies from 83% to 92% (soluble in carbon disulphide), with a penetration value near to zero and a softening point (ring and ball) around 120 °C. The insoluble matter, consisting mainly of silica ore, ranges from 8% to 17%.\nAlbanian bitumen extraction has a long history and was practiced in an organized way by the Romans. After centuries of silence, the first mentions of Albanian bitumen appeared only in 1868, when the Frenchman Coquand published the first geological description of the deposits of Albanian bitumen. In 1875, the exploitation rights were granted to the Ottoman government and in 1912, they were transferred to the Italian company Simsa. Since 1945, the mine was exploited by the Albanian government and from 2001 to date, the management passed to a French company, which organized the mining process for the manufacture of the natural bitumen on an industrial scale.\n\nToday the mine is predominantly exploited in an open pit quarry but several of the many underground mines (deep and extending over several km) still remain viable. Selenizza is produced primarily in granular form, after melting the bitumen pieces selected in the mine.\nSelenizza is mainly used as an additive in the road construction sector. It is mixed with traditional asphalt to improve both the viscoelastic properties and the resistance to ageing. It may be blended with the hot asphalt in tanks, but its granular form allows it to be fed in the mixer or in the recycling ring of normal asphalt plants. Other typical applications include the production of mastic asphalts for sidewalks, bridges, car-parks and urban roads as well as drilling fluid additives for the oil and gas industry. Selenizza is available in powder or in granular material of various particle sizes and is packaged in sacks or in thermal fusible polyethylene bags.\n\nA life-cycle assessment study of the natural selenizza compared with petroleum asphalt has shown that the environmental impact of the selenizza is about half the impact of the road asphalt produced in oil refineries in terms of carbon dioxide emission.\n\nPeople can be exposed to asphalt in the workplace by breathing in fumes or skin absorption. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit of 5 mg/m over a 15-minute period. \n\nAsphalt is basically an inert material that must be heated or diluted to a point where it becomes workable for the production of materials for paving, roofing, and other applications. In examining the potential health hazards associated with asphalt, the International Agency for Research on Cancer (IARC) determined that it is the application parameters, predominantly temperature, that affect occupational exposure and the potential bioavailable carcinogenic hazard/risk of the asphalt emissions. In particular, temperatures greater than 199 °C (390 °F), were shown to produce a greater exposure risk than when asphalt was heated to lower temperatures, such as those typically used in asphalt pavement mix production and placement. IARC has classified asphalt as a Class 2B possible carcinogen. \n\n\n"}
{"id": "659", "url": "https://en.wikipedia.org/wiki?curid=659", "title": "American National Standards Institute", "text": "American National Standards Institute\n\nThe American National Standards Institute (ANSI, ) is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States. The organization also coordinates U.S. standards with international standards so that American products can be used worldwide.\n\nANSI accredits standards that are developed by representatives of other standards organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.\n\nThe organization's headquarters are in Washington, D.C. ANSI's operations office is located in New York City. The ANSI annual operating budget is funded by the sale of publications, membership dues and fees, accreditation services, fee-based programs, and international standards programs.\n\nANSI was originally formed in 1918, when five engineering societies and three government agencies founded the American Engineering Standards Committee (AESC). In 1928, the AESC became the American Standards Association (ASA). In 1966, the ASA was reorganized and became United States of America Standards Institute (USASI). The present name was adopted in 1969.\n\nPrior to 1918, these five founding engineering societies:\nhad been members of the United Engineering Society (UES). At the behest of the AIEE, they invited the U.S. government Departments of War, Navy (combined in 1947 to become the Department of Defense or DOD) and Commerce to join in founding a national standards organization.\n\nAccording to Adam Stanton, the first permanent secretary and head of staff in 1919, AESC started as an ambitious program and little else. Staff for the first year consisted of one executive, Clifford B. LePage, who was on loan from a founding member, ASME. An annual budget of $7,500 was provided by the founding bodies.\n\nIn 1931, the organization (renamed ASA in 1928) became affiliated with the U.S. National Committee of the International Electrotechnical Commission (IEC), which had been formed in 1904 to develop electrical and electronics standards.\n\nANSI's members are government agencies, organizations, corporations, academic and international bodies, and individuals. In total, the Institute represents the interests of more than 125,000 companies and 3.5 million professionals.\n\nThough ANSI itself does not develop standards, the Institute oversees the development and use of standards by accrediting the procedures of standards developing organizations. ANSI accreditation signifies that the procedures used by standards developing organizations meet the Institute's requirements for openness, balance, consensus, and due process.\n\nANSI also designates specific standards as American National Standards, or ANS, when the Institute determines that the standards were developed in an environment that is equitable, accessible and responsive to the requirements of various stakeholders.\n\nVoluntary consensus standards quicken the market acceptance of products while making clear how to improve the safety of those products for the protection of consumers. There are approximately 9,500 American National Standards that carry the ANSI designation.\n\nThe American National Standards process involves:\n\nIn addition to facilitating the formation of standards in the United States, ANSI promotes the use of U.S. standards internationally, advocates U.S. policy and technical positions in international and regional standards organizations, and encourages the adoption of international standards as national standards where appropriate.\n\nThe Institute is the official U.S. representative to the two major international standards organizations, the International Organization for Standardization (ISO), as a founding member, and the International Electrotechnical Commission (IEC), via the U.S. National Committee (USNC). ANSI participates in almost the entire technical program of both the ISO and the IEC, and administers many key committees and subgroups. In many instances, U.S. standards are taken forward to ISO and IEC, through ANSI or the USNC, where they are adopted in whole or in part as international standards.\n\nThe Institute administers nine standards panels:\n\nEach of the panels works to identify, coordinate, and harmonize voluntary standards relevant to these areas.\n\nIn 2009, ANSI and the National Institute of Standards and Technology (NIST) formed the Nuclear Energy Standards Coordination Collaborative (NESCC). NESCC is a joint initiative to identify and respond to the current need for standards in the nuclear industry.\n\n\n\n\n"}
{"id": "661", "url": "https://en.wikipedia.org/wiki?curid=661", "title": "Argument (disambiguation)", "text": "Argument (disambiguation)\n\nIn logic and philosophy, an argument is an attempt to persuade someone of something, or give evidence or reasons for accepting a particular conclusion.\n\nArgument may also refer to: \n\n\n\n\n"}
{"id": "662", "url": "https://en.wikipedia.org/wiki?curid=662", "title": "Apollo 11", "text": "Apollo 11\n\nApollo 11 was the spaceflight that landed the first two humans on the Moon. Mission commander Neil Armstrong and pilot Buzz Aldrin, both American, landed the lunar module \"Eagle\" on July 20, 1969, at 20:18 UTC. Armstrong became the first to step onto the lunar surface six hours after landing on July 21 at 02:56:15 UTC; Aldrin joined him about 20 minutes later. They spent about two and a quarter hours together outside the spacecraft, and collected of lunar material to bring back to Earth. Michael Collins piloted the command module \"Columbia\" alone in lunar orbit while they were on the Moon's surface. Armstrong and Aldrin spent just under a day on the lunar surface before rejoining \"Columbia\" in lunar orbit.\n\nApollo 11 was launched by a Saturn V rocket from Kennedy Space Center in Merritt Island, Florida, on July 16, and was the fifth manned mission of NASA's Apollo program. The Apollo spacecraft had three parts: a command module (CM) with a cabin for the three astronauts, and the only part that landed back on Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages – a lower stage for landing on the Moon, and an upper stage to place the astronauts back into lunar orbit. \n\nAfter being sent toward the Moon by the Saturn V's upper stage, the astronauts separated the spacecraft from it and traveled for three days until they entered into lunar orbit. Armstrong and Aldrin then moved into the lunar module \"Eagle\" and landed in the Sea of Tranquility. They stayed a total of about 21.5 hours on the lunar surface. The astronauts used \"Eagle\"'s upper stage to lift off from the lunar surface and rejoin Collins in the command module. They jettisoned \"Eagle\" before they performed the maneuvers that blasted them out of lunar orbit on a trajectory back to Earth. They returned to Earth and landed in the Pacific Ocean on July 24.\n\nThe landing was broadcast on live TV to a worldwide audience. Armstrong stepped onto the lunar surface and described the event as \"one small step for [a] man, one giant leap for mankind.\" Apollo 11 effectively ended the Space Race and fulfilled a national goal proposed in 1961 by U.S. President John F. Kennedy: \"before this decade is out, of landing a man on the Moon and returning him safely to the Earth.\"\n\nApollo 11 was the second all-veteran multi-person crew (the first being Apollo 10) in human spaceflight history. A previous solo veteran flight had been made on Soyuz 1 in 1967 by Soviet cosmonaut Vladimir Komarov.\n\nCollins was originally slated to be the Command Module Pilot (CMP) on Apollo 8 but was removed when he required surgery on his back and was replaced by Jim Lovell, his backup for that flight. After Collins was medically cleared, he took what would have been Lovell's spot on Apollo 11; as a veteran of Apollo 8, Lovell was transferred to Apollo 11's backup crew and promoted to backup commander.\n\nIn early 1969, Anders accepted a job with the National Space Council effective August 1969 and announced that he would retire as an astronaut on that date. At that point Ken Mattingly was moved from the support crew into parallel training with Anders as backup Command Module Pilot in case Apollo 11 was delayed past its intended July launch (at which point Anders would be unavailable if needed) and would later join Lovell's crew and ultimately be assigned as the original Apollo 13 CMP.\n\n\n\nAfter the crew of Apollo 10 named their spacecraft \"Charlie Brown\" and \"Snoopy\", assistant manager for public affairs Julian Scheer wrote to Manned Spacecraft Center director George M. Low to suggest the Apollo 11 crew be less flippant in naming their craft. During early mission planning, the names \"Snowcone\" and \"Haystack\" were used and put in the news release, but the crew later decided to change them.\n\nThe Command Module was named \"Columbia\" after the \"Columbiad\", the giant cannon shell \"spacecraft\" fired by a giant cannon (also from Florida) in Jules Verne's 1865 novel \"From the Earth to the Moon\". The Lunar Module was named \"Eagle\" for the national bird of the United States, the bald eagle, which is featured prominently on the mission insignia.\n\nThe Apollo 11 mission insignia was designed by Collins, who wanted a symbol for \"peaceful lunar landing by the United States\". He chose an eagle as the symbol, put an olive branch in its beak, and drew a lunar background with the Earth in the distance. NASA officials said the talons of the eagle looked too \"warlike\" and after some discussion, the olive branch was moved to the claws. The crew decided the Roman numeral XI would not be understood in some nations and went with \"Apollo 11\"; they decided not to put their names on the patch, so it would \"be representative of \"everyone\" who had worked toward a lunar landing\". All colors are natural, with blue and gold borders around the patch.\n\nWhen the Eisenhower dollar coin was released in 1971, the patch design provided the eagle for its reverse side. The design was also used for the smaller Susan B. Anthony dollar unveiled in 1979, ten years after the Apollo 11 mission.\n\nNeil Armstrong's personal preference kit carried a piece of wood from the Wright brothers' 1903 airplane's left propeller and a piece of fabric from its wing, along with a diamond-studded astronaut pin originally given to Deke Slayton by the widows of the Apollo 1 crew. This pin had been intended to be flown on Apollo 1 and given to Slayton after the mission but following the disastrous launch pad fire and subsequent funerals, the widows gave the pin to Slayton and Armstrong took it on Apollo 11.\n\nIn addition to many people crowding highways and beaches near the launch site, millions watched the event on television, with NASA Chief of Public Information Jack King providing commentary. President Richard M. Nixon viewed the proceedings from the Oval Office of the White House.\n\nA Saturn V launched Apollo 11 from Launch Pad 39A, part of the Launch Complex 39 site at the Kennedy Space Center on July 16, 1969, at 13:32:00 UTC (9:32:00 a.m. EDT local time). It entered Earth orbit, at an altitude of by , twelve minutes later. After one and a half orbits, the S-IVB third-stage engine pushed the spacecraft onto its trajectory toward the Moon with the trans-lunar injection (TLI) burn at 16:22:13 UTC. About 30 minutes later, the transposition, docking, and extraction maneuver was performed: this involved separating the Apollo Command/Service Module (CSM) from the spent rocket stage, turning around, and docking with the Lunar Module still attached to the stage. After the Lunar Module was extracted, the combined spacecraft headed for the Moon, while the rocket stage flew on a trajectory past the Moon and into orbit around the Sun.\n\nOn July 19 at 17:21:50 UTC, Apollo 11 passed behind the Moon and fired its service propulsion engine to enter lunar orbit. In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquility (Mare Tranquillitatis) about southwest of the crater Sabine D (0.67408N, 23.47297E). The landing site was selected in part because it had been characterized as relatively flat and smooth by the automated \"Ranger 8\" and \"Surveyor 5\" landers along with the \"Lunar Orbiter\" mapping spacecraft and unlikely to present major landing or extravehicular activity (EVA) challenges.\n\nOn July 20, 1969, the Lunar Module \"Eagle\" separated from the Command Module \"Columbia\". Collins, alone aboard \"Columbia\", inspected \"Eagle\" as it pirouetted before him to ensure the craft was not damaged.\n\nAs the descent began, Armstrong and Aldrin found that they were passing landmarks on the surface four seconds early and reported that they were \"long\"; they would land miles west of their target point.\n\nFive minutes into the descent burn, and above the surface of the Moon, the LM navigation and guidance computer distracted the crew with the first of several unexpected \"1202\" and \"1201\" program alarms. Inside Mission Control Center in Houston, Texas, computer engineer Jack Garman told guidance officer Steve Bales it was safe to continue the descent, and this was relayed to the crew. The program alarms indicated \"executive overflows\", meaning the guidance computer could not complete all of its tasks in real time and had to postpone some of them.\n\nWhen Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a diameter crater (later determined to be West crater, named for its location in the western part of the originally planned landing ellipse). Armstrong took semi-automatic control and, with Aldrin calling out altitude and velocity data, landed at 20:17:40 UTC on Sunday July 20 with about 25 seconds of fuel left.\n\nApollo 11 landed with less fuel than other missions, and the astronauts encountered a premature low fuel warning. This was later found to be the result of greater propellant 'slosh' than expected, uncovering a fuel sensor. On subsequent missions, extra anti-slosh baffles were added to the tanks to prevent this.\n\nThroughout the descent, Aldrin had called out navigation data to Armstrong, who was busy piloting the LM. A few moments before the landing, a light informed Aldrin that at least one of the probes hanging from \"Eagle\" footpads had touched the surface, and he said: \"Contact light!\" Three seconds later, \"Eagle\" landed and Armstrong said \"Shutdown.\" Aldrin immediately said \"Okay, engine stop. ACA – out of detent.\" Armstrong acknowledged \"Out of detent. Auto\" and Aldrin continued \"Mode control – both auto. Descent engine command override off. Engine arm – off. 413 is in.\"\n\nCharles Duke, CAPCOM during the landing phase, acknowledged their landing by saying \"We copy you down, \"Eagle\".\"\n\nArmstrong acknowledged Aldrin's completion of the post landing checklist with \"Engine arm is off\", before responding to Duke with the words, \"Houston, Tranquility Base here. The \"Eagle\" has landed.\" Armstrong's unrehearsed change of call sign from \"Eagle\" to \"Tranquility Base\" emphasized to listeners that landing was complete and successful. Duke mispronounced his reply as he expressed the relief at Mission Control: \"Roger, Twan— Tranquility, we copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot.\"\n\nTwo and a half hours after landing, before preparations began for the EVA, Aldrin radioed to Earth:\n\nThe schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period as they had been awake since early morning. However, they elected to forgo the sleep period and begin the preparations for the EVA early, thinking that they would be unable to sleep.\n\nThe astronauts planned placement of the Early Apollo Scientific Experiment Package (EASEP) and the U.S. flag by studying their landing site through \"Eagle\" twin triangular windows, which gave them a 60° field of view. Preparation required longer than the two hours scheduled. Armstrong initially had some difficulties squeezing through the hatch with his Portable Life Support System (PLSS). According to veteran Moon-walker John Young, a redesign of the LM to incorporate a smaller hatch had not been followed by a redesign of the PLSS backpack, so some of the highest heart rates recorded from Apollo astronauts occurred during LM egress and ingress.\n\nSeveral books indicate early mission timelines had Buzz Aldrin rather than Neil Armstrong as the first man on the Moon.\n\nAt 02:39 UTC on Monday July 21, 1969, Armstrong opened the hatch, and at 02:51 UTC began his descent to the lunar surface. The Remote Control Unit controls on his chest kept him from seeing his feet. Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the Modular Equipment Stowage Assembly (MESA) folded against \"Eagle\" side and activate the TV camera, and at 02:56:15 UTC he set his left foot on the surface. The first landing used slow-scan television incompatible with commercial TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor, significantly reducing the quality of the picture. The signal was received at Goldstone in the United States but with better fidelity by Honeysuckle Creek Tracking Station in Australia. Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia. Despite some technical and weather difficulties, ghostly black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth. Although copies of this video in broadcast format were saved and are widely available, recordings of the original slow scan source transmission from the lunar surface were accidentally destroyed during routine magnetic tape re-use at NASA.\nWhile still on the ladder, Armstrong uncovered a plaque mounted on the LM Descent Stage bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Nixon. The inscription read:\nAfter describing the surface dust as \"very fine-grained\" and \"almost like a powder,\" six and a half hours after landing, Armstrong stepped off \"Eagle\" footpad and declared, \"That's one small step for [a] man, one giant leap for mankind.\"\n\nArmstrong intended to say \"That's one small step for a man\", but the word \"\"a\"\" is not audible in the transmission, and thus was not initially reported by most observers of the live broadcast. When later asked about his quote, Armstrong said he believed he said \"for a man\", and subsequent printed versions of the quote included the \"a\" in square brackets. One explanation for the absence may be that his accent caused him to slur the words \"for a\" together; another is the intermittent nature of the audio and video links to Earth, partly because of storms near Parkes Observatory. More recent digital analysis of the tape claims to reveal the \"a\" may have been spoken but obscured by static.\n\nAbout seven minutes after stepping onto the Moon's surface, Armstrong collected a contingency soil sample using a sample bag on a stick. He then folded the bag and tucked it into a pocket on his right thigh. This was to guarantee there would be some lunar soil brought back in case an emergency required the astronauts to abandon the EVA and return to the LM.\n\nTwelve minutes after the contingency sample was collected, Aldrin joined Armstrong on the surface, and described the view with the simple phrase, \"Magnificent desolation.\"\nIn addition to fulfilling President Kennedy's mandate to land a man on the Moon before the end of the 1960s, Apollo 11 was an engineering test of the Apollo system; therefore, Armstrong snapped photos of the LM so engineers would be able to judge its post-landing condition. He removed the TV camera from the MESA and made a panoramic sweep, then mounted it on a tripod from the LM. The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA.\n\nArmstrong said that moving in the lunar gravity, one-sixth of Earth's, was \"even perhaps easier than the simulations ... It's absolutely no trouble to walk around.\" Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops. The PLSS backpack created a tendency to tip backwards, but neither astronaut had serious problems maintaining balance. Loping became the preferred method of movement. The astronauts reported that they needed to plan their movements six or seven steps ahead. The fine soil was quite slippery. Aldrin remarked that moving from sunlight into \"Eagle\" shadow produced no temperature change inside the suit, though the helmet was warmer in sunlight, so he felt cooler in shadow.\n\nThe astronauts planted a specially designed U.S. flag on the lunar surface, in clear view of the TV camera. Some time later, President Richard Nixon spoke to them through a telephone-radio transmission which Nixon called \"the most historic phone call ever made from the White House.\" Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief, to respect the lunar landing as Kennedy's legacy. Armstrong thanked the President, and gave a brief reflection on the significance of the moment:\nNixon: Hello, Neil and Buzz. I'm talking to you by telephone from the Oval Room at the White House. And this certainly has to be the most historic telephone call ever made. I just can't tell you how proud we all are of what you've done. For every American, this has to be the proudest day of our lives. And for people all over the world, I am sure they too join with Americans in recognizing what an immense feat this is. Because of what you have done, the heavens have become a part of man's world. And as you talk to us from the Sea of Tranquility, it inspires us to redouble our efforts to bring peace and tranquility to Earth. For one priceless moment in the whole history of man, all the people on this Earth are truly one: one in their pride in what you have done, and one in our prayers that you will return safely to Earth.\n\nArmstrong: Thank you, Mr. President. It's a great honor and privilege for us to be here, representing not only the United States, but men of peace of all nations, and with interest and curiosity, and men with a vision for the future. It's an honor for us to be able to participate here today.\n\nThe MESA failed to provide a stable work platform and was in shadow, slowing work somewhat. As they worked, the moonwalkers kicked up gray dust which soiled the outer part of their suits, the integrated thermal meteoroid garment.\n\nThey deployed the EASEP, which included a passive seismograph and a Lunar Ranging Retroreflector (LRRR). Then Armstrong walked from the LM to snap photos at the rim of Little West Crater while Aldrin collected two core tubes. He used the geological hammer to pound in the tubes – the only time the hammer was used on Apollo 11. The astronauts then collected rock samples using scoops and tongs on extension handles. Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 minutes.\nThree new minerals were discovered in the rock samples collected by the astronauts: armalcolite, tranquillityite, and pyroxferroite. Armalcolite was named after Armstrong, Aldrin, and Collins.\n\nDuring this period, Mission Control used a coded phrase to warn Armstrong that his metabolic rates were high and that he should slow down. He was moving rapidly from task to task as time ran out. However, as metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension. In a 2010 interview, Armstrong, who had walked a maximum of from the LM, explained that NASA limited the first moonwalk's time and distance because there was no empirical proof of how much cooling water the astronauts' PLSS backpacks would consume to handle their body heat generation while working on the Moon.\n\nAldrin entered \"Eagle\" first. With some difficulty the astronauts lifted film and two sample boxes containing of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor. Armstrong reminded Aldrin of a bag of memorial items in his suit pocket sleeve, and Aldrin tossed the bag down; Armstrong then jumped to the ladder's third rung and climbed into the LM. After transferring to LM life support, the explorers lightened the ascent stage for return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, one Hasselblad camera, and other equipment. They then pressurized the LM and settled down to sleep.\nPresident Nixon's speech writer William Safire had prepared \"In Event of Moon Disaster\" for the President to read on television in the event the Apollo 11 astronauts were stranded on the Moon. The contingency plan originated in a memo from Safire to Nixon's White House Chief of Staff H. R. Haldeman, in which Safire suggested a protocol the administration might follow in reaction to such a disaster. According to the plan, Mission Control would \"close down communications\" with the LM, and a clergyman would \"commend their souls to the deepest of the deep\" in a public ritual likened to burial at sea. The last line of the prepared text contained an allusion to Rupert Brooke's First World War poem, \"The Soldier\". The plan included presidential telephone calls to the astronauts' wives.\n\nWhile moving within the cabin, Aldrin accidentally damaged the circuit breaker that would arm the main engine for lift off from the Moon. There was concern this would prevent firing the engine, stranding them on the Moon. Fortunately, a felt-tip pen was sufficient to activate the switch. Had this not worked, the Lunar Module circuitry could have been reconfigured to allow firing the ascent engine.\n\nAfter about seven hours of rest, the crew was awakened by Houston to prepare for the return flight. Two and a half hours later, at 17:54 UTC, they lifted off in \"Eagle\" ascent stage to rejoin Collins aboard \"Columbia\" in lunar orbit.\n\nAfter more than 21½ total hours on the lunar surface, they had left behind scientific instruments that included a retroreflector array used for the Lunar Laser Ranging Experiment and a Passive Seismic Experiment Package used to measure moonquakes. They also left an Apollo 1 mission patch, and a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace and a silicon message disk. The disk carries the goodwill statements by Presidents Eisenhower, Kennedy, Johnson, and Nixon and messages from leaders of 73 countries around the world. The disc also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and present top management. (In his 1989 book, \"Men from Earth\", Aldrin says that the items included Soviet medals commemorating Cosmonauts Vladimir Komarov and Yuri Gagarin.) Also, according to Deke Slayton's book \"Moonshot\", Armstrong carried with him a special diamond-studded astronaut pin from Slayton.\n\nFilm taken from the LM Ascent Stage upon liftoff from the Moon reveals the American flag, planted some from the descent stage, whipping violently in the exhaust of the ascent stage engine. Aldrin looked up in time to witness the flag topple: \"The ascent stage of the LM separated ... I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over.\" Subsequent Apollo missions usually planted the American flags at least from the LM to prevent them being blown over by the ascent engine exhaust.\n\nAfter rendezvous with \"Columbia\", \"Eagle\"s ascent stage was jettisoned into lunar orbit on July 21, 1969, at 23:41 UTC. Just before the Apollo 12 flight, it was noted that \"Eagle\" was still likely to be orbiting the Moon. Later NASA reports mentioned that \"Eagle\" orbit had decayed, resulting in it impacting in an \"uncertain location\" on the lunar surface. The location is uncertain because the \"Eagle\" ascent stage was not tracked after it was jettisoned, and the lunar gravity field is sufficiently non-uniform to make the orbit of the spacecraft unpredictable after a short time. NASA estimated that the orbit had decayed within months and would have impacted on the Moon.\n\nOn July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented: ... The Saturn V rocket which put us in orbit is an incredibly complicated piece of machinery, every piece of which worked flawlessly ... We have always had confidence that this equipment will work properly. All this is possible only through the blood, sweat, and tears of a number of a people ... All you see is the three of us, but beneath the surface are thousands and thousands of others, and to all of those, I would like to say, \"Thank you very much.\"\n\nAldrin added: This has been far more than three men on a mission to the Moon; more, still, than the efforts of a government and industry team; more, even, than the efforts of one nation. We feel that this stands as a symbol of the insatiable curiosity of all mankind to explore the unknown ... Personally, in reflecting on the events of the past several days, a verse from Psalms comes to mind. \"When I consider the heavens, the work of Thy fingers, the Moon and the stars, which Thou hast ordained; What is man that Thou art mindful of him?\"\n\nArmstrong concluded: The responsibility for this flight lies first with history and with the giants of science who have preceded this effort; next with the American people, who have, through their will, indicated their desire; next with four administrations and their Congresses, for implementing that will; and then, with the agency and industry teams that built our spacecraft, the Saturn, the Columbia, the Eagle, and the little EMU, the spacesuit and backpack that was our small spacecraft out on the lunar surface. We would like to give special thanks to all those Americans who built the spacecraft; who did the construction, design, the tests, and put their hearts and all their abilities into those craft. To those people tonight, we give a special thank you, and to all the other people that are listening and watching tonight, God bless you. Good night from Apollo 11.\n\nOn the return to Earth, a bearing at the Guam tracking station failed, potentially preventing communication on the last segment of the Earth return. A regular repair was not possible in the available time but the station director, Charles Force, had his ten-year-old son Greg use his small hands to reach into the housing and pack it with grease. Greg later was thanked by Armstrong.\n\nOn July 24, the astronauts returned home aboard the Command Module \"Columbia\" just before dawn local time (16:51 UTC) at , in the Pacific Ocean east of Wake Island, south of Johnston Atoll, and from the recovery ship, .\n\nAt 16:44 UTC the drogue parachutes had been deployed and seven minutes later the Command Module struck the water forcefully. During splashdown, the Command Module landed upside down but was righted within 10 minutes by flotation bags triggered by the astronauts. \"Everything's okay. Our checklist is complete. Awaiting swimmers\", was Armstrong's last official transmission from the \"Columbia\". A diver from the Navy helicopter hovering above attached a sea anchor to the Command Module to prevent it from drifting. Additional divers attached flotation collars to stabilize the module and position rafts for astronaut extraction. Though the chance of bringing back pathogens from the lunar surface was considered remote, it was considered a possibility and NASA took great precautions at the recovery site. Divers provided the astronauts with Biological Isolation Garments (BIGs) which were worn until they reached isolation facilities on board the \"Hornet\". Additionally, astronauts were rubbed down with a sodium hypochlorite solution and the Command Module wiped with Betadine to remove any lunar dust that might be present. The raft containing decontamination materials was then intentionally sunk.\n\nA second Sea King helicopter hoisted the astronauts aboard one by one, where a NASA flight surgeon gave each a brief physical check during the trip back to the \"Hornet\".\n\nAfter touchdown on the \"Hornet\", the astronauts exited the helicopter, leaving the flight surgeon and three crewmen. The helicopter was then lowered into hangar bay #2 where the astronauts walked the to the Mobile Quarantine Facility (MQF) where they would begin their 21 days of quarantine. This practice would continue for two more Apollo missions, Apollo 12 and Apollo 14, before the Moon was proven to be barren of life and the quarantine process dropped.\n\nPresident Richard Nixon was aboard \"Hornet\" to personally welcome the astronauts back to Earth. He told the astronauts, \"As a result of what you've done, the world has never been closer together before.\" After Nixon departed, the \"Hornet\" was brought alongside the five-ton Command Module where it was placed aboard by the ship's crane, placed on a dolly and moved next to the MQF. The \"Hornet\" sailed for Pearl Harbor where the Command Module and MQF were airlifted to the Manned Spacecraft Center.\n\nIn accordance with the recently promulgated Extra-Terrestrial Exposure Law, the astronauts were placed in quarantine for fear that the Moon might contain undiscovered pathogens and that the astronauts might have been exposed to them during their Moon walks. However, after almost three weeks in confinement (first in their trailer and later in the Lunar Receiving Laboratory at the Manned Spacecraft Center), the astronauts were given a clean bill of health. On August 10, 1969, the astronauts exited quarantine.\n\nOn August 13, they rode in parades in their honor in New York, Chicago, and Los Angeles. On the same evening in Los Angeles there was an official State Dinner to celebrate the flight, attended by members of Congress, 44 governors, the Chief Justice of the United States, and ambassadors from 83 nations at the Century Plaza Hotel. President Richard Nixon and Vice President Spiro T. Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom. This celebration was the beginning of a 45-day \"Giant Leap\" tour that brought the astronauts to 25 foreign countries and included visits with prominent leaders such as Queen Elizabeth II of the United Kingdom. Many nations honored the first manned Moon landing with special features in magazines or by issuing Apollo 11 commemorative postage stamps or coins.\n\nOn September 16, 1969, the three astronauts spoke before a joint session of Congress on Capitol Hill. They presented two US flags, one to the House of Representatives and the other to the Senate, that had been carried to the surface of the Moon with them.\n\nThe Soviet Union was secretly attempting to compete with the US in landing a man on the Moon but had been hampered by repeated failures in development of a launcher comparable to the Saturn V. Meanwhile, they tried to beat the US to return lunar material to the Earth by means of unmanned probes. On July 13, three days before Apollo 11's launch, they launched Luna 15, which reached lunar orbit before Apollo 11. During descent, a malfunction caused Luna 15 to crash in Mare Crisium about two hours before Armstrong and Aldrin took off from the Moon's surface to begin their voyage home. The Jodrell Bank Observatory radio telescope in England was later discovered to have recorded transmissions from Luna 15 during its descent, and this was published in July 2009 on the 40th anniversary of Apollo 11.\n\nThe Command Module \"Columbia\" is displayed at the National Air and Space Museum, Washington, D.C. It is in the central \"Milestones of Flight\" exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the Wright Flyer, the \"Spirit of St. Louis\", the Bell X-1, the North American X-15, Mercury spacecraft \"Friendship 7\", and Gemini 4. Armstrong's and Aldrin's space suits are displayed in the museum's \"Apollo to the Moon\" exhibit. The quarantine trailer, the flotation collar, and the righting spheres are displayed at the Smithsonian's Steven F. Udvar-Hazy Center annex near Washington Dulles International Airport in Virginia.\n\nIn 2009, the Lunar Reconnaissance Orbiter (LRO) imaged the various Apollo landing sites on the surface of the Moon, for the first time with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts.\n\nIn March 2012 a team of specialists financed by Amazon founder Jeff Bezos located the F-1 engines that launched Apollo 11 into space. The engines were found below the Atlantic Ocean's surface through the use of advanced sonar scanning. His team brought parts of two of the five engines to the surface. In July 2013, a conservator discovered a serial number under the rust on one of the engines raised from the Atlantic, which NASA confirmed was from the Apollo 11 launch.\n\nOn July 15, 2009, Life.com released a photo gallery of previously unpublished photos of the astronauts taken by \"Life\" photographer Ralph Morse prior to the Apollo 11 launch.\n\nFrom July 16–24, 2009, NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred.\nIn addition, it is in the process of restoring the video footage and has released a preview of key moments.\n\nOn July 20, 2009, the crew of Armstrong, Aldrin, and Collins met with U.S. President Barack Obama at the White House. \"We expect that there is, as we speak, another generation of kids out there who are looking up at the sky and are going to be the next Armstrong, Collins and Aldrin\", Obama said. \"We want to make sure that NASA is going to be there for them when they want to take their journey.\"\n\nThe John F. Kennedy Presidential Library and Museum set up a Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon.\n\nA group of British scientists interviewed as part of the anniversary events reflected on the significance of the Moon landing:\n\nIt was carried out in a technically brilliant way with risks taken ... that would be inconceivable in the risk-averse world of today ... The Apollo programme is arguably the greatest technical achievement of mankind to date ... nothing since Apollo has come close [to] the excitement that was generated by those astronauts – Armstrong, Aldrin and the 10 others who followed them.\n\nOn August 7, 2009, an act of Congress awarded the three astronauts a Congressional Gold Medal, the highest civilian award in the United States. The bill was sponsored by Florida Sen. Bill Nelson and Florida Rep. Alan Grayson.\n\nIn July 2010, air-to-ground voice recordings and film footage shot in Mission Control during the Apollo 11 powered descent and landing was re-synchronised and released for the first time.\n\n\n\nFor young readers\n\n\nNASA reports\n\nMultimedia\n"}
{"id": "663", "url": "https://en.wikipedia.org/wiki?curid=663", "title": "Apollo 8", "text": "Apollo 8\n\nApollo 8, the second human spaceflight mission in the United States Apollo space program, was launched on December 21, 1968, and became the first crewed spacecraft to leave Earth orbit, reach the Earth's Moon, orbit it and return safely to Earth. The three-astronaut crew — Commander Frank Borman, Command Module Pilot James Lovell, and Lunar Module Pilot William Anders — became the first humans to: travel beyond low Earth orbit; escape Earth's gravity; see Earth as a whole planet; enter the gravity well of another celestial body (Earth's moon); orbit another celestial body (Earth's moon); directly see the far side of the Moon with their own eyes; witness an Earthrise; escape the gravity of another celestial body (Earth's moon); and re-enter the gravitational well of Earth. The 1968 mission, the third flight of the Saturn V rocket and that rocket's first crewed launch, was also the first human spaceflight launch from the Kennedy Space Center, Florida, located adjacent to Cape Canaveral Air Force Station.\n\nOriginally planned as a second Lunar Module/Command Module test in an elliptical medium Earth orbit in early 1969, the mission profile was changed in August 1968 to a more ambitious Command Module-only lunar orbital flight to be flown in December, because the Lunar Module was not yet ready to make its first flight. This meant Borman's crew was scheduled to fly two to three months sooner than originally planned, leaving them a shorter time for training and preparation, thus placing more demands than usual on their time and discipline.\n\nApollo 8 took three days to travel to the Moon. It orbited ten times over the course of 20 hours, during which the crew made a Christmas Eve television broadcast where they read the first 10 verses from the Book of Genesis. At the time, the broadcast was the most watched TV program ever. Apollo 8's successful mission paved the way for Apollo 11 to fulfill U.S. President John F. Kennedy's goal of landing a man on the Moon before the end of the 1960s. The Apollo 8 astronauts returned to Earth on December 27, 1968, when their spacecraft splashed down in the Northern Pacific Ocean. The crew was named \"Time\" magazine's \"Men of the Year\" for 1968 upon their return.\n\nLovell was originally the CMP on the back-up crew, with Michael Collins as the prime crew's CMP. However, Collins was replaced in July 1968, after suffering a cervical disc herniation that required surgery to repair.\n\nThis crew was unique among pre-shuttle era missions in that the commander was not the most experienced member of the crew, as Lovell had flown twice before, on Gemini VII and Gemini XII. This was also the first case of the rarity of an astronaut who had commanded a spaceflight mission subsequently flying as a non-commander, as Lovell had previously commanded Gemini XII.\n\nOn a lunar mission, the Command Module Pilot (CMP) was assigned the role of navigator, while the Lunar Module Pilot (LMP) was assigned the role of flight engineer, responsible for monitoring all spacecraft systems, even if the flight didn't include a Lunar Module.\n\nEdwin \"Buzz\" Aldrin was originally the backup LMP. When Lovell was rotated to the prime crew, no one with experience on CSM-103 (the specific spacecraft used for the mission) was available, so Aldrin was moved to CMP and Fred Haise brought in as backup LMP. Neil Armstrong went on to command Apollo 11, where Aldrin was returned to the LMP position and Collins was assigned as CMP. Haise was rotated out of the crew and onto the backup crew of Apollo 11 as LMP.\n\nThe Earth-based mission control teams for Apollo 8 consisted of astronauts assigned to the support crew, as well as non-astronaut flight directors and their staffs. The support crew members were not trained to fly the mission, but were able to stand in for astronauts in meetings and be involved in the minutiae of mission planning, while the prime and backup crews trained. They also served as CAPCOMs during the mission. For Apollo 8, these crew members included astronauts John S. Bull, Vance D. Brand, Gerald P. Carr, and Ken Mattingly. The mission control teams on Earth rotated in three shifts, each led by a flight director. The directors for Apollo 8 included Clifford E. Charlesworth (Green team), Glynn Lunney (Black team), and Milton Windler (Maroon team).\n\nThe triangular shape of the insignia symbolizes the shape of the Apollo Command Module (CM). It shows a red figure-8 looping around the Earth and Moon representing the mission number as well as the circumlunar nature of the mission. On the red number 8 are the names of the three astronauts.\n\nThe initial design of the insignia was developed by Jim Lovell. Lovell reportedly sketched the initial design while riding in the backseat of a T-38 flight from California to Houston, shortly after learning of the re-designation of the flight to become a lunar-orbital mission. The graphic design of the insignia was done by Houston artist and animator William Bradley.\n\nApollo 4 and Apollo 6 had been \"A\" missions, unmanned tests of the Saturn V launch vehicle using an unmanned Block I production model of the Apollo Command and Service Module in Earth orbit. , scheduled for October 1968, would be a manned Earth-orbit flight of the CSM, completing the objectives for Mission \"C\".\nFurther missions depended on the readiness of the Lunar Module. Apollo 8 was planned as the \"D\" mission, to test the LM in a low Earth orbit in December 1968 by James McDivitt, David Scott and Russell Schweickart, while Borman's crew would fly the \"E\" mission, a more rigorous LM test in an elliptical medium Earth orbit as Apollo 9, in early 1969.\n\nBut production of the LM fell behind schedule, and when Apollo 8's LM arrived at the Kennedy Space Center in June 1968, significant defects were discovered, leading Grumman, the lead contractor for the LM, to predict that the first mission-ready LM would not be ready until at least February 1969. This would mean delaying the \"D\" and subsequent missions, endangering the program's goal of a lunar landing before the end of 1969.\n\nGeorge Low, the Manager of the Apollo Spacecraft Program Office, proposed a solution in August to keep the program on track despite the LM delay. Since the Command/Service Module (CSM) would be ready three months before the Lunar Module, a CSM-only mission could be flown in December 1968. Instead of just repeating the \"C\" mission flight of Apollo 7, this CSM could be sent all the way to the Moon, with the possibility of entering a lunar orbit. The new mission would also allow NASA to test lunar landing procedures that would otherwise have to wait until Apollo 10, the scheduled \"F\" mission. This also meant that the medium Earth orbit \"E\" mission could be dispensed with. The net result was that only the \"D\" mission had to be delayed.\nAlmost every senior manager at NASA agreed with this new mission, citing both confidence in the hardware and personnel, and the potential for a significant morale boost provided by a circumlunar flight. The only person who needed some convincing was James E. Webb, the NASA administrator. With the rest of his agency in support of the new mission, Webb eventually approved the mission change. The mission was officially changed from a \"D\" mission to a \"C-Prime\" lunar-orbit mission, but was still referred to in press releases as an Earth-orbit mission at Webb's direction. No public announcement was made about the change in mission until November 12, three weeks after Apollo 7's successful Earth-orbit mission and less than 40 days before launch.\n\nWith the change in mission for Apollo 8, Director of Flight Crew Operations Deke Slayton decided to swap the crews of the D and E missions. This swap also meant a swap of spacecraft, requiring Borman's crew to use CSM-103, while McDivitt's crew would use CSM-104.\n\nOn September 9, the crew entered the simulators to begin their preparation for the flight. By the time the mission flew, the crew had spent seven hours training for every actual hour of flight. Although all crew members were trained in all aspects of the mission, it was necessary to specialize. Borman, as commander, was given training on controlling the spacecraft during the re-entry. Lovell was trained on navigating the spacecraft in case communication was lost with the Earth. Anders was placed in charge of checking that the spacecraft was in working order.\n\nAdded pressure on the Apollo program to make its 1969 landing goal was provided by the Soviet Union's flight of some living creatures, including Russian tortoises, in a cislunar loop around the Moon on Zond 5 and return to Earth on September 21. There was speculation within NASA and the press that they might be preparing to launch cosmonauts on a similar circumlunar mission before the end of 1968.\n\nThe Apollo 8 crew, now living in the crew quarters at Kennedy Space Center, received a visit from Charles Lindbergh and his wife, Anne Morrow Lindbergh, the night before the launch. They talked about how, before his 1927 flight, Lindbergh had used a piece of string to measure the distance from New York City to Paris on a globe and from that calculated the fuel needed for the flight. The total was a tenth of the amount that the Saturn V would burn every second. The next day, the Lindberghs watched the launch of Apollo 8 from a nearby dune.\n\nThe Saturn V rocket used by Apollo 8 was designated SA-503, or the \"03rd\" model of the Saturn V (\"5\") Rocket to be used in the Saturn-Apollo (\"SA\") program. When it was erected in the Vertical Assembly Building on December 20, 1967, it was thought that the rocket would be used for an unmanned Earth-orbit test flight carrying a boilerplate Command/Service Module. Apollo 6 had suffered several major problems during its April 1968 flight, including severe pogo oscillation during its first stage, two second stage engine failures, and a third stage that failed to reignite in orbit. Without assurances that these problems had been rectified, NASA administrators could not justify risking a manned mission until additional unmanned test flights proved that the Saturn V was ready.\n\nTeams from the Marshall Space Flight Center (MSFC) went to work on the problems. Of primary concern was the pogo oscillation, which would not only hamper engine performance, but could exert significant g-forces on a crew. A task force of contractors, NASA agency representatives, and MSFC researchers concluded that the engines vibrated at a frequency similar to the frequency at which the spacecraft itself vibrated, causing a resonance effect that induced oscillations in the rocket. A system using helium gas to absorb some of these vibrations was installed.\n\nOf equal importance was the failure of three engines during flight. Researchers quickly determined that a leaking hydrogen fuel line ruptured when exposed to vacuum, causing a loss of fuel pressure in engine two. When an automatic shutoff attempted to close the liquid hydrogen valve and shut down engine two, it accidentally shut down engine three's liquid oxygen due to a miswired connection. As a result, engine three failed within one second of engine two's shutdown. Further investigation revealed the same problem for the third-stage engine—a faulty igniter line. The team modified the igniter lines and fuel conduits, hoping to avoid similar problems on future launches.\n\nThe teams tested their solutions in August 1968 at the Marshall Space Flight Center. A Saturn stage IC was equipped with shock absorbing devices to demonstrate the team's solution to the problem of pogo oscillation, while a Saturn Stage II was retrofitted with modified fuel lines to demonstrate their resistance to leaks and ruptures in vacuum conditions. Once NASA administrators were convinced that the problems were solved, they gave their approval for a manned mission using SA-503.\n\nThe Apollo 8 spacecraft was placed on top of the rocket on September 21 and the rocket made the slow 3-mile (5 km) journey to the launch pad on October 9. Testing continued all through December until the day before launch, including various levels of readiness testing from December 5 through 11. Final testing of modifications to address the problems of pogo oscillation, ruptured fuel lines, and bad igniter lines took place on December 18, a mere three days before the scheduled launch.\n\nAs the first manned spacecraft to orbit more than one celestial body, Apollo 8's profile had two different sets of orbital parameters, separated by a translunar injection maneuver.\n\nApollo lunar missions would begin with a nominal circular Earth parking orbit. Apollo 8 was launched into an initial orbit with an apogee of and a perigee of , with an inclination of 32.51° to the Equator, and an orbital period of 88.19 minutes. Propellant venting increased the apogee by over the 2 hours, 44 minutes and 30 seconds spent in the parking orbit.\n\nThis was followed by a Trans-Lunar Injection (TLI) burn of the S-IVB third stage for 318 seconds, accelerating the spacecraft from an orbital velocity of to the injection velocity of , which set a record for the highest speed, relative to Earth, that humans had ever traveled. This speed was slightly less than the Earth's escape velocity of , but put Apollo 8 into an elongated elliptical Earth orbit, to a point where the Moon's gravity would capture it.\n\nThe standard lunar orbit for Apollo missions was planned as a nominal circular orbit above the Moon's surface. Initial lunar orbit insertion was an ellipse with a perilune of and an apolune of , at an inclination of 12° from the lunar equator. This was then circularized at by , with an orbital period of 128.7 minutes. The effect of lunar mass concentrations (\"masscons\") on the orbit was found to be greater than initially predicted; over the course of the twenty-hour mission, the orbit was perturbated to by .\n\nApollo 8 achieved a maximum distance from Earth of .\n\nApollo 8 launched at 7:51:00 a.m. Eastern Standard Time on December 21, 1968, using the Saturn V's three stages to achieve Earth orbit. The S-IC first stage impacted the Atlantic Ocean at and the S-II second stage at . The S-IVB third stage injected the craft into Earth orbit, but remained attached to later perform the trans-lunar injection (TLI) burn that put the spacecraft on a trajectory to the Moon.\n\nThe Titan II launch vehicle used for the Gemini program had been notoriously rough-riding, and technicians promised the astronauts that the Saturn V, which was designed for the Apollo program rather than adapted from a missile, would have a much smoother ride. Lovell and Borman, both Gemini veterans, found this promise did not disappoint. During liftoff, they reported feeling nothing but a dull, muted rumble in the distance.\n\nOnce the vehicle reached Earth orbit, both the crew and Houston flight controllers spent the next 2 hours and 38 minutes checking that the spacecraft was in proper working order and ready for TLI. The proper operation of the S-IVB third stage of the rocket was crucial: in the last unmanned test, it had failed to re-ignite for TLI.\n\nDuring the flight, three fellow astronauts served on the ground as Capsule Communicators (usually referred to as \"CAPCOMs\") on a rotating schedule. The CAPCOMs were the only people who regularly communicated with the crew. Michael Collins was the first CAPCOM on duty and at 2 hours, 27 minutes and 22 seconds after launch radioed, \"Apollo 8. You are Go for TLI.\" This communication signified that Mission Control had given official permission for Apollo 8 to go to the Moon. Over the next 12 minutes before the TLI burn, the Apollo 8 crew continued to monitor the spacecraft and the S-IVB. The engine ignited on time and performed the TLI burn perfectly.\n\nAfter the S-IVB had performed its required tasks, it was jettisoned. The crew then rotated the spacecraft to take some photographs of the spent stage and then practiced flying in formation with it. As the crew rotated the spacecraft, they had their first views of the Earth as they moved away from it. This marked the first time humans could view the whole Earth at once. Borman became worried that the S-IVB was staying too close to the Command/Service Module and suggested to Mission Control that the crew perform a separation maneuver. Mission Control first suggested pointing the spacecraft towards Earth and using the Reaction Control System (RCS) thrusters on the Service Module (SM) to add away from the Earth, but Borman did not want to lose sight of the S-IVB. After discussion, the crew and Mission Control decided to burn in this direction, but at instead. These discussions put the crew an hour behind their flight plan.\n\nFive hours after launch, Mission Control sent a command to the S-IVB booster to vent its remaining fuel through its engine bell to change the booster's trajectory. This S-IVB would then pass the Moon and enter into a solar orbit, posing no further hazard to Apollo 8. The S-IVB subsequently went into a solar orbit with an inclination of 23.47° from the plane of the ecliptic, and an orbital period of 340.80 days. After the insertion into trans-Lunar orbit, the Saturn IVB third stage became a object. It will continue to orbit the Sun for many years.\n\nThe Apollo 8 crew were the first humans to pass through the Van Allen radiation belts, which extend up to from Earth. Scientists predicted that passing through the belts quickly at the spacecraft's high speed would cause a radiation dosage of no more than a chest X-ray, or 1 milligray (during a year, the average human receives a dose of 2 to 3 mGy). To record the actual radiation dosages, each crew member wore a Personal Radiation Dosimeter that transmitted data to Earth as well as three passive film dosimeters that showed the cumulative radiation experienced by the crew. By the end of the mission, the crew experienced an average radiation dose of 1.6 mGy.\n\nJim Lovell's main job as Command Module Pilot was as navigator. Although Mission Control performed all the actual navigation calculations, it was necessary to have a crew member serving as navigator so that the crew could return to Earth in case of loss of communication with Mission Control. Lovell navigated by star sightings using a sextant built into the spacecraft, measuring the angle between a star and the Earth's (or the Moon's) horizon. This task was difficult, because a large cloud of debris around the spacecraft, formed by the venting S-IVB, made it hard to distinguish the stars.\n\nBy seven hours into the mission, the crew was about one hour and 40 minutes behind flight plan, because of the problems in moving away from the S-IVB and Lovell's obscured star sightings. The crew now placed the spacecraft into Passive Thermal Control (PTC), also called \"barbecue roll\", in which the spacecraft rotated about once per hour around its long axis to ensure even heat distribution across the surface of the spacecraft. In direct sunlight, the spacecraft could be heated to over while the parts in shadow would be . These temperatures could cause the heat shield to crack and propellant lines to burst. Because it was impossible to get a perfect roll, the spacecraft swept out a cone as it rotated. The crew had to make minor adjustments every half hour as the cone pattern got larger and larger.\n\nThe first mid-course correction came 11 hours into the flight. Testing on the ground had shown that the Service Propulsion System (SPS) engine had a small chance of exploding when burned for long periods unless its combustion chamber was \"coated\" first. Burning the engine for a short period would accomplish coating. This first correction burn was only 2.4 seconds and added about velocity prograde (in the direction of travel). This change was less than the planned , because of a bubble of helium in the oxidizer lines, which caused unexpectedly low propellant pressure. The crew had to use the small RCS thrusters to make up the shortfall. Two later planned mid-course corrections were canceled because the Apollo 8 trajectory was found to be perfect.\n\nEleven hours into the flight, the crew had been awake for more than 16 hours. Before launch, NASA had decided that at least one crew member should be awake at all times to deal with problems that might arise. Borman started the first sleep shift, but found sleeping difficult because of the constant radio chatter and mechanical noises.\n\nAbout an hour after starting his sleep shift, Borman obtained permission from ground control to take a Seconal sleeping pill. The pill had little effect. Borman eventually fell asleep, and then awoke feeling ill. He vomited twice and had a bout of diarrhea; this left the spacecraft full of small globules of vomit and feces, which the crew cleaned up as well as they could. Borman initially did not want everyone to know about his medical problems, but Lovell and Anders wanted to inform Mission Control. The crew decided to use the Data Storage Equipment (DSE), which could tape voice recordings and telemetry and dump them to Mission Control at high speed. After recording a description of Borman's illness they asked Mission Control to check the recording, stating that they \"would like an evaluation of the voice comments\".\n\nThe Apollo 8 crew and Mission Control medical personnel held a conference using an unoccupied second-floor control room (there were two identical control rooms in Houston, on the second and third floors, only one of which was used during a mission). The conference participants concluded that there was little to worry about and that Borman's illness was either a 24-hour flu, as Borman thought, or a reaction to the sleeping pill. Researchers now believe that he was suffering from space-adaptation syndrome, which affects about a third of astronauts during their first day in space as their vestibular system adapts to weightlessness. Space-adaptation syndrome had not occurred on previous spacecraft (Mercury and Gemini), because those astronauts couldn't move freely in the small cabins of those spacecraft. The increased cabin space in the Apollo Command Module afforded astronauts greater freedom of movement, contributing to symptoms of space sickness for Borman and, later, astronaut Russell Schweickart during Apollo 9.\n\nThe cruise phase was a relatively uneventful part of the flight, except for the crew checking that the spacecraft was in working order and that they were on course. During this time, NASA scheduled a television broadcast at 31 hours after launch. The Apollo 8 crew used a 2 kg camera that broadcast in black-and-white only, using a Vidicon tube. The camera had two lenses, a very wide-angle (160°) lens, and a telephoto (9°) lens.\n\nDuring this first broadcast, the crew gave a tour of the spacecraft and attempted to show how the Earth appeared from space. However, difficulties aiming the narrow-angle lens without the aid of a monitor to show what it was looking at made showing the Earth impossible. Additionally, the Earth image became saturated by any bright source without proper filters. In the end, all the crew could show the people watching back on Earth was a bright blob. After broadcasting for 17 minutes, the rotation of the spacecraft took the high-gain antenna out of view of the receiving stations on Earth and they ended the transmission with Lovell wishing his mother a happy birthday.\n\nBy this time, the crew had completely abandoned the planned sleep shifts. Lovell went to sleep 32½ hours into the flight—3½ hours before he had planned to. A short while later, Anders also went to sleep after taking a sleeping pill.\n\nThe crew was unable to see the Moon for much of the outward cruise. Two factors made the Moon almost impossible to see from inside the spacecraft: three of the five windows fogging up due to out-gassed oils from the silicone sealant, and the attitude required for the PTC. It was not until the crew had gone behind the Moon that they would be able to see it for the first time.\n\nApollo 8 made a second television broadcast at 55 hours into the flight. This time, the crew rigged up filters meant for the still cameras so they could acquire images of the Earth through the telephoto lens. Although difficult to aim, as they had to maneuver the entire spacecraft, the crew was able to broadcast back to Earth the first television pictures of the Earth. The crew spent the transmission describing the Earth and what was visible and the colors they could see. The transmission lasted 23 minutes.\n\nAt about 55 hours and 40 minutes into the flight, the crew of Apollo 8 became the first humans to enter the gravitational sphere of influence of another celestial body. In other words, the effect of the Moon's gravitational force on Apollo 8 became stronger than that of the Earth. At the time it happened, Apollo 8 was from the Moon and had a speed of relative to the Moon. This historic moment was of little interest to the crew since they were still calculating their trajectory with respect to the launch pad at Kennedy Space Center. They would continue to do so until they performed their last mid-course correction, switching to a reference frame based on ideal orientation for the second engine burn they would make in lunar orbit. It was only 13 hours until they would be in lunar orbit.\n\nThe last major event before Lunar Orbit Insertion (LOI) was a second mid-course correction. It was in retrograde (against direction of travel) and slowed the spacecraft down by , effectively lowering the closest distance that the spacecraft would pass the moon. At exactly 61 hours after launch, about from the Moon, the crew burned the RCS for 11 seconds. They would now pass from the lunar surface.\n\nAt 64 hours into the flight, the crew began to prepare for Lunar Orbit Insertion-1 (LOI-1). This maneuver had to be performed perfectly, and due to orbital mechanics had to be on the far side of the Moon, out of contact with the Earth. After Mission Control was polled for a \"go/no go\" decision, the crew was told at 68 hours, they were Go and \"riding the best bird we can find\". Lovell replied, \"We'll see you on the other side\", and for the first time in history, humans travelled behind the Moon and out of radio contact with the Earth.\n\nWith 10 minutes before the LOI-1, the crew began one last check of the spacecraft systems and made sure that every switch was in the correct place. At that time, they finally got their first glimpses of the Moon. They had been flying over the unlit side, and it was Lovell who saw the first shafts of sunlight obliquely illuminating the lunar surface. The LOI burn was only two minutes away, so the crew had little time to appreciate the view.\n\nThe SPS ignited at 69 hours, 8 minutes, and 16 seconds after launch and burned for 4 minutes and 7 seconds, placing the Apollo 8 spacecraft in orbit around the Moon. The crew described the burn as being the longest four minutes of their lives. If the burn had not lasted exactly the correct amount of time, the spacecraft could have ended up in a highly elliptical lunar orbit or even flung off into space. If it lasted too long they could have struck the Moon. After making sure the spacecraft was working, they finally had a chance to look at the Moon, which they would orbit for the next 20 hours.\n\nOn Earth, Mission Control continued to wait. If the crew had not burned the engine or the burn had not lasted the planned length of time, the crew would appear early from behind the Moon. However, this time came and went without Apollo 8 reappearing. Exactly at the calculated moment, the signal was received from the spacecraft, indicating it was in a orbit about the Moon.\n\nAfter reporting on the status of the spacecraft, Lovell gave the first description of what the lunar surface looked like:\n\nLovell continued to describe the terrain they were passing over. One of the crew's major tasks was reconnaissance of planned future landing sites on the Moon, especially one in Mare Tranquillitatis that would be the Apollo 11 landing site. The launch time of Apollo 8 had been chosen to give the best lighting conditions for examining the site. A film camera had been set up in one of the spacecraft windows to record a frame every second of the Moon below. Bill Anders spent much of the next 20 hours taking as many photographs as possible of targets of interest. By the end of the mission the crew had taken 700 photographs of the Moon and 150 of the Earth.\n\nThroughout the hour that the spacecraft was in contact with Earth, Borman kept asking how the data for the SPS looked. He wanted to make sure that the engine was working and could be used to return early to the Earth if necessary. He also asked that they receive a \"go/no go\" decision before they passed behind the Moon on each orbit.\n\nAs they reappeared for their second pass in front of the Moon, the crew set up the equipment to broadcast a view of the lunar surface. Anders described the craters that they were passing over. At the end of this second orbit they performed the 11-second LOI-2 burn of the SPS to circularize the orbit to .\n\nThrough the next two orbits, the crew continued to keep check of the spacecraft and to observe and photograph the Moon. During the third pass, Borman read a small prayer for his church. He had been scheduled to participate in a service at St. Christopher's Episcopal Church near Seabrook, Texas, but due to the Apollo 8 flight he was unable to. A fellow parishioner and engineer at Mission Control, Rod Rose, suggested that Borman read the prayer which could be recorded and then replayed during the service.\n\nWhen the spacecraft came out from behind the Moon for its fourth pass across the front, the crew witnessed \"Earthrise\" for the first time in human history (NASA's Lunar Orbiter 1 took the very first picture of an Earthrise from the vicinity of the Moon, on August 23, 1966). Borman saw the Earth emerging from behind the lunar horizon, and then called in excitement to the others, taking a black-and-white photograph as he did so. In the ensuing scramble Anders took \"Earthrise\", a more famous color photo, later picked by \"Life\" magazine as one of its hundred photos of the century.\n\nDue to the synchronous rotation of the Moon about the Earth, Earthrise is not generally visible from the lunar surface. Earthrise is generally only visible when orbiting the Moon, other than at selected places near the Moon's limb, where libration carries the Earth slightly above and below the lunar horizon.\n\nAnders continued to take photographs while Lovell assumed control of the spacecraft so Borman could rest. Despite the difficulty resting in the cramped and noisy spacecraft, Borman was able to sleep for two orbits, awakening periodically to ask questions about their status. Borman awoke fully, however, when he started to hear his fellow crew members make mistakes. They were beginning to not understand questions and would have to ask for the answers to be repeated. Borman realized that everyone was extremely tired from not having a good night's sleep in over three days. He ordered Anders and Lovell to get some sleep and that the rest of the flight plan regarding observing the Moon be scrubbed. At first Anders protested saying that he was fine, but Borman would not be swayed. At last Anders agreed as long as Borman would set up the camera to continue to take automatic shots of the Moon. Borman also remembered that there was a second television broadcast planned, and with so many people expected to be watching he wanted the crew to be alert. For the next two orbits Anders and Lovell slept while Borman sat at the helm. On subsequent Apollo missions, crews would avoid this situation by sleeping on the same schedule.\nAs they rounded the Moon for the ninth time, the second television transmission began. Borman introduced the crew, followed by each man giving his impression of the lunar surface and what it was like to be orbiting the Moon. Borman described it as being \"a vast, lonely, forbidding expanse of nothing\". Then, after talking about what they were flying over, Anders said that the crew had a message for all those on Earth. Each man on board read a section from the Biblical creation story from the Book of Genesis. Borman finished the broadcast by wishing a Merry Christmas to everyone on Earth. His message appeared to sum up the feelings that all three crewmen had from their vantage point in lunar orbit. Borman said, \"And from the crew of Apollo 8, we close with good night, good luck, a Merry Christmas and God bless all of you—all of you on the good Earth.\"\n\nThe only task left for the crew at this point was to perform the Trans-Earth Injection (TEI), which was scheduled for 2½ hours after the end of the television transmission. The TEI was the most critical burn of the flight, as any failure of the SPS to ignite would strand the crew in lunar orbit, with little hope of escape. As with the previous burn, the crew had to perform the maneuver above the far side of the Moon, out of contact with Earth.\n\nThe burn occurred exactly on time. The spacecraft telemetry was reacquired as it re-emerged from behind the Moon at 89 hours, 28 minutes, and 39 seconds, the exact time calculated. When voice contact was regained, Lovell announced, \"Please be informed, there is a Santa Claus\", to which Ken Mattingly, the current CAPCOM, replied, \"That's affirmative, you are the best ones to know.\" The spacecraft began its journey back to Earth on December 25, Christmas Day.\n\nLater, Lovell used some otherwise idle time to do some navigational sightings, maneuvering the module to view various stars by using the computer keyboard. However, he accidentally erased some of the computer's memory, which caused the Inertial Measurement Unit (IMU) to think the module was in the same relative position it had been in before lift-off and fire the thrusters to \"correct\" the module's attitude.\n\nOnce the crew realized why the computer had changed the module's attitude, they realized they would have to re-enter data that would tell the computer its real position. It took Lovell ten minutes to figure out the right numbers, using the thrusters to get the stars Rigel and Sirius aligned, and another 15 minutes to enter the corrected data into the computer.\n\nSixteen months later, Lovell would once again have to perform a similar manual re-alignment, under more critical conditions, during the Apollo 13 mission, after that module's IMU had to be turned off to conserve energy. In his 1994 book, \"Lost Moon: The Perilous Voyage of Apollo 13\", Lovell wrote, \"My training [on Apollo 8] came in handy!\" In that book he dismissed the incident as a \"planned experiment\", requested by the ground crew. In subsequent interviews Lovell has acknowledged that the incident was an accident, caused by his mistake.\n\nThe cruise back to Earth was mostly a time for the crew to relax and monitor the spacecraft. As long as the trajectory specialists had calculated everything correctly, the spacecraft would re-enter two-and-half days after TEI and splashdown in the Pacific.\n\nOn Christmas afternoon, the crew made their fifth television broadcast. This time they gave a tour of the spacecraft, showing how an astronaut lived in space. When they finished broadcasting they found a small present from Deke Slayton in the food locker: a real turkey dinner with stuffing, in the same kind of pack that the troops in Vietnam received. Another Slayton surprise was a gift of three miniature bottles of brandy, that Borman ordered the crew to leave alone until after they landed. They remained unopened, even years after the flight. There were also small presents to the crew from their wives. The next day, at about 124 hours into the mission, the sixth and final TV transmission showed the mission's best video images of the earth, in a four-minute broadcast.\nAfter two uneventful days the crew prepared for re-entry. The computer would control the re-entry and all the crew had to do was put the spacecraft in the correct attitude, blunt end forward. If the computer broke down, Borman would take over.\n\nOnce the Command Module was separated from the Service Module, the astronauts were committed to re-entry. Six minutes before they hit the top of the atmosphere, the crew saw the Moon rising above the Earth's horizon, just as had been predicted by the trajectory specialists. As they hit the thin outer atmosphere they noticed it was becoming hazy outside as glowing plasma formed around the spacecraft. The spacecraft started slowing down and the deceleration peaked at 6 g (59 m/s). With the computer controlling the descent by changing the attitude of the spacecraft, Apollo 8 rose briefly like a skipping stone before descending to the ocean. At the drogue parachute stabilized the spacecraft and was followed at by the three main parachutes. The spacecraft splashdown position was officially reported as in the North Pacific Ocean south of Hawaii.\n\nWhen it hit the water, the parachutes dragged the spacecraft over and left it upside down, in what was termed Stable 2 position. About six minutes later the Command Module was righted into its normal apex-up splashdown orientation by the inflatable bag uprighting system. As they were buffeted by a swell, Borman was sick, waiting for the three flotation balloons to right the spacecraft. It was 43 minutes after splashdown before the first frogman from arrived, as the spacecraft had landed before sunrise. Forty-five minutes later, the crew was safe on the deck of the aircraft carrier.\n\nApollo 8 came at the end of 1968, a year that had seen much upheaval in the United States and most of the world. Even though the year saw political assassinations, political unrest in the streets of Europe and America, and the Prague Spring, \"Time\" magazine chose the crew of Apollo 8 as its Men of the Year for 1968, recognizing them as the people who most influenced events in the preceding year. They had been the first people ever to leave the gravitational influence of the Earth and orbit another celestial body. They had survived a mission that even the crew themselves had rated as only having a fifty-fifty chance of fully succeeding. The effect of Apollo 8 can be summed up by a telegram from a stranger, received by Borman after the mission, that simply stated, \"Thank you Apollo 8. You saved 1968.\"\n\nOne of the most famous aspects of the flight was the Earthrise picture that was taken as they came around for their fourth orbit of the Moon. This was the first time that humans had taken such a picture whilst actually behind the camera, and it has been credited with a role in inspiring the first Earth Day in 1970. It was selected as the first of \"Life\" magazine's \"100 Photographs That Changed the World\". Apollo 11 astronaut Michael Collins said, \"Eight's momentous historic significance was foremost\"; while many space historians, such as Robert K. Poole, see Apollo 8 as the most historically significant of all the Apollo missions.\n\nThe mission was the most widely covered by the media since the first American orbital flight, Mercury-Atlas 6 by John Glenn in 1962. There were 1200 journalists covering the mission, with the BBC coverage being broadcast in 54 countries in 15 different languages. The Soviet newspaper \"Pravda\" featured a quote from Boris Nikolaevich Petrov, Chairman of the Soviet Interkosmos program, who described the flight as an \"outstanding achievement of American space sciences and technology\". It is estimated that a quarter of the people alive at the time saw—either live or delayed—the Christmas Eve transmission during the ninth orbit of the Moon. The Apollo 8 broadcasts won an Emmy Award, the highest honor given by the Academy of Television Arts & Sciences.\n\nMadalyn Murray O'Hair, an atheist, later caused controversy by bringing a lawsuit against NASA over the reading from Genesis. O'Hair wished the courts to ban American astronauts—who were all government employees—from public prayer in space. Though the case was rejected by the Supreme Court of the United States for lack of jurisdiction, it caused NASA to be skittish about the issue of religion throughout the rest of the Apollo program. Buzz Aldrin, on Apollo 11, self-communicated Presbyterian Communion on the surface of the Moon after landing; he refrained from mentioning this publicly for several years, and only obliquely referred to it at the time.\n\nIn 1969, the United States Postal Service issued a postage stamp (Scott catalogue #1371) commemorating the Apollo 8 flight around the Moon. The stamp featured a detail of the famous photograph of the Earthrise over the Moon taken by Anders on Christmas Eve, and the words, \"In the beginning God ...\" Just 18 days after the crew's return to Earth, they were featured during the 1969 Super Bowl pre-game show reciting the Pledge of Allegiance prior to the national anthem being performed by Anita Bryant.\n\nIn January 1970, the spacecraft was delivered to Osaka, Japan, for display in the U.S. pavilion at Expo '70. It is now displayed at the Chicago Museum of Science and Industry, along with a collection of personal items from the flight donated by Lovell and the space suit worn by Frank Borman. Jim Lovell's Apollo 8 space suit is on public display in the Visitor Center at NASA's Glenn Research Center. Bill Anders's space suit is on display at the Science Museum in London, United Kingdom.\n\nApollo 8's historic mission has been shown and referred to in several forms, both documentary and fiction. The various television transmissions and 16 mm footage shot by the crew of Apollo 8 was compiled and released by NASA in the 1969 documentary, \"Debrief: Apollo 8\", which was hosted by Burgess Meredith. In addition, Spacecraft Films released, in 2003, a three-disc DVD set containing all of NASA's TV and 16 mm film footage related to the mission including all TV transmissions from space, training and launch footage, and motion pictures taken in flight. Portions of the Apollo 8 mission can be seen in the 1989 documentary \"For All Mankind\", which won the Grand Jury Prize Documentary at the Sundance Film Festival. The television series \"American Experience\" aired a documentary, \"Race to the Moon\", in 2005 during season 18. The Apollo 8 mission was well-covered in the 2007 British documentary \"In the Shadow of the Moon\".\n\nPortions of the Apollo 8 mission are dramatized in the 1998 miniseries \"From the Earth to the Moon\" episode \"1968\". The S-IVB stage of Apollo 8 was also portrayed as the location of an alien device in the 1970 \"UFO\" episode \"Conflict\".\n\nThe Apollo 8 mission was also featured in the Discovery channel's 6-part documentary series \"\" (Part 3, \"Landing the Eagle\"). All three astronauts were featured in this documentary, telling the story of their historic mission to the Moon in their own words.\n\n\n\n\n\n"}
{"id": "664", "url": "https://en.wikipedia.org/wiki?curid=664", "title": "Astronaut", "text": "Astronaut\n\nAn astronaut or cosmonaut () is a person trained by a human spaceflight program to command, pilot, or serve as a crew member of a spacecraft. Although generally reserved for professional space travelers, the terms are sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists.\n\nStarting in the 1950s up to 2002, astronauts were sponsored and trained exclusively by governments, either by the military or by civilian space agencies. With the suborbital flight of the privately funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.\n\nThe criteria for what constitutes human spaceflight vary. The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of. In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings.\n\n, a total of 552 people from 36 countries have reached or more in altitude, of which 549 reached low Earth orbit or beyond.\nOf these, 24 people have traveled beyond low Earth orbit, to either lunar orbit, the lunar surface, or in one case a loop around the Moon; three of the 24 did so twice: Jim Lovell, John Young and Eugene Cernan. The three astronauts who have not reached low Earth orbit are spaceplane pilots Joe Walker, Mike Melvill, and Brian Binnie.\n\n, under the U.S. definition 558 people qualify as having reached space, above altitude. Of eight X-15 pilots who exceeded in altitude, only one exceeded 100 kilometers (about 62 miles).\nSpace travelers have spent over 41,790 man-days (114.5 man-years) in space, including over 100 astronaut-days of spacewalks. As of 2016, the man with the longest cumulative time in space is Gennady Padalka, who has spent 879 days in space. Peggy A. Whitson holds the record for the most time in space by a woman, 377 days.\n\nIn 1959, when both the United States and Soviet Union were planning, but had yet to launch humans into space, NASA Administrator T. Keith Glennan and his Deputy Administrator, Dr. Hugh Dryden, discussed whether spacecraft crew members should be called \"astronauts\" or \"cosmonauts\". Dryden preferred \"cosmonaut\", on the grounds that flights would occur in the \"cosmos\" (near space), while the \"astro\" prefix suggested flight to the stars. Most NASA Space Task Group members preferred \"astronaut\", which survived by common usage as the preferred American term. When the Soviet Union launched the first man into space, Yuri Gagarin in 1961, they chose a term which anglicizes to \"cosmonaut\".\n\nIn English-speaking nations, a professional space traveler is called an \"astronaut\". The term derives from the Greek words \"ástron\" (ἄστρον), meaning \"star\", and \"nautes\" (ναύτης), meaning \"sailor\". The first known use of the term \"astronaut\" in the modern sense was by Neil R. Jones in his short story \"The Death's Head Meteor\" in 1930. The word itself had been known earlier. For example, in Percy Greg's 1880 book \"Across the Zodiac\", \"astronaut\" referred to a spacecraft. In \"Les Navigateurs de l'Infini\" (1925) of J.-H. Rosny aîné, the word \"astronautique\" (astronautic) was used. The word may have been inspired by \"aeronaut\", an older term for an air traveler first applied (in 1784) to balloonists. An early use in a non-fiction publication is Eric Frank Russell's poem \"The Astronaut\" in the November 1934 \"Bulletin of the British Interplanetary Society\".\n\nThe first known formal use of the term astronautics in the scientific community was the establishment of the annual International Astronautical Congress in 1950 and the subsequent founding of the International Astronautical Federation the following year.\n\nNASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond. NASA also uses the term as a title for those selected to join its Astronaut Corps. The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.\n\nBy convention, an astronaut employed by the Russian Federal Space Agency (or its Soviet predecessor) is called a \"cosmonaut\" in English texts. The word is an anglicisation of the Russian word \"kosmonavt\" ( ), one who works in space outside the Earth's atmosphere, a space traveler, which derives from the Greek words \"kosmos\" (κόσμος), meaning \"universe\", and \"nautes\" (ναύτης), meaning \"sailor\". Other countries of the former Eastern Bloc use variations of the Russian word \"kosmonavt\", such as the Polish \"kosmonauta\".\n\nCoinage of the term \"kosmonavt\" has been credited to Soviet aeronautics pioneer Mikhail Tikhonravov (1900–1974). The first cosmonaut was Soviet Air Force pilot Yuri Gagarin, also the first person in space. Valentina Tereshkova, a Russian factory worker, was the first woman in space, as well as the first civilian among the Soviet cosmonaut or NASA astronaut corps to make a spaceflight. On March 14, 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, and thus became the first \"American cosmonaut\".\n\nOfficial English-language texts issued by the government of China use \"astronaut\" while texts in Russian use космонавт (\"cosmonaut\"). In official Chinese-language texts, \"yǔ háng yuán\" (, \"space navigating personnel\") is used for astronauts and cosmonauts, and \"háng tiān yuán\" (, \"space navigating personnel\") is used for Chinese astronauts. The phrase \"tài kōng rén\" (, \"spaceman\") is often used in Hong Kong and Taiwan.\n\nThe term \"taikonaut\" is used by some English-language news media organizations for professional space travelers from China. The word has featured in the Longman and Oxford English dictionaries, the latter of which describes it as \"a hybrid of the Chinese term \"taikong\" (space) and the Greek \"naut\" (sailor)\"; the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the \"Shenzhou 5\" spacecraft. This is the term used by Xinhua News Agency in the English version of the Chinese \"People's Daily\" since the advent of the Chinese space program. The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia, used it in newsgroups.\n\nWith the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term \"spaceflight participant\" to distinguish those space travelers from professional astronauts on missions coordinated by those two agencies.\n\nWhile no nation other than the Russian Federation (and previously the former Soviet Union), the United States, and China have launched a manned spacecraft, several other nations have sent people into space in cooperation with one of these countries. Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage. For example, the term \"spationaut\" (French spelling: \"spationaute\") is sometimes used to describe French space travelers, from the Latin word \"spatium\" for \"space\", the Malay term \"angkasawan\" was used to describe participants in the Angkasawan program, and the Indian Space Research Organisation hope to launch a spacecraft in 2018 that would carry \"vyomanauts\", coined from the Sanskrit word for space.\n\nThe first human in space was Soviet Yuri Gagarin, who was launched on April 12, 1961, aboard Vostok 1 and orbited around the Earth for 108 minutes. The first woman in space was Soviet Valentina Tereshkova, who launched on June 16, 1963, aboard Vostok 6 and orbited Earth for almost three days.\n\nAlan Shepard became the first American and second person in space on May 5, 1961, on a 15-minute sub-orbital flight. The first American to orbit the Earth was John Glenn, aboard Friendship 7 on February 20, 1962. The first American woman in space was Sally Ride, during Space Shuttle Challenger's mission STS-7, on June 18, 1983. In 1992 Mae Jemison became the first African American woman to travel in space aboard STS-47.\n\nCosmonaut Alexei Leonov was the first person to conduct an extravehicular activity (EVA), (commonly called a \"spacewalk\"), on March 18, 1965, on the Soviet Union's Voskhod 2 mission. This was followed two and a half months later by astronaut Ed White who made the first American EVA on NASA's Gemini 4 mission.\n\nThe first manned mission to orbit the Moon, \"Apollo 8\", included American William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968.\n\nThe Soviet Union, through its Intercosmos program, allowed people from other \"socialist\" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions, with the notable exception of France participating in Soyuz TM-7. An example is Czechoslovak Vladimír Remek, the first cosmonaut from a country other than the Soviet Union or the United States, who flew to space in 1978 on a Soyuz-U rocket.\n\nOn July 23, 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37. Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, and in 1983, Guion Bluford became the first African American to fly into space. In April 1985, Taylor Wang became the first ethnic Chinese person in space. The first person born in Africa to fly in space was Patrick Baudry (France), in 1985. In 1985, Saudi Arabian Prince Sultan Bin Salman Bin AbdulAziz Al-Saud became the first Arab Muslim astronaut in space. In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the Mir space station.\n\nWith the larger number of seats available on the Space Shuttle, the U.S. began taking international astronauts. In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft. In 1984, Marc Garneau became the first of 8 Canadian astronauts to fly in space (through 2010).\nIn 1985, Rodolfo Neri Vela became the first Mexican-born person in space. In 1991, Helen Sharman became the first Briton to fly in space.\nIn 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant. In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.\n\nOn October 15, 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft.\n\nThe youngest person to fly in space is Gherman Titov, who was 25 years old when he flew Vostok 2. (Titov was also the first person to suffer space sickness).\nThe oldest person who has flown in space is John Glenn, who was 77 when he flew on STS-95.\n\nThe longest stay in space thus far has been 438 days, by Russian Valeri Polyakov.\nAs of 2006, the most spaceflights by an individual astronaut is seven, a record held by both Jerry L. Ross and Franklin Chang-Diaz. The farthest distance from Earth an astronaut has traveled was , when Jim Lovell, Jack Swigert, and Fred Haise went around the Moon during the Apollo 13 emergency.\n\nThe first civilian in space was Valentina Tereshkova aboard Vostok 6 (she also became the first woman in space on that mission).\nTereshkova was only honorarily inducted into the USSR's Air Force, which had no female pilots whatsoever at that time. A month later, Joseph Albert Walker became the first American civilian in space when his X-15 Flight 90 crossed the line, qualifying him by the international definition of spaceflight. Walker had joined the US Army Air Force but was not a member during his flight. \nThe first people in space who had never been a member of any country's armed forces were both Konstantin Feoktistov and Boris Yegorov aboard Voskhod 1.\n\nThe first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983. In December 1990, Toyohiro Akiyama became the first paying space traveler as a reporter for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was \"Research Cosmonaut\". Akiyama suffered severe space sickness during his mission, which affected his productivity.\n\nThe first self-funded space tourist was Dennis Tito on board the Russian spacecraft Soyuz TM-3 on April 28, 2001.\n\nThe first person to fly on an entirely privately funded mission was Mike Melvill, piloting SpaceShipOne flight 15P on a suborbital journey, although he was a test pilot employed by Scaled Composites and not an actual paying space tourist. Seven others have paid the Russian Space Agency to fly into space:\n\n\nThe first NASA astronauts were selected for training in 1959. Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection. Selection was initially limited to military pilots. The earliest astronauts for both America and the USSR tended to be jet fighter pilots, and were often test pilots.\n\nOnce selected, NASA astronauts go through twenty months of training in a variety of areas, including training for extravehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory. Astronauts-in-training may also experience short periods of weightlessness in aircraft called the \"vomit comet\", the nickname given to a pair of modified KC-135s (retired in 2000 and 2004 respectively, and replaced in 2005 with a C-9) which perform parabolic flights. Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft. This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center. Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are done out of Edwards Air Force Base.\n\n\n\n\nMission Specialist Educators, or \"Educator Astronauts\", were first selected in 2004, and as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger.\nBarbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist.\nThe Educator Astronaut program is a successor to the Teacher in Space program from the 1980s.\n\nAstronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, loss of eyesight, orthostatic intolerance, sleep disturbances, and radiation injury. A variety of large scale medical studies are being conducted in space via the National Space and Biomedical Research Institute (NSBRI) to address these issues. Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space. This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students. It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare.\n\nOn December 31, 2012, a NASA-supported study reported that manned spaceflight may harm the brain and accelerate the onset of Alzheimer's disease.\n\nIn October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.\n\nOver the last decade, flight surgeons and scientists at NASA have seen a pattern of vision problems in astronauts on long-duration space missions. The syndrome, known as visual impairment intracranial pressure (VIIP), has been reported in nearly two-thirds of space explorers after long periods spent aboard the International Space Station (ISS).\n\nAn astronaut on the International Space Station requires about 0.83 kilograms (1.83 pounds) weight of food inclusive of food packaging per meal each day. (The packaging for each meal weighs around 0.12 kilograms - 0.27 pounds) Longer-duration missions require more food.\n\nShuttle astronauts worked with nutritionists to select menus that appeal to their individual tastes. Five months before flight, menus are selected and analyzed for nutritional content by the shuttle dietician. Foods are tested to see how they will react in a reduced gravity environment. Caloric requirements are determined using a basal energy expenditure (BEE) formula.\nOn Earth, the average American uses about 35 gallons (132 liters) of water every day. On board the ISS astronauts limit water use to only about three gallons (11 liters) per day.\n\nIn Russia, cosmonauts are awarded Pilot-Cosmonaut of the Russian Federation upon completion of their missions, often accompanied with the award of Hero of the Russian Federation. This follows the practice established in the USSR where cosmonauts were usually awarded the title Hero of the Soviet Union.\n\nAt NASA, those who complete astronaut candidate training receive a silver lapel pin. Once they have flown in space, they receive a gold pin. U.S. astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight. The United States Air Force also presents an Astronaut Badge to its pilots who exceed in altitude.\n\nEighteen astronauts (fourteen men and four women) have lost their lives during four space flights. By nationality, thirteen were American (including one born in India), four were Russian (Soviet Union), and one was Israeli.\n\nEleven people (all men) have lost their lives training for spaceflight: eight Americans and three Russians. Six of these were in crashes of training jet aircraft, one drowned during water recovery training, and four were due to fires in pure oxygen environments.\n\nThe Space Mirror Memorial, which stands on the grounds of the John F. Kennedy Space Center Visitor Complex, commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States. In addition to twenty NASA career astronauts, the memorial includes the names of a U.S. Air Force X-15 test pilot, a U.S. Air Force officer who died while training for a then-classified military space program, and a civilian spaceflight participant.\n\n"}
{"id": "665", "url": "https://en.wikipedia.org/wiki?curid=665", "title": "A Modest Proposal", "text": "A Modest Proposal\n\nA Modest Proposal For preventing the Children of Poor People From being a Burthen to Their Parents or Country, and For making them Beneficial to the Publick, commonly referred to as A Modest Proposal, is a Juvenalian satirical essay written and published anonymously by Jonathan Swift in 1729. Swift suggests that the impoverished Irish might ease their economic troubles by selling their children as food for rich gentlemen and ladies. This satirical hyperbole mocked heartless attitudes towards the poor, as well as British policy toward the Irish in general.\n\nIn English writing, the phrase \"a modest proposal\" is now conventionally an allusion to this style of straight-faced satire.\n\nSwift goes to great lengths to support his argument, including a list of possible preparation styles for the children, and calculations showing the financial benefits of his suggestion. He uses methods of argument throughout his essay which lampoon the then-influential William Petty and the social engineering popular among followers of Francis Bacon. These lampoons include appealing to the authority of \"a very knowing American of my acquaintance in London\" and \"the famous Psalmanazar, a native of the island Formosa\" (who had already confessed to \"not\" being from Formosa in 1706). This essay is widely held to be one of the greatest examples of sustained irony in the history of the English language. Much of its shock value derives from the fact that the first portion of the essay describes the plight of starving beggars in Ireland, so that the reader is unprepared for the surprise of Swift's solution when he states, \"A young healthy child well nursed, is, at a year old, a most delicious nourishing and wholesome food, whether stewed, roasted, baked, or boiled; and I make no doubt that it will equally serve in a fricassee, or a ragout.\"\nIn the tradition of Roman satire, Swift introduces the reforms he is actually suggesting by paralipsis:\nGeorge Wittkowsky argued that Swift’s main target in \"A Modest Proposal\" was not the conditions in Ireland, but rather the can-do spirit of the times that led people to devise a number of illogical schemes that would purportedly solve social and economic ills. Swift was especially insulted by projects that tried to fix population and labour issues with a simple cure-all solution. A memorable example of these sorts of schemes \"involved the idea of running the poor through a joint-stock company\". In response, Swift's \"Modest Proposal\" was \"a burlesque of projects concerning the poor\" that were in vogue during the early 18th century.\n\n\"A Modest Proposal\" also targets the calculating way people perceived the poor in designing their projects. The pamphlet targets reformers who \"regard people as commodities\". In the piece, Swift adopts the \"technique of a political arithmetician\" to show the utter ridiculousness of trying to prove any proposal with dispassionate statistics.\n\nCritics differ about Swift's intentions in using this faux-mathematical philosophy. Edmund Wilson argues that statistically \"the logic of the 'Modest proposal' can be compared with defense of crime (arrogated to Marx) in which he argues that crime takes care of the superfluous population\". Wittkowsky counters that Swift's satiric use of statistical analysis is an effort to enhance his satire that \"springs from a spirit of bitter mockery, not from the delight in calculations for their own sake\".\n\nCharles K. Smith argues that Swift's rhetorical style persuades the reader to detest the speaker and pity the Irish. Swift's specific strategy is twofold, using a \"trap\" to create sympathy for the Irish and a dislike of the narrator who, in the span of one sentence, \"details vividly and with rhetorical emphasis the grinding poverty\" but feels emotion solely for members of his own class. Swift's use of gripping details of poverty and his narrator's cool approach towards them create \"two opposing points of view\" that \"alienate the reader, perhaps unconsciously, from a narrator who can view with 'melancholy' detachment a subject that Swift has directed us, rhetorically, to see in a much less detached way.\"\n\nSwift has his proposer further degrade the Irish by using language ordinarily reserved for animals. Lewis argues that the speaker uses \"the vocabulary of animal husbandry\" to describe the Irish. Once the children have been commodified, Swift's rhetoric can easily turn \"people into animals, then meat, and from meat, logically, into tonnage worth a price per pound\".\n\nSwift uses the proposer's serious tone to highlight the absurdity of his proposal. In making his argument, the speaker uses the conventional, textbook-approved order of argument from Swift's time (which was derived from the Latin rhetorician Quintilian). The contrast between the \"careful control against the almost inconceivable perversion of his scheme\" and \"the ridiculousness of the proposal\" create a situation in which the reader has \"to consider just what perverted values and assumptions would allow such a diligent, thoughtful, and conventional man to propose so perverse a plan\".\n\nScholars have speculated about which earlier works Swift may have had in mind when he wrote \"A Modest Proposal\".\n\nJames Johnson argued that \"A Modest Proposal\" was largely influenced and inspired by Tertullian's \"Apology\": a satirical attack against early Roman persecution of Christianity. James William Johnson believes that Swift saw major similarities between the two situations. Johnson notes Swift's obvious affinity for Tertullian and the bold stylistic and structural similarities between the works \"A Modest Proposal\" and \"Apology\". In structure, Johnson points out the same central theme, that of cannibalism and the eating of babies as well as the same final argument, that \"human depravity is such that men will attempt to justify their own cruelty by accusing their victims of being lower than human.\" Stylistically, Swift and Tertullian share the same command of sarcasm and language. In agreement with Johnson, Donald C. Baker points out the similarity between both authors' tones and use of irony. Baker notes the uncanny way that both authors imply an ironic \"justification by ownership\" over the subject of sacrificing children—Tertullian while attacking pagan parents, and Swift while attacking the English mistreatment of the Irish poor.\n\nIt has also been argued that \"A Modest Proposal\" was, at least in part, a response to the 1728 essay \"The Generous Projector or, A Friendly Proposal to Prevent Murder and Other Enormous Abuses, By Erecting an Hospital for Foundlings and Bastard Children\" by Swift's rival Daniel Defoe.\n\nBernard Mandeville's \"Modest Defence of Publick Stews\" asked to introduce public and state controlled bordellos. The 1726 paper acknowledges women's interests andwhile not being a complete satirical texthas been discussed as well as an inspiration for Jonathan Swifts title. Mandeville had become famous with the Fable of The Bees and deliberations on private vices and public benefits in 1705 already.\n\nJohn Locke from the First Treatise of Government\n\n\"Be it then as Sir Robert says, that Anciently, it was usual for Men to sell and Castrate their Children. Let it be, that they exposed them; Add to it, if you please, for this is still greater Power, \"that they begat them for their Tables to fat and eat them\": If this proves a right to do so, we may, by the same Argument, justifie Adultery, Incest and Sodomy, for there are examples of these too, both Ancient and Modern; Sins, which I suppose, have the Principle Aggravation from this, that they cross the main intention of Nature, which willeth the increase of Mankind, and the continuation of the Species in the highest perfection, and the distinction of Families, with the Security of the Marriage Bed, as necessary thereunto\" (First Treatise, sec. 59).\n\nRobert Phiddian's article \"Have you eaten yet? The Reader in A Modest Proposal\" focuses on two aspects of \"A Modest Proposal\": the voice of Swift and the voice of the Proposer. Phiddian stresses that a reader of the pamphlet must learn to distinguish between the satiric voice of Jonathan Swift and the apparent economic projections of the Proposer. He reminds readers that \"there is a gap between the narrator's meaning and the text's, and that a moral-political argument is being carried out by means of parody\".\n\nWhile Swift's proposal is obviously not a serious economic proposal, George Wittkowsky, author of \"Swift's Modest Proposal: The Biography of an Early Georgian Pamphlet\", argues that to understand the piece fully, it is important to understand the economics of Swift’s time. Wittowsky argues that not enough critics have taken the time to focus directly on the mercantilism and theories of labour in 18th century England. \"[I]f one regards the \"Modest Proposal\" simply as a criticism of condition, about all one can say is that conditions were bad and that Swift's irony brilliantly underscored this fact\".\n\nAt the start of a new industrial age in the 18th century, it was believed that \"people are the riches of the nation\", and there was a general faith in an economy that paid its workers low wages because high wages meant workers would work less. Furthermore, \"in the mercantilist view no child was too young to go into industry\". In those times, the \"somewhat more humane attitudes of an earlier day had all but disappeared and the laborer had come to be regarded as a commodity\".\n\nLouis A. Landa presents Swift's \"A Modest Proposal\" as a critique of the popular and unjustified maxim of mercantilism in the 18th century that \"people are the riches of a nation\". Swift presents the dire state of Ireland and shows that mere population itself, in Ireland's case, did not always mean greater wealth and economy. The uncontrolled maxim fails to take into account that a person who does not produce in an economic or political way makes a country poorer, not richer. Swift also recognises the implications of such a fact in making mercantilist philosophy a paradox: the wealth of a country is based on the poverty of the majority of its citizens. Swift however, Landa argues, is not merely criticising economic maxims but also addressing the fact that England was denying Irish citizens their natural rights and dehumanising them by viewing them as a mere commodity.\n\n\"A Modest Proposal\" is included in many literature programs as an example of early modern western satire. It also serves as an exceptional introduction to the concept and use of argumentative language, lending itself well to secondary and post-secondary essay courses. Outside of the realm of English studies, \"A Modest Proposal\" is a relevant piece included in many comparative and global literature and history courses, as well as those of numerous other disciplines in the arts, humanities, and even the social sciences.\n\nThe essay has been emulated many times. In his book \"A Modest Proposal\" (1984), evangelical author Frank Schaeffer emulated Swift's work in social conservative polemic against abortion and euthanasia in a future dystopia that advocated recycling of aborted embryos and fetuses, as well as some disabled infants with compound intellectual, physical and physiological difficulties. (Such Baby Doe Rules cases were then a major concern of the pro-life movement of the early 1980s, which viewed selective treatment of those infants as disability discrimination.) In his book \"A Modest Proposal for America\" (2013), statistician Howard Friedman opens with a satirical reflection of the extreme drive to fiscal stability by ultra-conservatives.\n\nA Modest Video Game Proposal is the title of an open letter sent by activist/former attorney Jack Thompson on October 10, 2005. He proposed that, if someone could \"create, manufacture, distribute, and sell a video game in 2006\" that allows players to play the scenario he has written, in which the character kills video game developers.\n\nHunter S. Thompson's \"Fear and Loathing in America: The Brutal Odyssey of an Outlaw Journalist\", which contains hundreds of private letters written by Thompson over the years, contains a letter in which he uses \"A Modest Proposal\"'s satire technique against the Vietnam War. Thompson writes a letter to a local Aspen newspaper informing them that, on Christmas Eve, he was going to use napalm to burn a number of dogs and hopefully any humans they find. This letter protests against the burning of Vietnamese people occurring overseas.\n\nThe 2012 film \"Butcher Boys,\" written by \"The Texas Chain Saw Massacre\" co-scribe Kim Henkel, is said to be loosely based on Jonathan Swift's \"A Modest Proposal.\" The film's opening scene takes place in a restaurant named \"J. Swift's.\"\n\n\n"}
{"id": "666", "url": "https://en.wikipedia.org/wiki?curid=666", "title": "Alkali metal", "text": "Alkali metal\n\nThe alkali metals are a group (column) in the periodic table consisting of the chemical elements lithium (Li), sodium (Na), potassium (K), rubidium (Rb), caesium (Cs), and francium (Fr). This group lies in the s-block of the periodic table of elements as all alkali metals have their outermost electron in an s-orbital: this shared electron configuration results in them having very similar characteristic properties. Indeed, the alkali metals provide the best example of group trends in properties in the periodic table, with elements exhibiting well-characterised homologous behaviour.\n\nThe alkali metals are all shiny, soft, highly reactive metals at standard temperature and pressure and readily lose their outermost electron to form cations with charge +1. They can all be cut easily with a knife due to their softness, exposing a shiny surface that tarnishes rapidly in air due to oxidation by atmospheric moisture and oxygen (and in the case of lithium, nitrogen). Because of their high reactivity, they must be stored under oil to prevent reaction with air, and are found naturally only in salts and never as the free elements. Caesium, the fifth alkali metal, is the most reactive of all the metals. In the modern IUPAC nomenclature, the alkali metals comprise the group 1 elements, excluding hydrogen (H), which is nominally a group 1 element but not normally considered to be an alkali metal as it rarely exhibits behaviour comparable to that of the alkali metals. All the alkali metals react with water, with the heavier alkali metals reacting more vigorously than the lighter ones.\n\nAll of the discovered alkali metals occur in nature as their compounds: in order of abundance, sodium is the most abundant, followed by potassium, lithium, rubidium, caesium, and finally francium, which is very rare due to its extremely high radioactivity; francium occurs only in the minutest traces in nature as an intermediate step in some obscure side branches of the natural decay chains. Experiments have been conducted to attempt the synthesis of ununennium (Uue), which is likely to be the next member of the group, but they have all met with failure. However, ununennium may not be an alkali metal due to relativistic effects, which are predicted to have a large influence on the chemical properties of superheavy elements; even if it does turn out to be an alkali metal, it is predicted to have some differences in physical and chemical properties from its lighter homologues.\n\nMost alkali metals have many different applications. One of the best-known applications of the pure elements is the use of rubidium and caesium in atomic clocks, of which caesium atomic clocks are the most accurate and precise representation of time. A common application of the compounds of sodium is the sodium-vapour lamp, which emits very efficient light. Table salt, or sodium chloride, has been used since antiquity. Sodium and potassium are also essential elements, having major biological roles as electrolytes, and although the other alkali metals are not essential, they also have various effects on the body, both beneficial and harmful.\n\nThe physical and chemical properties of the alkali metals can be readily explained by their having an ns valence electron configuration, which results in weak metallic bonding. Hence, all the alkali metals are soft and have low densities, melting and boiling points, as well as heats of sublimation, vaporisation, and dissociation. They all crystallise in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited. The ns configuration also results in the alkali metals having very large atomic and ionic radii, as well as very high thermal and electrical conductivity. Their chemistry is dominated by the loss of their lone valence electron in the outermost s-orbital to form the +1 oxidation state, due to the ease of ionising this electron and the very high second ionisation energy. Most of the chemistry has been observed only for the first five members of the group. The chemistry of francium is not well established due to its extreme radioactivity; thus, the presentation of its properties here is limited. What little is known about francium shows that it is very close in behaviour to caesium, as expected. The physical properties of francium are even sketchier because the bulk element has never been observed; hence any data that may be found in the literature are certainly speculative extrapolations.\n\nThe alkali metals are more similar to each other than the elements in any other group are to each other. Indeed, the similarity is so great that it is quite difficult to separate potassium, rubidium, and caesium, due to their similar ionic radii; lithium and sodium are more distinct. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation. In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium. One of the very few properties of the alkali metals that does not display a very smooth trend is their reduction potentials: lithium's value is anomalous, being more negative than the others. This is because the Li ion has a very high hydration energy in the gas phase: though the lithium ion disrupts the structure of water significantly, causing a higher change in entropy, this high hydration energy is enough to make the reduction potentials indicate it as being the most electropositive alkali metal, despite the difficulty of ionising it in the gas phase.\n\nThe stable alkali metals are all silver-coloured metals except for caesium, which has a pale golden tint: it is one of only three metals that are clearly coloured (the other two being copper and gold). Additionally, the heavy alkaline earth metals calcium, strontium, and barium, as well as the divalent lanthanides europium and ytterbium, are pale yellow, though the colour is much less prominent than it is for caesium. Their lustre tarnishes rapidly in air due to oxidation. They all crystallise in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited. Indeed, these flame test colours are the most common way of identifying them since all their salts with common ions are soluble.\n\nAll the alkali metals are highly reactive and are never found in elemental forms in nature. Because of this, they are usually stored in mineral oil or kerosene (paraffin oil). They react aggressively with the halogens to form the alkali metal halides, which are white ionic crystalline compounds that are all soluble in water except lithium fluoride (LiF). The alkali metals also react with water to form strongly alkaline hydroxides and thus should be handled with great care. The heavier alkali metals react more vigorously than the lighter ones; for example, when dropped into water, caesium produces a larger explosion than potassium if the same number of moles of each metal is used. The alkali metals have the lowest first ionisation energies in their respective periods of the periodic table because of their low effective nuclear charge and the ability to attain a noble gas configuration by losing just one electron. Not only do the alkali metals react with water, but also with proton donors like alcohols and phenols, gaseous ammonia, and alkynes, the last demonstrating the phenomenal degree of their reactivity. Their great power as reducing agents makes them very useful in liberating other metals from their oxides or halides.\n\nThe second ionisation energy of all of the alkali metals is very high as it is in a full shell that is also closer to the nucleus; thus, they almost always lose a single electron, forming cations. The alkalides are an exception: they are unstable compounds which contain alkali metals in a −1 oxidation state, which is very unusual as before the discovery of the alkalides, the alkali metals were not expected to be able to form anions and were thought to be able to appear in salts only as cations. The alkalide anions have filled s-subshells, which gives them enough stability to exist. All the stable alkali metals except lithium are known to be able to form alkalides, and the alkalides have much theoretical interest due to their unusual stoichiometry and low ionisation potentials. Alkalides are chemically similar to the electrides, which are salts with trapped electrons acting as anions. A particularly striking example of an alkalide is \"inverse sodium hydride\", HNa (both ions being complexed), as opposed to the usual sodium hydride, NaH: it is unstable in isolation, due to its high energy resulting from the displacement of two electrons from hydrogen to sodium, although several derivatives are predicted to be metastable or stable.\n\nIn aqueous solution, the alkali metal ions form aqua ions of the formula [M(HO)], where \"n\" is the solvation number. Their coordination numbers and shapes agree well with those expected from their ionic radii. In aqueous solution the water molecules directly attached to the metal ion are said to belong to the first coordination sphere, also known as the first, or primary, solvation shell. The bond between a water molecule and the metal ion is a dative covalent bond, with the oxygen atom donating both electrons to the bond. Each coordinated water molecule may be attached by hydrogen bonds to other water molecules. The latter are said to reside in the second coordination sphere. However, for the alkali metal cations, the second coordination sphere is not well-defined as the +1 charge on the cation is not high enough to polarise the water molecules in the primary solvation shell enough for them to form strong hydrogen bonds with those in the second coordination sphere, producing a more stable entity. The solvation number for Li has been experimentally determined to be 4, forming the tetrahedral [Li(HO)]: while solvation numbers of 3 to 6 have been found for lithium aqua ions, solvation numbers less than 4 may be the result of the formation of contact ion pairs, and the higher solvation numbers may be interpreted in terms of water molecules that approach [Li(HO)] through a face of the tetrahedron, though molecular dynamic simulations may indicate the existence of an octahedral hexaaqua ion. There are also probably six water molecules in the primary solvation sphere of the sodium ion, forming the octahedral [Na(HO)] ion. While it was previously thought that the heavier alkali metals also formed octahedral hexaaqua ions, it has since been found that potassium and rubidium probably form the [K(HO)] and [Rb(HO)] ions, which have the square antiprismatic structure, and that caesium forms the 12-coordinate [Cs(HO)] ion.\nThe chemistry of lithium shows several differences from that of the rest of the group as the small Li cation polarises anions and gives its compounds a more covalent character. Lithium and magnesium have a diagonal relationship due to their similar atomic radii, so that they show some similarities. For example, lithium forms a stable nitride, a property common among all the alkaline earth metals (magnesium's group) but unique among the alkali metals. In addition, among their respective groups, only lithium and magnesium form organometallic compounds with significant covalent character (e.g. LiMe and MgMe).\n\nLithium fluoride is the only alkali metal halide that is poorly soluble in water, and lithium hydroxide is the only alkali metal hydroxide that is not deliquescent. Conversely, lithium perchlorate and other lithium salts with large anions that cannot be polarised are much more stable than the analogous compounds of the other alkali metals, probably because Li has a high solvation energy. This effect also means that most simple lithium salts are commonly encountered in hydrated form, because the anhydrous forms are extremely hygroscopic: this allows salts like lithium chloride and lithium bromide to be used in dehumidifiers and air-conditioners.\n\nFrancium is also predicted to show some differences due to its high atomic weight, causing its electrons to travel at considerable fractions of the speed of light and thus making relativistic effects more prominent. In contrast to the trend of decreasing electronegativities and ionisation energies of the alkali metals, francium's electronegativity and ionisation energy are predicted to be higher than caesium's due to the relativistic stabilisation of the 7s electrons; also, its atomic radius is expected to be abnormally low. Thus, contrary to expectation, caesium is the most reactive of the alkali metals, not francium. All known physical properties of francium also deviate from the clear trends going from lithium to caesium, such as the first ionisation energy, electron affinity, and anion polarisability, though due to the paucity of known data about francium many sources give extrapolated values, ignoring that relativistic effects make the trend from lithium to caesium become inapplicable at francium. Some of the few properties of francium that have been predicted taking relativity into account are the electron affinity (47.2 kJ/mol) and the enthalpy of dissociation of the Fr molecule (42.1 kJ/mol). The CsFr molecule is polarised as CsFr, showing that the 7s subshell of francium is much more strongly affected by relativistic effects than the 6s subshell of caesium. Additionally, francium superoxide (FrO) is expected to have significant covalent character, unlike the other alkali metal superoxides, because of bonding contributions from the 6p electrons of francium.\n\nAll the alkali metals have odd atomic numbers; hence, their isotopes must be either odd–odd (both proton and neutron number are odd) or odd–even (proton number is odd, but neutron number is even). Odd–odd nuclei have even mass numbers, whereas odd–even nuclei have odd mass numbers. Odd–odd primordial nuclides are rare because most odd–odd nuclei are highly unstable with respect to beta decay, because the decay products are even–even, and are therefore more strongly bound, due to nuclear pairing effects.\n\nDue to the great rarity of odd–odd nuclei, almost all the primordial isotopes of the alkali metals are odd–even (the exceptions being the light stable isotope lithium-6 and the long-lived radioisotope potassium-40). For a given odd mass number, there can be only a single beta-stable nuclide, since there is not a difference in binding energy between even–odd and odd–even comparable to that between even–even and odd–odd, leaving other nuclides of the same mass number (isobars) free to beta decay toward the lowest-mass nuclide. An effect of the instability of an odd number of either type of nucleons is that odd-numbered elements, such as the alkali metals, tend to have fewer stable isotopes than even-numbered elements. Of the 26 monoisotopic elements that have only a single stable isotope, all but one have an odd atomic number and all but one also have an even number of neutrons. Beryllium is the single exception to both rules, due to its low atomic number.\n\nAll of the alkali metals except lithium and caesium have at least one naturally occurring radioisotope: sodium-22 and sodium-24 are trace radioisotopes produced cosmogenically, potassium-40 and rubidium-87 have very long half-lives and thus occur naturally, and all isotopes of francium are radioactive. Caesium was also thought to be radioactive in the early 20th century, although it has no naturally occurring radioisotopes. (Francium had not been discovered yet at that time.) The natural long-lived radioisotope of potassium, potassium-40, makes up about 0.012% of natural potassium, and thus natural potassium is weakly radioactive. This natural radioactivity became a basis for a mistaken claim of the discovery for element 87 (the next alkali metal after caesium) in 1925. Natural rubidium is similarly slightly radioactive, with 27.83% being the long-lived radioisotope rubidium-87.\n\nCaesium-137, with a half-life of 30.17 years, is one of the two principal medium-lived fission products, along with strontium-90, which are responsible for most of the radioactivity of spent nuclear fuel after several years of cooling, up to several hundred years after use. It constitutes most of the radioactivity still left from the Chernobyl accident. Caesium-137 undergoes high-energy beta decay and eventually becomes stable barium-137. It is a strong emitter of gamma radiation. Caesium-137 has a very low rate of neutron capture and cannot be feasibly disposed of in this way, but must be allowed to decay. Caesium-137 has been used as a tracer in hydrologic studies, analogous to the use of tritium. Small amounts of caesium-134 and caesium-137 were released into the environment during nearly all nuclear weapon tests and some nuclear accidents, most notably the Goiânia accident and the Chernobyl disaster. As of 2005, caesium-137 is the principal source of radiation in the zone of alienation around the Chernobyl nuclear power plant. Its chemical properties as one of the alkali metals make it one of most problematic of the short-to-medium-lifetime fission products because it easily moves and spreads in nature due to the high water solubility of its salts, and is taken up by the body, which mistakes it for its essential congeners sodium and potassium.\n\nThe alkali metals are more similar to each other than the elements in any other group are to each other. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation. In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium.\n\nThe atomic radii of the alkali metals increase going down the group. Because of the shielding effect, when an atom has more than one electron shell, each electron feels electric repulsion from the other electrons as well as electric attraction from the nucleus. In the alkali metals, the outermost electron only feels a net charge of +1, as some of the nuclear charge (which is equal to the atomic number) is cancelled by the inner electrons; the number of inner electrons of an alkali metal is always one less than the nuclear charge. Therefore, the only factor which affects the atomic radius of the alkali metals is the number of electron shells. Since this number increases down the group, the atomic radius must also increase down the group.\n\nThe ionic radii of the alkali metals are much smaller than their atomic radii. This is because the outermost electron of the alkali metals is in a different electron shell than the inner electrons, and thus when it is removed the resulting atom has one fewer electron shell and is smaller. Additionally, the effective nuclear charge has increased, and thus the electrons are attracted more strongly towards the nucleus and the ionic radius decreases.\n\nThe first ionisation energy of an element or molecule is the energy required to move the most loosely held electron from one mole of gaseous atoms of the element or molecules to form one mole of gaseous ions with electric charge +1. The factors affecting the first ionisation energy are the nuclear charge, the amount of shielding by the inner electrons and the distance from the most loosely held electron from the nucleus, which is always an outer electron in main group elements. The first two factors change the effective nuclear charge the most loosely held electron feels. Since the outermost electron of alkali metals always feels the same effective nuclear charge (+1), the only factor which affects the first ionisation energy is the distance from the outermost electron to the nucleus. Since this distance increases down the group, the outermost electron feels less attraction from the nucleus and thus the first ionisation energy decreases. (This trend is broken in francium due to the relativistic stabilisation and contraction of the 7s orbital, bringing francium's valence electron closer to the nucleus than would be expected from non-relativistic calculations. This makes francium's outermost electron feel more attraction from the nucleus, increasing its first ionisation energy slightly beyond that of caesium.)\n\nThe second ionisation energy of the alkali metals is much higher than the first as the second-most loosely held electron is part of a fully filled electron shell and is thus difficult to remove.\n\nThe reactivities of the alkali metals increase going down the group. This is the result of a combination of two factors: the first ionisation energies and atomisation energies of the alkali metals. Because the first ionisation energy of the alkali metals decreases down the group, it is easier for the outermost electron to be removed from the atom and participate in chemical reactions, thus increasing reactivity down the group. The atomisation energy measures the strength of the metallic bond of an element, which falls down the group as the atoms increase in radius and thus the metallic bond must increase in length, making the delocalised electrons further away from the attraction of the nuclei of the heavier alkali metals. Adding the atomisation and first ionisation energies gives a quantity closely related to (but not equal to) the activation energy of the reaction of an alkali metal with another substance. This quantity decreases going down the group, and so does the activation energy; thus, chemical reactions can occur faster and the reactivity increases down the group.\n\nElectronegativity is a chemical property that describes the tendency of an atom or a functional group to attract electrons (or electron density) towards itself. If the bond between sodium and chlorine in sodium chloride were covalent, the pair of shared electrons would be attracted to the chlorine because the effective nuclear charge on the outer electrons is +7 in chlorine but is only +1 in sodium. The electron pair is attracted so close to the chlorine atom that they are practically transferred to the chlorine atom (an ionic bond). However, if the sodium atom was replaced by a lithium atom, the electrons will not be attracted as close to the chlorine atom as before because the lithium atom is smaller, making the electron pair more strongly attracted to the closer effective nuclear charge from lithium. Hence, the larger alkali metal atoms (further down the group) will be less electronegative as the bonding pair is less strongly attracted towards them. As mentioned previously, francium is expected to be an exception.\n\nBecause of the higher electronegativity of lithium, some of its compounds have a more covalent character. For example, lithium iodide (LiI) will dissolve in organic solvents, a property of most covalent compounds. Lithium fluoride (LiF) is the only alkali halide that is not soluble in water, and lithium hydroxide (LiOH) is the only alkali metal hydroxide that is not deliquescent.\n\nThe melting point of a substance is the point where it changes state from solid to liquid while the boiling point of a substance (in liquid state) is the point where the vapour pressure of the liquid equals the environmental pressure surrounding the liquid and all the liquid changes state to gas. As a metal is heated to its melting point, the metallic bonds keeping the atoms in place weaken so that the atoms can move around, and the metallic bonds eventually break completely at the metal's boiling point. Therefore, the falling melting and boiling points of the alkali metals indicate that the strength of the metallic bonds of the alkali metals decreases down the group. This is because metal atoms are held together by the electromagnetic attraction from the positive ions to the delocalised electrons. As the atoms increase in size going down the group (because their atomic radius increases), the nuclei of the ions move further away from the delocalised electrons and hence the metallic bond becomes weaker so that the metal can more easily melt and boil, thus lowering the melting and boiling points. (The increased nuclear charge is not a relevant factor due to the shielding effect.)\n\nThe alkali metals all have the same crystal structure (body-centred cubic) and thus the only relevant factors are the number of atoms that can fit into a certain volume and the mass of one of the atoms, since density is defined as mass per unit volume. The first factor depends on the volume of the atom and thus the atomic radius, which increases going down the group; thus, the volume of an alkali metal atom increases going down the group. The mass of an alkali metal atom also increases going down the group. Thus, the trend for the densities of the alkali metals depends on their atomic weights and atomic radii; if figures for these two factors are known, the ratios between the densities of the alkali metals can then be calculated. The resultant trend is that the densities of the alkali metals increase down the table, with an exception at potassium. Due to having the lowest atomic weight of all the elements in their period and having the largest atomic radius for their periods, the alkali metals are the least dense metals in the periodic table. Lithium, sodium, and potassium are the only three metals in the periodic table that are less dense than water: in fact, lithium is the least dense known solid at room temperature.\n\nThe alkali metals form complete series of compounds with all usually encountered anions, which well illustrate group trends. These compounds can be described as involving the alkali metals losing electrons to acceptor species and forming monopositive ions. This description is most accurate for alkali halides and becomes less and less accurate as cationic and anionic charge increase, and as the anion becomes larger and more polarisable. For instance, ionic bonding gives way to metallic bonding along the series NaCl, NaO, NaS, NaP, NaAs, NaSb, NaBi, Na.\n\nAll the alkali metals react vigorously or explosively with cold water, producing an aqueous solution of a strongly basic alkali metal hydroxide and releasing hydrogen gas. This reaction becomes more vigorous going down the group: lithium reacts steadily with effervescence, but sodium and potassium can ignite and rubidium and caesium sink in water and generate hydrogen gas so rapidly that shock waves form in the water that may shatter glass containers. When an alkali metal is dropped into water, it produces an explosion, of which there are two separate stages. The metal reacts with the water first, breaking the hydrogen bonds in the water and producing hydrogen gas; this takes place faster for the more reactive heavier alkali metals. Second, the heat generated by the first part of the reaction often ignites the hydrogen gas, causing it to burn explosively into the surrounding air. This secondary hydrogen gas explosion produces the visible flame above the bowl of water, lake or other body of water, not the initial reaction of the metal with water (which tends to happen mostly under water). The alkali metal hydroxides are the most basic known hydroxides.\n\nRecent research has suggested that the explosive behavior of alkali metals in water is driven by a Coulomb explosion rather than solely by rapid generation of hydrogen itself. All alkali metals melt as a part of the reaction with water. Water molecules ionise the bare metallic surface of the liquid metal, leaving a positively charged metal surface and negatively charged water ions. The attraction between the charged metal and water ions will rapidly increase the surface area, causing an exponential increase of ionisation. When the repulsive forces within the liquid metal surface exceeds the forces of the surface tension, it vigorously explodes.\n\nThe hydroxides themselves are the most basic hydroxides known, reacting with acids to give salts and with alcohols to give oligomeric alkoxides. They easily react with carbon dioxide to form carbonates or bicarbonates, or with hydrogen sulfide to form sulfides or bisulfides, and may be used to separate thiols from petroleum. They react with amphoteric oxides: for example, the oxides of aluminium, zinc, tin, and lead react with the alkali metal hydroxides to give aluminates, zincates, stannates, and plumbates. Silicon dioxide is acidic, and thus the alkali metal hydroxides can also attack silicate glass.\n\nThe alkali metals form many intermetallic compounds with each other and the elements from groups 2 to 13 in the periodic table of varying stoichiometries, such as the sodium amalgams with mercury, including NaHg and NaHg. Some of these have ionic characteristics: taking the alloys with gold, the most electronegative of metals, as an example, NaAu and KAu are metallic, but RbAu and CsAu are semiconductors. NaK is an alloy of sodium and potassium that is very useful because it is liquid at room temperature, although precautions must be taken due to its extreme reactivity towards water and air. The eutectic mixture melts at −12.6 °C. An alloy of 41% caesium, 47% sodium, and 12% potassium has the lowest known melting point of any metal or alloy, −78 °C.\n\nThe intermetallic compounds of the alkali metals with the heavier group 13 elements (aluminium, gallium, indium, and thallium), such as NaTl, are poor conductors or semiconductors, unlike the normal alloys with the preceding elements, implying that the alkali metal involved has lost an electron to the Zintl anions involved. Nevertheless, while the elements in group 14 and beyond tend to form discrete anionic clusters, group 13 elements tend to form polymeric ions with the alkali metal cations located between the giant ionic lattice. For example, NaTl consists of a polymeric anion (—Tl—) with a covalent diamond cubic structure with Na ions located between the anionic lattice. The larger alkali metals cannot fit similarly into an anionic lattice and tend to force the heavier group 13 elements to form anionic clusters.\n\nBoron is a special case, being the only nonmetal in group 13. The alkali metal borides tend to be boron-rich, involving appreciable boron–boron bonding involving deltahedral structures, and are thermally unstable due to the alkali metals having a very high vapour pressure at elevated temperatures. This makes direct synthesis problematic because the alkali metals do not react with boron below 700 °C, and thus this must be accomplished in sealed containers with the alkali metal in excess. Furthermore, exceptionally in this group, reactivity with boron decreases down the group: lithium reacts completely at 700 °C, but sodium at 900 °C and potassium not until 1200 °C, and the reaction is instantaneous for lithium but takes hours for potassium. Rubidium and caesium borides have not even been characterised. Various phases are known, such as LiB, NaB, NaB, and KB. Under high pressure the boron–boron bonding in the lithium borides changes from following Wade's rules to forming Zintl anions like the rest of group 13.\n\nLithium and sodium react with carbon to form acetylides, LiC and NaC, which can also be obtained by reaction of the metal with acetylene. Potassium, rubidium, and caesium react with graphite; their atoms are intercalated between the hexagonal graphite layers, forming graphite intercalation compounds of formulae MC (dark grey, almost black), MC (dark grey, almost black), MC (blue), MC (steel blue), and MC (bronze) (M = K, Rb, or Cs). These compounds are over 200 times more electrically conductive than pure graphite, suggesting that the valence electron of the alkali metal is transferred to the graphite layers (e.g. ). Upon heating of KC, the elimination of potassium atoms results in the conversion in sequence to KC, KC, KC and finally KC. KC is a very strong reducing agent and is pyrophoric and explodes on contact with water. While the larger alkali metals (K, Rb, and Cs) initially form MC, the smaller ones initially form MC, and indeed they require reaction of the metals with graphite at high temperatures around 500 °C to form. Apart from this, the alkali metals are such strong reducing agents that they can even reduce buckminsterfullerene to produce solid fullerides MC; sodium, potassium, rubidium, and caesium can form fullerides where \"n\" = 2, 3, 4, or 6, and rubidium and caesium additionally can achieve \"n\" = 1.\n\nWhen the alkali metals react with the heavier elements in the carbon group (silicon, germanium, tin, and lead), ionic substances with cage-like structures are formed, such as the silicides MSi (M = K, Rb, or Cs), which contains M and tetrahedral ions. The chemistry of alkali metal germanides, involving the germanide ion Ge and other cluster (Zintl) ions such as , , , and [(Ge)], is largely analogous to that of the corresponding silicides. Alkali metal stannides are mostly ionic, sometimes with the stannide ion (Sn), and sometimes with more complex Zintl ions such as , which appears in tetrapotassium nonastannide (KSn). The monatomic plumbide ion (Pb) is unknown, and indeed its formation is predicted to be energetically unfavourable; alkali metal plumbides have complex Zintl ions, such as . These alkali metal germanides, stannides, and plumbides may be produced by reducing germanium, tin, and lead with sodium metal in liquid ammonia.\n\nLithium, the lightest of the alkali metals, is the only alkali metal which reacts with nitrogen at standard conditions, and its nitride is the only stable alkali metal nitride. Nitrogen is an unreactive gas because breaking the strong triple bond in the dinitrogen molecule (N) requires a lot of energy. The formation of an alkali metal nitride would consume the ionisation energy of the alkali metal (forming M ions), the energy required to break the triple bond in N and the formation of N ions, and all the energy released from the formation of an alkali metal nitride is from the lattice energy of the alkali metal nitride. The lattice energy is maximised with small, highly charged ions; the alkali metals do not form highly charged ions, only forming ions with a charge of +1, so only lithium, the smallest alkali metal, can release enough lattice energy to make the reaction with nitrogen exothermic, forming lithium nitride. The reactions of the other alkali metals with nitrogen would not release enough lattice energy and would thus be endothermic, so they do not form nitrides at standard conditions. Sodium nitride (NaN) and potassium nitride (KN), while existing, are extremely unstable, being prone to decomposing back into their constituent elements, and cannot be produced by reacting the elements with each other at standard conditions. Steric hindrance forbids the existence of rubidium or caesium nitride. However, sodium and potassium form colourless azide salts involving the linear anion; due to the large size of the alkali metal cations, they are thermally stable enough to be able to melt before decomposing.\n\nAll the alkali metals react readily with phosphorus and arsenic to form phosphides and arsenides with the formula MPn (where M represents an alkali metal and Pn represents a pnictogen – phosphorus, arsenic, antimony, or bismuth). This is due to the greater size of the P and As ions, so that less lattice energy needs to be released for the salts to form. These are not the only phosphides and arsenides of the alkali metals: for example, potassium has nine different known phosphides, with formulae KP, KP, KP, KP, KP, KP, KP, KP, and KP. While most metals form arsenides, only the alkali and alkaline earth metals form mostly ionic arsenides. The structure of NaAs is complex with unusually short Na–Na distances of 328–330 pm which are shorter than in sodium metal, and this indicates that even with these electropositive metals the bonding cannot be straightforwardly ionic. Other alkali metal arsenides not conforming to the formula MAs are known, such as LiAs, which has a metallic lustre and electrical conductivity indicating the presence of some metallic bonding. The antimonides are unstable and reactive as the Sb ion is a strong reducing agent; reaction of them with acids form the toxic and unstable gas stibine (SbH). Indeed, they have some metallic properties, and the alkali metal antimonides of stoichiometry MSb involve antimony atoms bonded in a spiral Zintl structure. Bismuthides are not even wholly ionic; they are intermetallic compounds containing partially metallic and partially ionic bonds.\n\nAll the alkali metals react vigorously with oxygen at standard conditions. They form various types of oxides, such as simple oxides (containing the O ion), peroxides (containing the ion, where there is a single bond between the two oxygen atoms), superoxides (containing the ion), and many others. Lithium burns in air to form lithium oxide, but sodium reacts with oxygen to form a mixture of sodium oxide and sodium peroxide. Potassium forms a mixture of potassium peroxide and potassium superoxide, while rubidium and caesium form the superoxide exclusively. Their reactivity increases going down the group: while lithium, sodium and potassium merely burn in air, rubidium and caesium are pyrophoric (spontaneously catch fire in air).\n\nThe smaller alkali metals tend to polarise the larger anions (the peroxide and superoxide) due to their small size. This attracts the electrons in the more complex anions towards one of its constituent oxygen atoms, forming an oxide ion and an oxygen atom. This causes lithium to form the oxide exclusively on reaction with oxygen at room temperature. This effect becomes drastically weaker for the larger sodium and potassium, allowing them to form the less stable peroxides. Rubidium and caesium, at the bottom of the group, are so large that even the least stable superoxides can form. Because the superoxide releases the most energy when formed, the superoxide is preferentially formed for the larger alkali metals where the more complex anions are not polarised. (The oxides and peroxides for these alkali metals do exist, but do not form upon direct reaction of the metal with oxygen at standard conditions.) In addition, the small size of the Li and O ions contributes to their forming a stable ionic lattice structure. Under controlled conditions, however, all the alkali metals, with the exception of francium, are known to form their oxides, peroxides, and superoxides. The alkali metal peroxides and superoxides are powerful oxidising agents. Sodium peroxide and potassium superoxide react with carbon dioxide to form the alkali metal carbonate and oxygen gas, which allows them to be used in submarine air purifiers; the presence of water vapour, naturally present in breath, makes the removal of carbon dioxide by potassium superoxide even more efficient. All the stable alkali metals except lithium can form red ozonides (MO) through low-temperature reaction of the powdered anhydrous hydroxide with ozone: the ozonides may be then extracted using liquid ammonia. They slowly decompose at standard conditions to the superoxides and oxygen, and hydrolyse immediately to the hydroxides when in contact with water. Potassium, rubidium, and caesium also form sesquioxides MO, which may be better considered peroxide disuperoxides, .\n\nRubidium and caesium can form a great variety of suboxides with the metals in formal oxidation states below +1. Rubidium can form RbO and RbO (copper-coloured) upon oxidation in air, while caesium forms an immense variety of oxides, such as the ozonide CsO and several brightly coloured suboxides, such as CsO (bronze), CsO (red-violet), CsO (violet), CsO (dark green), CsO, CsO, as well as CsO. The last of these may be heated under vacuum to generate CsO.\n\nThe alkali metals can also react analogously with the heavier chalcogens (sulfur, selenium, tellurium, and polonium), and all the alkali metal chalcogenides are known (with the exception of francium's). Reaction with an excess of the chalcogen can similarly result in lower chalcogenides, with chalcogen ions containing chains of the chalcogen atoms in question. For example, sodium can react with sulfur to form the sulfide (NaS) and various polysulfides with the formula NaS (\"x\" from 2 to 6), containing the ions. Due to the basicity of the Se and Te ions, the alkali metal selenides and tellurides are alkaline in solution; when reacted directly with selenium and tellurium, alkali metal polyselenides and polytellurides are formed along with the selenides and tellurides with the and ions. They may be obtained directly from the elements in liquid ammonia or when air is not present, and are colourless, water-soluble compounds that air oxidises quickly back to selenium or tellurium. The alkali metal polonides are all ionic compounds containing the Po ion; they are very chemically stable and can be produced by direct reaction of the elements at around 300–400 °C.\n\nThe alkali metals are among the most electropositive elements on the periodic table and thus tend to bond ionically to the most electronegative elements on the periodic table, the halogens (fluorine, chlorine, bromine, iodine, and astatine), forming salts known as the alkali metal halides. The reaction is very vigorous and can sometimes result in explosions. All twenty stable alkali metal halides are known; the unstable ones are not known, with the exception of sodium astatide, because of the great instability and rarity of astatine and francium. The most well-known of the twenty is certainly sodium chloride, otherwise known as common salt. All of the stable alkali metal halides have the formula MX where M is an alkali metal and X is a halogen. They are all white ionic crystalline solids that have high melting points. All the alkali metal halides are soluble in water except for lithium fluoride (LiF), which is insoluble in water due to its very high lattice enthalpy. The high lattice enthalpy of lithium fluoride is due to the small sizes of the Li and F ions, causing the electrostatic interactions between them to be strong: a similar effect occurs for magnesium fluoride, consistent with the diagonal relationship between lithium and magnesium.\n\nThe alkali metals also react similarly with hydrogen to form ionic alkali metal hydrides, where the hydride anion acts as a pseudohalide: these are often used as reducing agents, producing hydrides, complex metal hydrides, or hydrogen gas. Other pseudohalides are also known, notably the cyanides. These are isostructural to the respective halides except for lithium cyanide, indicating that the cyanide ions may rotate freely. Ternary alkali metal halide oxides, such as NaClO, KBrO (yellow), NaBrO, NaIO, and KBrO, are also known. The polyhalides are rather unstable, although those of rubidium and caesium are greatly stabilised by the feeble polarising power of these extremely large cations.\n\nAlkali metal cations do not usually form coordination complexes with simple Lewis bases due to their low charge of just +1 and their relatively large size; thus the Li ion forms most complexes and the heavier alkali metal ions form less and less (though exceptions occur for weak complexes). Lithium in particular has a very rich coordination chemistry in which it exhibits coordination numbers from 1 to 12, although octahedral hexacoordination is its preferred mode. In aqueous solution, the alkali metal ions exist as octahedral hexahydrate complexes ([M(HO))]), with the exception of the lithium ion, which due to its small size forms tetrahedral tetrahydrate complexes ([Li(HO))]); the alkali metals form these complexes because their ions are attracted by electrostatic forces of attraction to the polar water molecules. Because of this, anhydrous salts containing alkali metal cations are often used as desiccants. Alkali metals also readily form complexes with crown ethers (e.g. 12-crown-4 for Li, 15-crown-5 for Na, 18-crown-6 for K, and 21-crown-7 for Rb) and cryptands due to electrostatic attraction.\n\nThe alkali metals dissolve slowly in liquid ammonia, forming ammoniacal solutions of solvated M and e, which react to form hydrogen gas and the alkali metal amide (MNH, where M represents an alkali metal): this was first noted by Humphry Davy in 1809 and rediscovered by W. Weyl in 1864. The process may be speeded up by a catalyst. Similar solutions are formed by the heavy divalent alkaline earth metals calcium, strontium, barium, as well as the divalent lanthanides, europium and ytterbium. The amide salt is quite insoluble and readily precipitates out of solution, leaving intensely coloured ammonia solutions of the alkali metals. In 1907, Charles Krause identified the colour as being due to the presence of solvated electrons, which contribute to the high electrical conductivity of these solutions. At low concentrations (below 3 M), the solution is dark blue and has ten times the conductivity of aqueous sodium chloride; at higher concentrations (above 3 M), the solution is copper-coloured and has approximately the conductivity of liquid metals like mercury. In addition to the alkali metal amide salt and solvated electrons, such ammonia solutions also contain the alkali metal cation (M), the neutral alkali metal atom (M), diatomic alkali metal molecules (M) and alkali metal anions (M). These are unstable and eventually become the more thermodynamically stable alkali metal amide and hydrogen gas. Solvated electrons are powerful reducing agents and are often used in chemical synthesis.\n\nBeing the smallest alkali metal, lithium forms the widest variety of and most stable organometallic compounds, which are bonded covalently. Organolithium compounds are electrically non-conducting volatile solids or liquids that melt at low temperatures, and tend to form oligomers with the structure (RLi) where R is the organic group. As the electropositive nature of lithium puts most of the charge density of the bond on the carbon atom, effectively creating a carbanion, organolithium compounds are extremely powerful bases and nucleophiles. For use as bases, butyllithiums are often used and are commercially available. An example of an organolithium compound is methyllithium ((CHLi)), which exists in tetrameric (\"x\" = 4, tetrahedral) and hexameric (\"x\" = 6, octahedral) forms. Organolithium compounds, especially \"n\"-butyllithium, are useful reagents in organic synthesis, as might be expected given lithium's diagonal relationship with magnesium, which plays an important role in the Grignard reaction. For example, alkyllithiums and aryllithiums may be used to synthesise aldehydes and ketones by reaction with metal carbonyls. The reaction with nickel tetracarbonyl, for example, proceeds through an unstable acyl nickel carbonyl complex which then undergoes electrophilic substitution to give the desired aldehyde (using H as the electrophile) or ketone (using an alkyl halide) product.\n\nAlkyllithiums and aryllithiums may also react with \"N\",\"N\"-disubstituted amides to give aldehydes and ketones, and symmetrical ketones by reacting with carbon monoxide. They thermally decompose to eliminate a β-hydrogen, producing alkenes and lithium hydride: another route is the reaction of ethers with alkyl- and aryllithiums that act as strong bases. In non-polar solvents, aryllithiums react as the carbanions they effectively are, turning carbon dioxide to aromatic carboxylic acids (ArCOH) and aryl ketones to tertiary carbinols (Ar'C(Ar)OH). Finally, they may be used to synthesise other organometallic compounds through metal-halogen exchange.\n\nUnlike the organolithium compounds, the organometallic compounds of the heavier alkali metals are predominantly ionic. The application of organosodium compounds in chemistry is limited in part due to competition from organolithium compounds, which are commercially available and exhibit more convenient reactivity. The principal organosodium compound of commercial importance is sodium cyclopentadienide. Sodium tetraphenylborate can also be classified as an organosodium compound since in the solid state sodium is bound to the aryl groups. Organometallic compounds of the higher alkali metals are even more reactive than organosodium compounds and of limited utility. A notable reagent is Schlosser's base, a mixture of \"n\"-butyllithium and potassium \"tert\"-butoxide. This reagent reacts with propene to form the compound allylpotassium (KCHCHCH). \"cis\"-2-Butene and \"trans\"-2-butene equilibrate when in contact with alkali metals. Whereas isomerisation is fast with lithium and sodium, it is slow with the heavier alkali metals. The heavier alkali metals also favour the sterically congested conformation. Several crystal structures of organopotassium compounds have been reported, establishing that they, like the sodium compounds, are polymeric. Organosodium, organopotassium, organorubidium and organocaesium compounds are all mostly ionic and are insoluble (or nearly so) in nonpolar solvents.\n\nAlkyl and aryl derivatives of sodium and potassium tend to react with air. They cause the cleavage of ethers, generating alkoxides. Unlike alkyllithium compounds, alkylsodiums and alkylpotassiums cannot be made by reacting the metals with alkyl halides because Wurtz coupling occurs:\n\nAs such, they have to be made by reacting alkylmercury compounds with sodium or potassium metal in inert hydrocarbon solvents. While methylsodium forms tetramers like methyllithium, methylpotassium is more ionic and has the nickel arsenide structure with discrete methyl anions and potassium cations.\n\nThe alkali metals and their hydrides react with acidic hydrocarbons, for example cyclopentadienes and terminal alkynes, to give salts. Liquid ammonia, ether, or hydrocarbon solvents are used, the most common of which being tetrahydrofuran. The most important of these compounds is sodium cyclopentadienide, NaCH, an important precursor to many transition metal cyclopentadienyl derivatives. Similarly, the alkali metals react with cyclooctatetraene in tetrahydrofuran to give alkali metal cyclooctatetraenides; for example, dipotassium cyclooctatetraenide (KCH) is an important precursor to many metal cyclooctatetraenyl derivatives, such as uranocene. The large and very weakly polarising alkali metal cations can stabilise large, aromatic, polarisable radical anions, such as the dark-green sodium naphthalenide, Na[CH•], a strong reducing agent.\n\nAlthough francium is the heaviest alkali metal that has been discovered, there has been some theoretical work predicting the physical and chemical characteristics of the hypothetical heavier alkali metals. Being the first period 8 element, the undiscovered element ununennium (element 119) is predicted to be the next alkali metal after francium and behave much like their lighter congeners; however, it is also predicted to differ from the lighter alkali metals in some properties. Its chemistry is predicted to be closer to that of potassium or rubidium instead of caesium or francium. This is unusual as periodic trends, ignoring relativistic effects would predict ununennium to be even more reactive than caesium and francium. This lowered reactivity is due to the relativistic stabilisation of ununennium's valence electron, increasing ununennium's first ionisation energy and decreasing the metallic and ionic radii; this effect is already seen for francium. This assumes that ununennium will behave chemically as an alkali metal, which, although likely, may not be true due to relativistic effects. The relativistic stabilisation of the 8s orbital also increases ununennium's electron affinity far beyond that of caesium and francium; indeed, ununennium is expected to have an electron affinity higher than all the alkali metals lighter than it. Relativistic effects also cause a very large drop in the polarisability of ununennium. On the other hand, ununennium is predicted to continue the trend of melting points decreasing going down the group, being expected to have a melting point between 0 °C and 30 °C.\nThe stabilisation of ununennium's valence electron and thus the contraction of the 8s orbital cause its atomic radius to be lowered to 240 pm, very close to that of rubidium (247 pm), so that the chemistry of ununennium in the +1 oxidation state should be more similar to the chemistry of rubidium than to that of francium. On the other hand, the ionic radius of the Uue ion is predicted to be larger than that of Rb, because the 7p orbitals are destabilised and are thus larger than the p-orbitals of the lower shells. Ununennium may also show the +3 oxidation state, which is not seen in any other alkali metal, in addition to the +1 oxidation state that is characteristic of the other alkali metals and is also the main oxidation state of all the known alkali metals: this is because of the destabilisation and expansion of the 7p spinor, causing its outermost electrons to have a lower ionisation energy than what would otherwise be expected. Indeed, many ununennium compounds are expected to have a large covalent character, due to the involvement of the 7p electrons in the bonding.\nNot as much work has been done predicting the properties of the alkali metals beyond ununennium. Although a simple extrapolation of the periodic table would put element 169, unhexennium, under ununennium, Dirac-Fock calculations predict that the next alkali metal after ununennium may actually be element 165, unhexpentium, which is predicted to have the electron configuration [Og] 5 g 6f 7d 8s 8p 9s. Furthermore, this element would be intermediate in properties between an alkali metal and a group 11 element, and while its physical and atomic properties would be closer to the former, its chemistry may be closer to that of the latter. Further calculations show that unhexpentium would follow the trend of increasing ionisation energy beyond caesium, having an ionisation energy comparable to that of sodium, and that it should also continue the trend of decreasing atomic radii beyond caesium, having an atomic radius comparable to that of potassium. However, the 7d electrons of unhexpentium may also be able to participate in chemical reactions along with the 9s electron, possibly allowing oxidation states beyond +1, whence the likely transition metal behaviour of unhexpentium. Due to the alkali and alkaline earth metals both being s-block elements, these predictions for the trends and properties of ununennium and unhexpentium also mostly hold quite similarly for the corresponding alkaline earth metals unbinilium (Ubn) and unhexhexium (Uhh).\n\nThe probable properties of further alkali metals beyond unhexpentium have not been explored yet as of 2015; in fact, it is suspected that they may not be able to exist. In periods 8 and above of the periodic table, relativistic and shell-structure effects become so strong that extrapolations from lighter congeners become completely inaccurate. In addition, the relativistic and shell-structure effects (which stabilise the s-orbitals and destabilise and expand the d-, f-, and g-orbitals of higher shells) have opposite effects, causing even larger difference between relativistic and non-relativistic calculations of the properties of elements with such high atomic numbers. Interest in the chemical properties of ununennium and unhexpentium stems from the fact that both elements are located close to the expected locations of islands of stabilities, centered at elements 122 (Ubb) and 164 (Uhq).\n\nMany other substances are similar to the alkali metals in their tendency to form monopositive cations. Analogously to the pseudohalogens, they have sometimes been called \"pseudo-alkali metals\". These substances include some elements and many more polyatomic ions; the polyatomic ions are especially similar to the alkali metals in their large size and weak polarising power.\n\nThe element hydrogen, with one electron per neutral atom, is usually placed at the top of Group 1 of the periodic table for convenience, but hydrogen is not normally considered to be an alkali metal; when it is considered to be an alkali metal, it is because of its atomic properties and not its chemical properties. Under typical conditions, pure hydrogen exists as a diatomic gas consisting of two atoms per molecule (H); however, the alkali metals only form diatomic molecules (such as dilithium, Li) at high temperatures, when they are in the gaseous state.\n\nHydrogen, like the alkali metals, has one valence electron and reacts easily with the halogens, but the similarities end there because of the small size of a bare proton H compared to the alkali metal cations. Its placement above lithium is primarily due to its electron configuration. It is sometimes placed above carbon due to their similar electronegativities or fluorine due to their similar chemical properties.\n\nThe first ionisation energy of hydrogen (1312.0 kJ/mol) is much higher than that of the alkali metals. As only one additional electron is required to fill in the outermost shell of the hydrogen atom, hydrogen often behaves like a halogen, forming the negative hydride ion, and is very occasionally considered to be a halogen on that basis. (The alkali metals can also form negative ions, known as alkalides, but these are little more than laboratory curiosities, being unstable.) An argument against this placement is that formation of hydride from hydrogen is endothermic, unlike the exothermic formation of halides from halogens. The radius of the H anion also does not fit the trend of increasing size going down the halogens: indeed, H is very diffuse because its single proton cannot easily control both electrons. It was expected for some time that liquid hydrogen would show metallic properties; while this has been shown to not be the case, under extremely high pressures, such as those found at the cores of Jupiter and Saturn, hydrogen does become metallic and behaves like an alkali metal; in this phase, it is known as metallic hydrogen. The electrical resistivity of liquid metallic hydrogen at 3000 K is approximately equal to that of liquid rubidium and caesium at 2000 K at the respective pressures when they undergo a nonmetal-to-metal transition.\n\nThe 1s electron configuration of hydrogen, while superficially similar to that of the alkali metals (ns), is unique because there is no 1p subshell. Hence it can lose an electron to form the hydron H, or gain one to form the hydride ion H. In the former case it resembles superficially the alkali metals; in the latter case, the halogens, but the differences due to the lack of a 1p subshell are important enough that neither group fits the properties of hydrogen well. Group 14 is also a good fit in terms of thermodynamic properties such as ionisation energy and electron affinity, but makes chemical nonsense because hydrogen cannot be tetravalent. Thus none of the three placements are entirely satisfactory, although group 1 is the most common placement (if one is chosen) because the hydron is by far the most important of all monatomic hydrogen species, being the foundation of acid-base chemistry. As an example of hydrogen's unorthodox properties stemming from its unusual electron configuration and small size, the hydrogen ion is very small (radius around 150 fm compared to the 50–220 pm size of most other atoms and ions) and so is nonexistent in condensed systems other than in association with other atoms or molecules. Indeed, transferring of protons between chemicals is the basis of acid-base chemistry. Also unique is hydrogen's ability to form hydrogen bonds, which are an effect of charge-transfer, electrostatic, and electron correlative contributing phenomena. While analogous lithium bonds are also known, they are mostly electrostatic. Nevertheless, hydrogen can take on the same structural role as the alkali metals in some molecular crystals, and has a close relationship with the lightest alkali metals (especially lithium).\n\nThe ammonium ion () has very similar properties to the heavier alkali metals, acting as an alkali metal intermediate between potassium and rubidium, and is often considered a close relative. For example, most alkali metal salts are soluble in water, a property which ammonium salts share. Ammonium is expected to behave stably as a metal ( ions in a sea of delocalised electrons) at very high pressures (though less than the typical pressure where transitions from insulating to metallic behaviour occur around, 100 GPa), and could possibly occur inside the ice giants Uranus and Neptune, which may have significant impacts on their interior magnetic fields. It has been estimated that the transition from a mixture of ammonia and dihydrogen molecules to metallic ammonium may occur at pressures just below 25 GPa. Under standard conditions, ammonium can form a metallic amalgam with mercury.\n\nOther \"pseudo-alkali metals\" include the alkylammonium cations, in which some of the hydrogen atoms in the ammonium cation are replaced by alkyl or aryl groups. In particular, the quaternary ammonium cations () are very useful since they are permanently charged, and they are often used as an alternative to the expensive Cs to stabilise very large and very easily polarisable anions such as . Tetraalkylammonium hydroxides, like alkali metal hydroxides, are very strong bases that react with atmospheric carbon dioxide to form carbonates. Furthermore, the nitrogen atom may be replaced by a phosphorus, arsenic, or antimony atom (the heavier nonmetallic pnictogens), creating a phosphonium () or arsonium () cation that can itself be substituted similarly; while stibonium () itself is not known, some of its organic derivatives are characterised.\n\nCobaltocene, Co(CH), is a metallocene, the cobalt analogue of ferrocene. It is a dark purple solid. Cobaltocene has 19 valence electrons, one more than usually found in organotransition metal complexes, such as its very stable relative, ferrocene, in accordance with the 18-electron rule. This additional electron occupies an orbital that is antibonding with respect to the Co–C bonds. Consequently, many chemical reactions of Co(CH) are characterized by its tendency to lose this \"extra\" electron, yielding a very stable 18-electron cation known as cobaltocenium. Many cobaltocenium salts coprecipitate with caesium salts, and cobaltocenium hydroxide is a strong base that absorbs atmospheric carbon dioxide to form cobaltocenium carbonate. Like the alkali metals, cobaltocene is a strong reducing agent, and decamethylcobaltocene is stronger still due to the combined inductive effect of the ten methyl groups. Cobalt may be substituted by its heavier congener rhodium to give rhodocene, an even stronger reducing agent. Iridocene (involving iridium) would presumably be still more potent, but is not very well-studied due to its instability.\n\nThallium is the heaviest stable element in group 13 of the periodic table. At the bottom of the periodic table, the inert pair effect is quite strong, because of the relativistic stabilisation of the 6s orbital and the decreasing bond energy as the atoms increase in size so that the amount of energy released in forming two more bonds is not worth the high ionisation energies of the 6s electrons. It displays the +1 oxidation state that all the known alkali metals display, and thallium compounds with thallium in its +1 oxidation state closely resemble the corresponding potassium or silver compounds stoichiometrically due to the similar ionic radii of the Tl (164 pm), K (152 pm) and Ag (129 pm) ions. It was sometimes considered an alkali metal in continental Europe (but not in England) in the years immediately following its discovery, and was placed just after caesium as the sixth alkali metal in Dmitri Mendeleev's 1869 periodic table and Julius Lothar Meyer's 1868 periodic table. (Mendeleev's 1871 periodic table and Meyer's 1870 periodic table put thallium in its current position in the boron group and left the space below caesium blank.) However, thallium also displays the oxidation state +3, which no known alkali metal displays (although ununennium, the undiscovered seventh alkali metal, is predicted to possibly display the +3 oxidation state). The sixth alkali metal is now considered to be francium. While Tl is stabilised by the inert pair effect, this inert pair of 6s electrons is still able to participate chemically, so that these electrons are stereochemically active in aqueous solution. Additionally, the thallium halides (except TlF) are quite insoluble in water, and TlI has an unusual structure because of the presence of the stereochemically active inert pair in thallium.\n\nThe group 11 metals (or coinage metals), copper, silver, and gold, are typically categorised as transition metals given they can form ions with incomplete d-shells. Physically, they have the relatively low melting points and high electronegativity values associated with post-transition metals. \"The filled \"d\" subshell and free \"s\" electron of Cu, Ag, and Au contribute to their high electrical and thermal conductivity. Transition metals to the left of group 11 experience interactions between \"s\" electrons and the partially filled \"d\" subshell that lower electron mobility.\" Chemically, the group 11 metals behave like main-group metals in their +1 valence states, and are hence somewhat related to the alkali metals: this is one reason for their previously being labelled as \"group IB\", paralleling the alkali metals' \"group IA\". They are occasionally classified as post-transition metals. Their spectra are analogous to those of the alkali metals. Their monopositive ions are paramagnetic and contribute no colour to their salts, like those of the alkali metals.\n\nIn Mendeleev's 1871 periodic table, copper, silver, and gold are listed twice, once under group VIII (with the iron triad and platinum group metals), and once under group IB. Group IB was nonetheless parenthesised to note that it was tentative. Mendeleev's main criterion for group assignment was the maximum oxidation state of an element: on that basis, the group 11 elements could not be classified in group IB, due to the existence of copper(II) and gold(III) compounds being known at that time. However, eliminating group IB would make group I the only main group (group VIII was labelled a transition group) to lack an A–B bifurcation. Soon afterward, a majority of chemists chose to classify these elements in group IB and remove them from group VIII for the resulting symmetry: this was the predominant classification until the rise of the modern medium-long 18-column periodic table, which separated the alkali metals and group 11 metals.\n\nThe coinage metals were traditionally regarded as a subdivision of the alkali metal group, due to them sharing the characteristic s electron configuration of the alkali metals (group 1: ps; group 11: ds). However, the similarities are largely confined to the stoichiometries of the +1 compounds of both groups, and not their chemical properties. This stems from the filled d subshell providing a much weaker shielding effect on the outermost s electron than the filled p subshell, so that the coinage metals have much higher first ionisation energies and smaller ionic radii than do the corresponding alkali metals. Furthermore, they have higher melting points, hardnesses, and densities, and lower reactivities and solubilities in liquid ammonia, as well as having more covalent character in their compounds. Finally, the alkali metals are at the top of the electrochemical series, whereas the coinage metals are almost at the very bottom. The coinage metals' filled d shell is much more easily disrupted than the alkali metals' filled p shell, so that the second and third ionisation energies are lower, enabling higher oxidation states than +1 and a richer coordination chemistry, thus giving the group 11 metals clear transition metal character. Particularly noteworthy is gold forming ionic compounds with rubidium and caesium, in which it forms the auride ion (Au) which also occurs in solvated form in liquid ammonia solution: here gold behaves as a pseudohalogen because its 5d6s configuration has one electron less than the quasi-closed shell 5d6s configuration of mercury.\n\nSodium compounds have been known since ancient times; salt (sodium chloride) has been an important commodity in human activities, as testified by the English word \"salary\", referring to \"salarium\", money paid to Roman soldiers for the purchase of salt. While potash has been used since ancient times, it was not understood for most of its history to be a fundamentally different substance from sodium mineral salts. Georg Ernst Stahl obtained experimental evidence which led him to suggest the fundamental difference of sodium and potassium salts in 1702, and Henri Louis Duhamel du Monceau was able to prove this difference in 1736. The exact chemical composition of potassium and sodium compounds, and the status as chemical element of potassium and sodium, was not known then, and thus Antoine Lavoisier did include the alkali in his list of chemical elements in 1789.\n\nPure potassium was first isolated in 1807 in England by Sir Humphry Davy, who derived it from caustic potash (KOH, potassium hydroxide) by the use of electrolysis of the molten salt with the newly invented voltaic pile. Previous attempts at electrolysis of the aqueous salt were unsuccessful due to potassium's extreme reactivity. Potassium was the first metal that was isolated by electrolysis. Later that same year, Davy reported extraction of sodium from the similar substance caustic soda (NaOH, lye) by a similar technique, demonstrating the elements, and thus the salts, to be different. Later that year, the first pieces of pure molten sodium metal were similarly prepared by Humphry Davy through the electrolysis of molten caustic soda (now called sodium hydroxide).\nPetalite (LiAlSiO) was discovered in 1800 by the Brazilian chemist José Bonifácio de Andrada in a mine on the island of Utö, Sweden. However, it was not until 1817 that Johan August Arfwedson, then working in the laboratory of the chemist Jöns Jacob Berzelius, detected the presence of a new element while analysing petalite ore. This new element was noted by him to form compounds similar to those of sodium and potassium, though its carbonate and hydroxide were less soluble in water and more alkaline than the other alkali metals. Berzelius gave the unknown material the name \"\"lithion\"/\"lithina\"\", from the Greek word \"λιθoς\" (transliterated as \"lithos\", meaning \"stone\"), to reflect its discovery in a solid mineral, as opposed to potassium, which had been discovered in plant ashes, and sodium, which was known partly for its high abundance in animal blood. He named the metal inside the material \"\"lithium\"\". Lithium, sodium, and potassium were part of the discovery of periodicity, as they are among a series of triads of elements in the same group that were noted by Johann Wolfgang Döbereiner in 1850 as having similar properties.\nRubidium and caesium were the first elements to be discovered using the spectroscope, invented in 1859 by Robert Bunsen and Gustav Kirchhoff. The next year, they discovered caesium in the mineral water from Bad Dürkheim, Germany. Their discovery of rubidium came the following year in Heidelberg, Germany, finding it in the mineral lepidolite. The names of rubidium and caesium come from the most prominent lines in their emission spectra: a bright red line for rubidium (from the Latin word \"rubidus\", meaning dark red or bright red), and a sky-blue line for caesium (derived from the Latin word \"caesius\", meaning sky-blue).\n\nAround 1865 John Newlands produced a series of papers where he listed the elements in order of increasing atomic weight and similar physical and chemical properties that recurred at intervals of eight; he likened such periodicity to the octaves of music, where notes an octave apart have similar musical functions. His version put all the alkali metals then known (lithium to caesium), as well as copper, silver, and thallium (which show the +1 oxidation state characteristic of the alkali metals), together into a group. His table placed hydrogen with the halogens.\nAfter 1869, Dmitri Mendeleev proposed his periodic table placing lithium at the top of a group with sodium, potassium, rubidium, caesium, and thallium. Two years later, Mendeleev revised his table, placing hydrogen in group 1 above lithium, and also moving thallium to the boron group. In this 1871 version, copper, silver, and gold were placed twice, once as part of group IB, and once as part of a \"group VIII\" encompassing today's groups 8 to 11. After the introduction of the 18-column table, the group IB elements were moved to their current position in the d-block, while alkali metals were left in \"group IA\". Later the group's name was changed to \"group 1\" in 1988. The trivial name \"alkali metals\" comes from the fact that the hydroxides of the group 1 elements are all strong alkalis when dissolved in water.\n\nThere were at least four erroneous and incomplete discoveries before Marguerite Perey of the Curie Institute in Paris, France discovered francium in 1939 by purifying a sample of actinium-227, which had been reported to have a decay energy of 220 keV. However, Perey noticed decay particles with an energy level below 80 keV. Perey thought this decay activity might have been caused by a previously unidentified decay product, one that was separated during purification, but emerged again out of the pure actinium-227. Various tests eliminated the possibility of the unknown element being thorium, radium, lead, bismuth, or thallium. The new product exhibited chemical properties of an alkali metal (such as coprecipitating with caesium salts), which led Perey to believe that it was element 87, caused by the alpha decay of actinium-227. Perey then attempted to determine the proportion of beta decay to alpha decay in actinium-227. Her first test put the alpha branching at 0.6%, a figure that she later revised to 1%.\n\nThe next element below francium (eka-francium) in the periodic table would be ununennium (Uue), element 119. The synthesis of ununennium was first attempted in 1985 by bombarding a target of einsteinium-254 with calcium-48 ions at the superHILAC accelerator at Berkeley, California. No atoms were identified, leading to a limiting yield of 300 nb.\n\nIt is highly unlikely that this reaction will be able to create any atoms of ununennium in the near future, given the extremely difficult task of making sufficient amounts of einsteinium-254, which is favoured for production of ultraheavy elements because of its large mass, relatively long half-life of 270 days, and availability in significant amounts of several micrograms, to make a large enough target to increase the sensitivity of the experiment to the required level; einsteinium has not been found in nature and has only been produced in laboratories, and in quantities smaller than those needed for effective synthesis of ultraheavy elements. However, given that ununennium is only the first period 8 element on the extended periodic table, it may well be discovered in the near future through other reactions, and indeed attempts to synthesise it in 2019 and 2020 are currently planned at laboratories at Japan and Russia. Currently, none of the period 8 elements have been discovered yet, and it is also possible, due to drip instabilities, that only the lower period 8 elements, up to around element 128, are physically possible. No attempts at synthesis have been made for any heavier alkali metals, such as unhexpentium, due to their extremely high atomic number: they would require new, more powerful technology to make.\n\nThe Oddo–Harkins rule holds that elements with even atomic numbers are more common that those with odd atomic numbers, with the exception of hydrogen. This rule argues that elements with odd atomic numbers have one unpaired proton and are more likely to capture another, thus increasing their atomic number. In elements with even atomic numbers, protons are paired, with each member of the pair offsetting the spin of the other, enhancing stability. All the alkali metals have odd atomic numbers and they are not as common as the elements with even atomic numbers adjacent to them (the noble gases and the alkaline earth metals) in the Solar System. The heavier alkali metals are also less abundant than the lighter ones as the alkali metals from rubidium onward can only be synthesised in supernovae and not in stellar nucleosynthesis. Lithium is also much less abundant than sodium and potassium as it is poorly synthesised in both Big Bang nucleosynthesis and in stars: the Big Bang could only produce trace quantities of lithium, beryllium and boron due to the absence of a stable nucleus with 5 or 8 nucleons, and stellar nucleosynthesis could only pass this bottleneck by the triple-alpha process, fusing three helium nuclei to form carbon, and skipping over those three elements.\n\nThe Earth formed from the same cloud of matter that formed the Sun, but the planets acquired different compositions during the formation and evolution of the solar system. In turn, the natural history of the Earth caused parts of this planet to have differing concentrations of the elements. The mass of the Earth is approximately 5.98 kg. It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%); with the remaining 1.2% consisting of trace amounts of other elements. Due to planetary differentiation, the core region is believed to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.\n\nThe alkali metals, due to their high reactivity, do not occur naturally in pure form in nature. They are lithophiles and therefore remain close to the Earth's surface because they combine readily with oxygen and so associate strongly with silica, forming relatively low-density minerals that do not sink down into the Earth's core. Potassium, rubidium and caesium are also incompatible elements due to their large ionic radii.\n\nSodium and potassium are very abundant in earth, both being among the ten most common elements in Earth's crust; sodium makes up approximately 2.6% of the Earth's crust measured by weight, making it the sixth most abundant element overall and the most abundant alkali metal. Potassium makes up approximately 1.5% of the Earth's crust and is the seventh most abundant element. Sodium is found in many different minerals, of which the most common is ordinary salt (sodium chloride), which occurs in vast quantities dissolved in seawater. Other solid deposits include halite, amphibole, cryolite, nitratine, and zeolite. Many of these solid deposits occur as a result of ancient seas evaporating, which still occurs now in places such as Utah's Great Salt Lake and the Dead Sea. Despite their near-equal abundance in Earth's crust, sodium is far more common than potassium in the ocean, both because potassium's larger size makes its salts less soluble, and because potassium is bound by silicates in soil and what potassium leaches is absorbed far more readily by plant life than sodium.\n\nDespite its chemical similarity, lithium typically does not occur together with sodium or potassium due to its smaller size. Due to its relatively low reactivity, it can be found in seawater in large amounts; it is estimated that seawater is approximately 0.14 to 0.25 parts per million (ppm) or 25 micromolar. Its diagonal relationship with magnesium often allows it to replace magnesium in ferromagnesium minerals, where its crustal concentration is about 18 ppm, comparable to that of gallium and niobium. Commercially, the most important lithium mineral is spodumene, which occurs in large deposits worldwide.\n\nRubidium is approximately as abundant as zinc and more abundant than copper. It occurs naturally in the minerals leucite, pollucite, carnallite, zinnwaldite, and lepidolite, although none of these contain only rubidium and no other alkali metals. Caesium is more abundant than some commonly known elements, such as antimony, cadmium, tin, and tungsten, but is much less abundant than rubidium.\n\nFrancium-223, the only naturally occurring isotope of francium, is the product of the alpha decay of actinium-227 and can be found in trace amounts in uranium minerals. In a given sample of uranium, there is estimated to be only one francium atom for every 10 uranium atoms. It has been calculated that there is at most 30 g of francium in the earth's crust at any time, due to its extremely short half-life of 22 minutes.\n\nThe production of pure alkali metals is somewhat complicated due to their extreme reactivity with commonly used substances, such as water. From their silicate ores, all the stable alkali metals may be obtained the same way: sulfuric acid is first used to dissolve the desired alkali metal ion and aluminium(III) ions from the ore (leaching), whereupon basic precipitation removes aluminium ions from the mixture by precipitating it as the hydroxide. The remaining insoluble alkali metal carbonate is then precipitated selectively; the salt is then dissolved in hydrochloric acid to produce the chloride. The result is then left to evaporate and the alkali metal can then be isolated. Lithium and sodium are typically isolated through electrolysis from their liquid chlorides, with calcium chloride typically added to lower the melting point of the mixture. The heavier alkali metals, however, is more typically isolated in a different way, where a reducing agent (typically sodium for potassium and magnesium or calcium for the heaviest alkali metals) is used to reduce the alkali metal chloride. The liquid or gaseous product (the alkali metal) then undergoes fractional distillation for purification.\n\nLithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits. The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride.\n\nSodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 °C through the use of a Downs cell. Extremely pure sodium can be produced through the thermal decomposition of sodium azide. Potassium occurs in many minerals, such as sylvite (potassium chloride). Previously, potassium was generally made from the electrolysis of potassium chloride or potassium hydroxide, found extensively in places such as Canada, Russia, Belarus, Germany, Israel, United States, and Jordan, in a method similar to how sodium was produced in the late 1800s and early 1900s. It can also be produced from seawater. However, these methods are problematic because the potassium metal tends to dissolve in its molten chloride and vaporises significantly at the operating temperatures, potentially forming the explosive superoxide. As a result, pure potassium metal is now produced by reducing molten potassium chloride with sodium metal at 850 °C.\nAlthough sodium is less reactive than potassium, this process works because at such high temperatures potassium is more volatile than sodium and can easily be distilled off, so that the equilibrium shifts towards the right to produce more potassium gas and proceeds almost to completion.\nFor several years in the 1950s and 1960s, a by-product of the potassium production called Alkarb was a main source for rubidium. Alkarb contained 21% rubidium while the rest was potassium and a small fraction of caesium. Today the largest producers of caesium, for example the Tanco Mine in Manitoba, Canada, produce rubidium as by-product from pollucite. Today, a common method for separating rubidium from potassium and caesium is the fractional crystallisation of a rubidium and caesium alum (Cs,Rb)Al(SO)·12HO, which yields pure rubidium alum after approximately 30 recrystallisations. The limited applications and the lack of a mineral rich in rubidium limit the production of rubidium compounds to 2 to 4 tonnes per year. Caesium, however, is not produced from the above reaction. Instead, the mining of pollucite ore is the main method of obtaining pure caesium, extracted from the ore mainly by three methods: acid digestion, alkaline decomposition, and direct reduction. Both metals are produced as by-products of lithium production: after 1958, when interest in lithium's thermonuclear properties increased sharply, the production of rubidium and caesium also increased correspondingly. Pure rubidium and caesium metals are produced by reducing their chlorides with calcium metal at 750 °C and low pressure.\n\nAs a result of its extreme rarity in nature, most francium is synthesised in the nuclear reaction Au + O → Fr + 5 n, yielding francium-209, francium-210, and francium-211. The greatest quantity of francium ever assembled to date is about 300,000 neutral atoms, which were synthesised using the nuclear reaction given above. When the only natural isotope francium-223 is specifically required, it is produced as the alpha daughter of actinium-227, itself produced synthetically from the neutron irradiation of natural radium-226, one of the daughters of natural uranium-238.\n\nLithium, sodium, and potassium have many applications, while rubidium and caesium are very useful in academic contexts but do not have many applications yet. Lithium is often used in batteries, and lithium oxide can help process silica. Lithium stearate is a thickener and can be used to make lubricating greases; it is produced from lithium hydroxide, which is also used to absorb carbon dioxide in space capsules and submarines. Lithium chloride is used as a brazing alloy for aluminium parts. Metallic lithium is used in alloys with magnesium and aluminium to give very tough and light alloys.\n\nSodium compounds have many applications, the most well-known being sodium chloride as table salt. Sodium salts of fatty acids are used as soap. Pure sodium metal also has many applications, including use in sodium-vapour lamps, which produce very efficient light compared to other types of lighting, and can help smooth the surface of other metals. Being a strong reducing agent, it is often used to reduce many other metals, such as titanium and zirconium, from their chlorides. Furthermore, it is very useful as a heat-exchange liquid in fast breeder nuclear reactors due to its low melting point, viscosity, and cross-section towards neutron absorption.\n\nPotassium compounds are often used as fertilisers as potassium is an important element for plant nutrition. Potassium hydroxide is a very strong base, and is used to control the pH of various substances. Potassium nitrate and potassium permanganate are often used as powerful oxidising agents. Potassium superoxide is used in breathing masks, as it reacts with carbon dioxide to give potassium carbonate and oxygen gas. Pure potassium metal is not often used, but its alloys with sodium may substitute for pure sodium in fast breeder nuclear reactors.\n\nRubidium and caesium are often used in atomic clocks. Caesium atomic clocks are extraordinarily accurate; if a clock had been made at the time of the dinosaurs, it would be off by less than four seconds (after 80 million years). For that reason, caesium atoms are used as the definition of the second. Rubidium ions are often used in purple fireworks, and caesium is often used in drilling fluids in the petroleum industry.\n\nFrancium has no commercial applications, but because of francium's relatively simple atomic structure, among other things, it has been used in spectroscopy experiments, leading to more information regarding energy levels and the coupling constants between subatomic particles. Studies on the light emitted by laser-trapped francium-210 ions have provided accurate data on transitions between atomic energy levels, similar to those predicted by quantum theory.\n\nPure alkali metals are dangerously reactive with air and water and must be kept away from heat, fire, oxidising agents, acids, most organic compounds, halocarbons, plastics, and moisture. They also react with carbon dioxide and carbon tetrachloride, so that normal fire extinguishers are counterproductive when used on alkali metal fires. Some Class D dry powder extinguishers designed for metal fires are effective, depriving the fire of oxygen and cooling the alkali metal.\n\nExperiments are usually conducted using only small quantities of a few grams in a fume hood. Small quantities of lithium may be disposed of by reaction with cool water, but the heavier alkali metals should be dissolved in the less reactive isopropanol. The alkali metals must be stored under mineral oil or an inert atmosphere. The inert atmosphere used may be argon or nitrogen gas, except for lithium, which reacts with nitrogen. Rubidium and caesium must be kept away from air, even under oil, because even a small amount of air diffused into the oil may trigger formation of the dangerously explosive peroxide; for the same reason, potassium should not be stored under oil in an oxygen-containing atmosphere for longer than 6 months.\n\nThe bioinorganic chemistry of the alkali metal ions has been extensively reviewed.\nSolid state crystal structures have been determined for many complexes of alkali metal ions in small peptides, nucleic acid constituents, carbohydrates and ionophore complexes.\n\nLithium naturally only occurs in traces in biological systems and has no known biological role, but does have effects on the body when ingested. Lithium carbonate is used as a mood stabiliser in psychiatry to treat bipolar disorder (manic-depression) in daily doses of about 0.5 to 2 grams, although there are side-effects. Excessive ingestion of lithium causes drowsiness, slurred speech and vomiting, among other symptoms, and poisons the central nervous system, which is dangerous as the required dosage of lithium to treat bipolar disorder is only slightly lower than the toxic dosage. Its biochemistry, the way it is handled by the human body and studies using rats and goats suggest that it is an essential trace element, although the natural biological function of lithium in humans has yet to be identified.\n\nSodium and potassium occur in all known biological systems, generally functioning as electrolytes inside and outside cells. Sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride (also known as common salt) is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods. The Dietary Reference Intake for sodium is 1.5 grams per day, but most people in the United States consume more than 2.3 grams per day, the minimum amount that promotes hypertension; this in turn causes 7.6 million premature deaths worldwide.\n\nPotassium is the major cation (positive ion) inside animal cells, while sodium is the major cation outside animal cells. The concentration differences of these charged particles causes a difference in electric potential between the inside and outside of cells, known as the membrane potential. The balance between potassium and sodium is maintained by ion transporter proteins in the cell membrane. The cell membrane potential created by potassium and sodium ions allows the cell to generate an action potential—a \"spike\" of electrical discharge. The ability of cells to produce electrical discharge is critical for body functions such as neurotransmission, muscle contraction, and heart function. Disruption of this balance may thus be fatal: for example, ingestion of large amounts of potassium compounds can lead to hyperkalemia strongly influencing the cardiovascular system. Potassium chloride is used in the United States for lethal injection executions.\nDue to their similar atomic radii, rubidium and caesium in the body mimic potassium and are taken up similarly. Rubidium has no known biological role, but may help stimulate metabolism, and, similarly to caesium, replace potassium in the body causing potassium deficiency. Partial substitution is quite possible and rather non-toxic: a 70 kg person contains on average 0.36 g of rubidium, and an increase in this value by 50 to 100 times did not show negative effects in test persons. Rats can survive up to 50% substitution of potassium by rubidium. Rubidium (and to a much lesser extent caesium) can function as temporary cures for hypokalemia; while rubidium can adequately physiologically substitute potassium in some systems, caesium is never able to do so. There is only very limited evidence in the form of deficiency symptoms for rubidium being possibly essential in goats; even if this is true, the trace amounts usually present in food are more than enough.\n\nCaesium compounds are rarely encountered by most people, but most caesium compounds are mildly toxic. Like rubidium, caesium tends to substitute potassium in the body, but is significantly larger and is therefore a poorer substitute. Excess caesium can lead to hypokalemia, arrythmia, and acute cardiac arrest, but such amounts would not ordinarily be encountered in natural sources. As such, caesium is not a major chemical environmental pollutant. The median lethal dose (LD) value for caesium chloride in mice is 2.3 g per kilogram, which is comparable to the LD values of potassium chloride and sodium chloride. Caesium chloride has been promoted as an alternative cancer therapy, but has been linked to the deaths of over 50 patients, on whom it was used as part of a scientifically unvalidated cancer treatment.\n\nRadioisotopes of caesium require special precautions: the improper handling of caesium-137 gamma ray sources can lead to release of this radioisotope and radiation injuries. Perhaps the best-known case is the Goiânia accident of 1987, in which an improperly-disposed-of radiation therapy system from an abandoned clinic in the city of Goiânia, Brazil, was scavenged from a junkyard, and the glowing caesium salt sold to curious, uneducated buyers. This led to four deaths and serious injuries from radiation exposure. Together with caesium-134, iodine-131, and strontium-90, caesium-137 was among the isotopes distributed by the Chernobyl disaster which constitute the greatest risk to health. Radioisotopes of francium would presumably be dangerous as well due to their high decay energy and short half-life, but none have been produced in large enough amounts to pose any serious risk.\n"}
{"id": "670", "url": "https://en.wikipedia.org/wiki?curid=670", "title": "Alphabet", "text": "Alphabet\n\nAn alphabet is a standard set of letters (basic written symbols or graphemes) that is used to write one or more languages based upon the general principle that the letters represent phonemes (basic significant sounds) of the spoken language. This is in contrast to other types of writing systems, such as syllabaries (in which each character represents a syllable) and logographies (in which each character represents a word, morpheme, or semantic unit).\n\nThe Proto-Canaanite script, later known as the Phoenician alphabet, is the first fully phonemic script. Thus the Phoenician alphabet is considered to be the first alphabet. The Phoenician alphabet is the ancestor of most modern alphabets, including Arabic, Greek, Latin, Cyrillic, Hebrew, and possibly Brahmic. Under a terminological distinction promoted by Peter T. Daniels, an \"alphabet\" is a script that represents both vowels and consonants as letters equally. In this narrow sense of the word the first \"true\" alphabet was the Greek alphabet, which was developed on the basis of the earlier Phoenician alphabet. In other alphabetic scripts such as the original Phoenician, Hebrew or Arabic, letters predominantly or exclusively represent consonants; such a script is also called an abjad. A third type, called abugida or alphasyllabary, is one where vowels are shown by diacritics or modifications of consonantal base letters, as in Devanagari and other South Asian scripts. The Khmer alphabet (for Cambodian) is the longest, with 74 letters.\n\nThere are dozens of alphabets in use today, the most popular being the Latin alphabet (which was derived from the Greek). Many languages use modified forms of the Latin alphabet, with additional letters formed using diacritical marks. While most alphabets have letters composed of lines (linear writing), there are also exceptions such as the alphabets used in Braille.\n\nAlphabets are usually associated with a standard ordering of letters. This makes them useful for purposes of collation, specifically by allowing words to be sorted in alphabetical order. It also means that their letters can be used as an alternative method of \"numbering\" ordered items, in such contexts as numbered lists and number placements.\n\nThe English word \"alphabet\" came into Middle English from the Late Latin word \"alphabetum\", which in turn originated in the Greek ἀλφάβητος (\"alphabētos\"). The Greek word was made from the first two letters, \"alpha\" and \"beta\". The names for the Greek letters came from the first two letters of the Phoenician alphabet; \"aleph\", which also meant \"ox\", and \"bet\", which also meant \"house\".\n\nSometimes, like in the alphabet song in English, the term \"ABCs\" is used instead of the word \"alphabet\" (\"Now I know my ABCs\"...). \"Knowing one's ABCs\", in general, can be used as a metaphor for knowing the basics about anything.\n\nThe history of the alphabet started in ancient Egypt. Egyptian writing had a set of some 24 hieroglyphs that are called uniliterals, to represent syllables that begin with a single consonant of their language, plus a vowel (or no vowel) to be supplied by the native speaker. These glyphs were used as pronunciation guides for logograms, to write grammatical inflections, and, later, to transcribe loan words and foreign names.\nIn the Middle Bronze Age, an apparently \"alphabetic\" system known as the Proto-Sinaitic script appears in Egyptian turquoise mines in the Sinai peninsula dated to circa the 15th century BC, apparently left by Canaanite workers. In 1999, John and Deborah Darnell discovered an even earlier version of this first alphabet at Wadi el-Hol dated to circa 1800 BC and showing evidence of having been adapted from specific forms of Egyptian hieroglyphs that could be dated to circa 2000 BC, strongly suggesting that the first alphabet had been developed about that time. Based on letter appearances and names, it is believed to be based on Egyptian hieroglyphs. This script had no characters representing vowels, although originally it probably was a syllabary, but unneeded symbols were discarded. An alphabetic cuneiform script with 30 signs including three that indicate the following vowel was invented in Ugarit before the 15th century BC. This script was not used after the destruction of Ugarit.\n\nThe Proto-Sinaitic script eventually developed into the Phoenician alphabet, which is conventionally called \"Proto-Canaanite\" before ca. 1050 BC. The oldest text in Phoenician script is an inscription on the sarcophagus of King Ahiram. This script is the parent script of all western alphabets. By the tenth century, two other forms can be distinguished, namely Canaanite and Aramaic. The Aramaic gave rise to the Hebrew script. The South Arabian alphabet, a sister script to the Phoenician alphabet, is the script from which the Ge'ez alphabet (an abugida) is descended. Vowelless alphabets, which are not true alphabets, are called abjads, currently exemplified in scripts including Arabic, Hebrew, and Syriac. The omission of vowels was not always a satisfactory solution and some \"weak\" consonants are sometimes used to indicate the vowel quality of a syllable (matres lectionis). These letters have a dual function since they are also used as pure consonants.\n\nThe Proto-Sinaitic or Proto-Canaanite script and the Ugaritic script were the first scripts with a limited number of signs, in contrast to the other widely used writing systems at the time, Cuneiform, Egyptian hieroglyphs, and Linear B. The Phoenician script was probably the first phonemic script and it contained only about two dozen distinct letters, making it a script simple enough for common traders to learn. Another advantage of Phoenician was that it could be used to write down many different languages, since it recorded words phonemically.\n\nThe script was spread by the Phoenicians across the Mediterranean. In Greece, the script was modified to add the vowels, giving rise to the ancestor of all alphabets in the West. The vowels have independent letter forms separate from the consonants, therefore it was the first true alphabet. The Greeks chose letters representing sounds that did not exist in Greek to represent the vowels. The vowels are significant in the Greek language, and the syllabical Linear B script that was used by the Mycenaean Greeks from the 16th century BC had 87 symbols including 5 vowels. In its early years, there were many variants of the Greek alphabet, a situation that caused many different alphabets to evolve from it.\n\nThe Greek alphabet, in its Euboean form, was carried over by Greek colonists to the Italian peninsula, where it gave rise to a variety of alphabets used to write the Italic languages. One of these became the Latin alphabet, which was spread across Europe as the Romans expanded their empire. Even after the fall of the Roman state, the alphabet survived in intellectual and religious works. It eventually became used for the descendant languages of Latin (the Romance languages) and then for most of the other languages of Europe.\n\nSome adaptations of the Latin alphabet are augmented with ligatures, such as æ in Danish and Icelandic and Ȣ in Algonquian; by borrowings from other alphabets, such as the thorn þ in Old English and Icelandic, which came from the Futhark runes; and by modifying existing letters, such as the eth ð of Old English and Icelandic, which is a modified \"d\". Other alphabets only use a subset of the Latin alphabet, such as Hawaiian, and Italian, which uses the letters \"j, k, x, y\" and \"w\" only in foreign words.\n\nAnother notable script is Elder Futhark, which is believed to have evolved out of one of the Old Italic alphabets. Elder Futhark gave rise to a variety of alphabets known collectively as the Runic alphabets. The Runic alphabets were used for Germanic languages from AD 100 to the late Middle Ages. Its usage is mostly restricted to engravings on stone and jewelry, although inscriptions have also been found on bone and wood. These alphabets have since been replaced with the Latin alphabet, except for decorative usage for which the runes remained in use until the 20th century.\n\nThe Old Hungarian script is a contemporary writing system of the Hungarians. It was in use during the entire history of Hungary, albeit not as an official writing system. From the 19th century it once again became more and more popular.\n\nThe Glagolitic alphabet was the initial script of the liturgical language Old Church Slavonic and became, together with the Greek uncial script, the basis of the Cyrillic script. Cyrillic is one of the most widely used modern alphabetic scripts, and is notable for its use in Slavic languages and also for other languages within the former Soviet Union. Cyrillic alphabets include the Serbian, Macedonian, Bulgarian, Russian, Belarusian and Ukrainian. The Glagolitic alphabet is believed to have been created by Saints Cyril and Methodius, while the Cyrillic alphabet was invented by Clement of Ohrid, who was their disciple. They feature many letters that appear to have been borrowed from or influenced by the Greek alphabet and the Hebrew alphabet.\n\nThe longest European alphabet is the Slovak alphabet which has 46 letters.\n\nBeyond the logographic Chinese writing, many phonetic scripts are in existence in Asia. The Arabic alphabet, Hebrew alphabet, Syriac alphabet, and other abjads of the Middle East are developments of the Aramaic alphabet, but because these writing systems are largely consonant-based they are often not considered true alphabets.\n\nMost alphabetic scripts of India and Eastern Asia are descended from the Brahmi script, which is often believed to be a descendant of Aramaic.\n\nIn Korea, the Hangul alphabet was created by Sejong the Great. Hangul is a unique alphabet: it is a featural alphabet, where many of the letters are designed from a sound's place of articulation (P to look like the widened mouth, L to look like the tongue pulled in, etc.); its design was planned by the government of the day; and it places individual letters in syllable clusters with equal dimensions, in the same way as Chinese characters, to allow for mixed-script writing (one syllable always takes up one type-space no matter how many letters get stacked into building that one sound-block).\n\nZhuyin (sometimes called \"Bopomofo\") is a semi-syllabary used to phonetically transcribe Mandarin Chinese in the Republic of China. After the later establishment of the People's Republic of China and its adoption of Hanyu Pinyin, the use of Zhuyin today is limited, but it is still widely used in Taiwan where the Republic of China still governs. Zhuyin developed out of a form of Chinese shorthand based on Chinese characters in the early 1900s and has elements of both an alphabet and a syllabary. Like an alphabet the phonemes of syllable initials are represented by individual symbols, but like a syllabary the phonemes of the syllable finals are not; rather, each possible final (excluding the medial glide) is represented by its own symbol. For example, \"luan\" is represented as ㄌㄨㄢ (\"l-u-an\"), where the last symbol ㄢ represents the entire final \"-an\". While Zhuyin is not used as a mainstream writing system, it is still often used in ways similar to a romanization system—that is, for aiding in pronunciation and as an input method for Chinese characters on computers and cellphones.\n\nEuropean alphabets, especially Latin and Cyrillic, have been adapted for many languages of Asia. Arabic is also widely used, sometimes as an abjad (as with Urdu and Persian) and sometimes as a complete alphabet (as with Kurdish and Uyghur).\n\nThe term \"alphabet\" is used by linguists and paleographers in both a wide and a narrow sense. In the wider sense, an alphabet is a script that is \"segmental\" at the phoneme level—that is, it has separate glyphs for individual sounds and not for larger units such as syllables or words. In the narrower sense, some scholars distinguish \"true\" alphabets from two other types of segmental script, abjads and abugidas. These three differ from each other in the way they treat vowels: abjads have letters for consonants and leave most vowels unexpressed; abugidas are also consonant-based, but indicate vowels with diacritics to or a systematic graphic modification of the consonants. In alphabets in the narrow sense, on the other hand, consonants and vowels are written as independent letters. The earliest known alphabet in the wider sense is the Wadi el-Hol script, believed to be an abjad, which through its successor Phoenician is the ancestor of modern alphabets, including Arabic, Greek, Latin (via the Old Italic alphabet), Cyrillic (via the Greek alphabet) and Hebrew (via Aramaic).\n\nExamples of present-day abjads are the Arabic and Hebrew scripts; true alphabets include Latin, Cyrillic, and Korean hangul; and abugidas are used to write Tigrinya, Amharic, Hindi, and Thai. The Canadian Aboriginal syllabics are also an abugida rather than a syllabary as their name would imply, since each glyph stands for a consonant that is modified by rotation to represent the following vowel. (In a true syllabary, each consonant-vowel combination would be represented by a separate glyph.)\n\nAll three types may be augmented with syllabic glyphs. Ugaritic, for example, is basically an abjad, but has syllabic letters for . (These are the only time vowels are indicated.) Cyrillic is basically a true alphabet, but has syllabic letters for (я, е, ю); Coptic has a letter for . Devanagari is typically an abugida augmented with dedicated letters for initial vowels, though some traditions use अ as a zero consonant as the graphic base for such vowels.\n\nThe boundaries between the three types of segmental scripts are not always clear-cut. For example, Sorani Kurdish is written in the Arabic script, which is normally an abjad. However, in Kurdish, writing the vowels is mandatory, and full letters are used, so the script is a true alphabet. Other languages may use a Semitic abjad with mandatory vowel diacritics, effectively making them abugidas. On the other hand, the Phagspa script of the Mongol Empire was based closely on the Tibetan abugida, but all vowel marks were written after the preceding consonant rather than as diacritic marks. Although short \"a\" was not written, as in the Indic abugidas, one could argue that the linear arrangement made this a true alphabet. Conversely, the vowel marks of the Tigrinya abugida and the Amharic abugida (ironically, the original source of the term \"abugida\") have been so completely assimilated into their consonants that the modifications are no longer systematic and have to be learned as a syllabary rather than as a segmental script. Even more extreme, the Pahlavi abjad eventually became logographic. (See below.)\n\nThus the primary classification of alphabets reflects how they treat vowels. For tonal languages, further classification can be based on their treatment of tone, though names do not yet exist to distinguish the various types. Some alphabets disregard tone entirely, especially when it does not carry a heavy functional load, as in Somali and many other languages of Africa and the Americas. Such scripts are to tone what abjads are to vowels. Most commonly, tones are indicated with diacritics, the way vowels are treated in abugidas. This is the case for Vietnamese (a true alphabet) and Thai (an abugida). In Thai, tone is determined primarily by the choice of consonant, with diacritics for disambiguation. In the Pollard script, an abugida, vowels are indicated by diacritics, but the placement of the diacritic relative to the consonant is modified to indicate the tone. More rarely, a script may have separate letters for tones, as is the case for Hmong and Zhuang. For most of these scripts, regardless of whether letters or diacritics are used, the most common tone is not marked, just as the most common vowel is not marked in Indic abugidas; in Zhuyin not only is one of the tones unmarked, but there is a diacritic to indicate lack of tone, like the virama of Indic.\n\nThe number of letters in an alphabet can be quite small. The Book Pahlavi script, an abjad, had only twelve letters at one point, and may have had even fewer later on. Today the Rotokas alphabet has only twelve letters. (The Hawaiian alphabet is sometimes claimed to be as small, but it actually consists of 18 letters, including the ʻokina and five long vowels. However, Hawaiian Braille has only 13 letters.) While Rotokas has a small alphabet because it has few phonemes to represent (just eleven), Book Pahlavi was small because many letters had been \"conflated\"—that is, the graphic distinctions had been lost over time, and diacritics were not developed to compensate for this as they were in Arabic, another script that lost many of its distinct letter shapes. For example, a comma-shaped letter represented \"g, d, y, k,\" or \"j\". However, such apparent simplifications can perversely make a script more complicated. In later Pahlavi papyri, up to half of the remaining graphic distinctions of these twelve letters were lost, and the script could no longer be read as a sequence of letters at all, but instead each word had to be learned as a whole—that is, they had become logograms as in Egyptian Demotic.\n\nThe largest segmental script is probably an abugida, Devanagari. When written in Devanagari, Vedic Sanskrit has an alphabet of 53 letters, including the \"visarga\" mark for final aspiration and special letters for \"kš\" and \"jñ,\" though one of the letters is theoretical and not actually used. The Hindi alphabet must represent both Sanskrit and modern vocabulary, and so has been expanded to 58 with the \"khutma\" letters (letters with a dot added) to represent sounds from Persian and English. Thai has a total of 59 symbols, consisting of 44 consonants, 13 vowels and 2 syllabics, not including 4 diacritics for tone marks and one for vowel length.\n\nThe largest known abjad is Sindhi, with 51 letters. The largest alphabets in the narrow sense include Kabardian and Abkhaz (for Cyrillic), with 58 and 56 letters, respectively, and Slovak (for the Latin script), with 46. However, these scripts either count di- and tri-graphs as separate letters, as Spanish did with \"ch\" and \"ll\" until recently, or uses diacritics like Slovak \"č\".\n\nThe Georgian alphabet ( \"\") is alphabetical writing system. It is the largest true alphabet where each letter is graphically independent with 33 letters. Original Georgian alphabet had 38 letters but 5 letters were removed in 19th century by Ilia Chavchavadze. The Georgian Alphabet is much closer to Greek than the other Caucasian alphabets. The numeric value runs parallel to the Greek one, the consonants without a Greek equivalent are organized at the end of the alphabet. Origins of the Alphabet are still unknown, some Armenian and Western scholars believe it was created by Mesrop Mashtots (Armenian: Մեսրոպ Մաշտոց Mesrop Maštoc')also known as Mesrob the Vartabed,who was an early medieval Armenian linguist, theologian, statesman and hymnologist, best known for inventing the Armenian alphabet c. 405 AD, other Georgian and Western, scholars are against this theory.\n\nSyllabaries typically contain 50 to 400 glyphs, and the glyphs of logographic systems typically number from the many hundreds into the thousands. Thus a simple count of the number of distinct symbols is an important clue to the nature of an unknown script.\n\nThe Armenian alphabet ( ' or ') is a graphically unique alphabetical writing system that has been used to write the Armenian language. It was introduced by Mesrob Mashdots around 405 AD, an Armenian linguist and ecclesiastical leader, and originally contained 36 letters. Two more letters, օ (o) and ֆ (f), were added in the Middle Ages. During the 1920s orthography reform, a new letter և (capital ԵՎ) was added, which was a ligature before ե+ւ, while the letter Ւ ւ was discarded and reintroduced as part of a new letter ՈՒ ու (which was a digraph before).\n\nThe Armenian word for \"alphabet\" is \"\" (), named after the first two letters of the Armenian alphabet Ա այբ ayb and Բ բեն ben. The Armenian script's directionality is horizontal left-to-right, like the Latin and Greek alphabets.\n\nAlphabets often come to be associated with a standard ordering of their letters, which can then be used for purposes of collation—namely for the listing of words and other items in what is called \"alphabetical order\".\n\nThe basic ordering of the Latin alphabet (A\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ), which is derived from the Northwest Semitic \"Abgad\" order, is well established, although languages using this alphabet have different conventions for their treatment of modified letters (such as the French \"é\", \"à\", and \"ô\") and of certain combinations of letters (multigraphs). In French, these are not considered to be additional letters for the purposes of collation. However, in Icelandic, the accented letters such as \"á\", \"í\", and \"ö\" are considered distinct letters representing different vowel sounds from the sounds represented by their unaccented counterparts. In Spanish, \"ñ\" is considered a separate letter, but accented vowels such as \"á\" and \"é\" are not. The \"ll\" and \"ch\" were also considered single letters, but in 1994 the Real Academia Española changed the collating order so that \"ll\" is between \"lk\" and \"lm\" in the dictionary and \"ch\" is between \"cg\" and \"ci\", and in 2010 the tenth congress of the Association of Spanish Language Academies changed it so they were no longer letters at all.\n\nIn German, words starting with \"sch-\" (which spells the German phoneme ) are inserted between words with initial \"sca-\" and \"sci-\" (all incidentally loanwords) instead of appearing after initial \"sz\", as though it were a single letter—in contrast to several languages such as Albanian, in which \"dh-\", \"ë-\", \"gj-\", \"ll-\", \"rr-\", \"th-\", \"xh-\" and \"zh-\" (all representing phonemes and considered separate single letters) would follow the letters \"d\", \"e\", \"g\", \"l\", \"n\", \"r\", \"t\", \"x\" and \"z\" respectively, as well as Hungarian and Welsh. Further, German words with umlaut are collated ignoring the umlaut—contrary to Turkish that adopted the graphemes ö and ü, and where a word like \"tüfek\", would come after \"tuz\", in the dictionary. An exception is the German telephone directory where umlauts are sorted like \"ä\" = \"ae\" since names as \"Jäger\" appear also with the spelling \"Jaeger\", and are not distinguished in the spoken language.\n\nThe Danish and Norwegian alphabets end with \"æ\"—\"ø\"—\"å\", whereas the Swedish and Finnish ones conventionally put \"å\"—\"ä\"—\"ö\" at the end.\n\nIt is unknown whether the earliest alphabets had a defined sequence. Some alphabets today, such as the Hanuno'o script, are learned one letter at a time, in no particular order, and are not used for collation where a definite order is required. However, a dozen Ugaritic tablets from the fourteenth century BC preserve the alphabet in two sequences. One, the \"ABCDE\" order later used in Phoenician, has continued with minor changes in Hebrew, Greek, Armenian, Gothic, Cyrillic, and Latin; the other, \"HMĦLQ,\" was used in southern Arabia and is preserved today in Ethiopic. Both orders have therefore been stable for at least 3000 years.\n\nRunic used an unrelated Futhark sequence, which was later simplified. Arabic uses its own sequence, although Arabic retains the traditional abjadi order for numbering.\n\nThe Brahmic family of alphabets used in India use a unique order based on phonology: The letters are arranged according to how and where they are produced in the mouth. This organization is used in Southeast Asia, Tibet, Korean hangul, and even Japanese kana, which is not an alphabet.\n\nThe Phoenician letter names, in which each letter was associated with a word that begins with that sound (acrophony), continue to be used to varying degrees in Samaritan, Aramaic, Syriac, Hebrew, Greek and Arabic.\n\nThe names were abandoned in Latin, which instead referred to the letters by adding a vowel (usually e) before or after the consonant; the two exceptions were Y and Z, which were borrowed from the Greek alphabet rather than Etruscan, and were known as \"Y Graeca\" \"Greek Y\" (pronounced \"I Graeca\" \"Greek I\") and \"zeta\" (from Greek)—this discrepancy was inherited by many European languages, as in the term \"zed\" for Z in all forms of English other than American English. Over time names sometimes shifted or were added, as in \"double U\" for W (\"double V\" in French), the English name for Y, and American \"zee\" for Z. Comparing names in English and French gives a clear reflection of the Great Vowel Shift: A, B, C and D are pronounced /eɪ, biː, siː, diː/ in today's English, but in contemporary French they are /a, be, se, de/. The French names (from which the English names are derived) preserve the qualities of the English vowels from before the Great Vowel Shift. By contrast, the names of F, L, M, N and S (/ɛf, ɛl, ɛm, ɛn, ɛs/) remain the same in both languages, because \"short\" vowels were largely unaffected by the Shift.\n\nIn Cyrillic originally the letters were given names based on Slavic words; this was later abandoned as well in favor of a system similar to that used in Latin.\n\nWhen an alphabet is adopted or developed to represent a given language, an orthography generally comes into being, providing rules for the spelling of words in that language. In accordance with the principle on which alphabets are based, these rules will generally map letters of the alphabet to the phonemes (significant sounds) of the spoken language. In a perfectly phonemic orthography there would be a consistent one-to-one correspondence between the letters and the phonemes, so that a writer could predict the spelling of a word given its pronunciation, and a speaker would always know the pronunciation of a word given its spelling, and vice versa. However this ideal is not usually achieved in practice; some languages (such as Spanish and Finnish) come close to it, while others (such as English) deviate from it to a much larger degree.\n\nThe pronunciation of a language often evolves independently of its writing system, and writing systems have been borrowed for languages they were not designed for, so the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.\n\nLanguages may fail to achieve a one-to-one correspondence between letters and sounds in any of several ways:\n\nNational languages sometimes elect to address the problem of dialects by simply associating the alphabet with the national standard. However, with an international language with wide variations in its dialects, such as English, it would be impossible to represent the language in all its variations with a single phonetic alphabet.\n\nSome national languages like Finnish, Turkish, Russian, Serbo-Croatian (Serbian, Croatian and Bosnian) and Bulgarian have a very regular spelling system with a nearly one-to-one correspondence between letters and phonemes. Strictly speaking, these national languages lack a word corresponding to the verb \"to spell\" (meaning to split a word into its letters), the closest match being a verb meaning to split a word into its syllables. Similarly, the Italian verb corresponding to 'spell (out)', \"compitare\", is unknown to many Italians because spelling is usually trivial, as Italian spelling is highly phonemic. In standard Spanish, one can tell the pronunciation of a word from its spelling, but not vice versa, as certain phonemes can be represented in more than one way, but a given letter is consistently pronounced. French, with its silent letters and its heavy use of nasal vowels and elision, may seem to lack much correspondence between spelling and pronunciation, but its rules on pronunciation, though complex, are actually consistent and predictable with a fair degree of accuracy.\n\nAt the other extreme are languages such as English, where the pronunciations of many words simply have to be memorized as they do not correspond to the spelling in a consistent way. For English, this is partly because the Great Vowel Shift occurred after the orthography was established, and because English has acquired a large number of loanwords at different times, retaining their original spelling at varying levels. Even English has general, albeit complex, rules that predict pronunciation from spelling, and these rules are successful most of the time; rules to predict spelling from the pronunciation have a higher failure rate.\n\nSometimes, countries have the written language undergo a spelling reform to realign the writing with the contemporary spoken language. These can range from simple spelling changes and word forms to switching the entire writing system itself, as when Turkey switched from the Arabic alphabet to a Latin-based Turkish alphabet.\n\nThe standard system of symbols used by linguists to represent sounds in any language, independently of orthography, is called the International Phonetic Alphabet.\n\n\n"}
{"id": "673", "url": "https://en.wikipedia.org/wiki?curid=673", "title": "Atomic number", "text": "Atomic number\n\nThe atomic number or proton number (symbol Z) of a chemical element is the number of protons found in the nucleus of an atom. It is identical to the charge number of the nucleus. The atomic number uniquely identifies a chemical element. In an uncharged atom, the atomic number is also equal to the number of electrons.\n\nThe sum of the atomic number \"Z\" and the number of neutrons, \"N\", gives the mass number \"A\" of an atom. Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes) and the mass defect of nucleon binding is always small compared to the nucleon mass, the atomic mass of any atom, when expressed in unified atomic mass units (making a quantity called the \"relative isotopic mass\"), is within 1% of the whole number \"A\".\n\nAtoms with the same atomic number \"Z\" but different neutron numbers \"N\", and hence different atomic masses, are known as isotopes. A little more than three-quarters of naturally occurring elements exist as a mixture of isotopes (see monoisotopic elements), and the average isotopic mass of an isotopic mixture for an element (called the relative atomic mass) in a defined environment on Earth, determines the element's standard atomic weight. Historically, it was these atomic weights of elements (in comparison to hydrogen) that were the quantities measurable by chemists in the 19th century.\n\nThe conventional symbol \"Z\" comes from the German word meaning \"number\", which, prior to the modern synthesis of ideas from chemistry and physics, merely denoted an element's numerical place in the periodic table, whose order is approximately, but not completely, consistent with the order of the elements by atomic weights. Only after 1915, with the suggestion and evidence that this \"Z\" number was also the nuclear charge and a physical characteristic of atoms, did the word (and its English equivalent \"atomic number\") come into common use in this context.\n\nLoosely speaking, the existence or construction of a periodic table of elements creates an ordering of the elements, and so they can be numbered in order.\n\nDmitri Mendeleev claimed that he arranged his first periodic tables in order of atomic weight (\"Atomgewicht\"). However, in consideration of the elements' observed chemical properties, he changed the order slightly and placed tellurium (atomic weight 127.6) ahead of iodine (atomic weight 126.9). This placement is consistent with the modern practice of ordering the elements by proton number, \"Z\", but that number was not known or suspected at the time.\n\nA simple numbering based on periodic table position was never entirely satisfactory, however. Besides the case of iodine and tellurium, later several other pairs of elements (such as argon and potassium, cobalt and nickel) were known to have nearly identical or reversed atomic weights, thus requiring their placement in the periodic table to be determined by their chemical properties. However the gradual identification of more and more chemically similar lanthanide elements, whose atomic number was not obvious, led to inconsistency and uncertainty in the periodic numbering of elements at least from lutetium (element 71) onwards (hafnium was not known at this time).\n\nIn 1911, Ernest Rutherford gave a model of the atom in which a central core held most of the atom's mass and a positive charge which, in units of the electron's charge, was to be approximately equal to half of the atom's atomic weight, expressed in numbers of hydrogen atoms. This central charge would thus be approximately half the atomic weight (though it was almost 25% different from the atomic number of gold , ), the single element from which Rutherford made his guess). Nevertheless, in spite of Rutherford's estimation that gold had a central charge of about 100 (but was element on the periodic table), a month after Rutherford's paper appeared, Antonius van den Broek first formally suggested that the central charge and number of electrons in an atom was \"exactly\" equal to its place in the periodic table (also known as element number, atomic number, and symbolized \"Z\"). This proved eventually to be the case.\n\nThe experimental position improved dramatically after research by Henry Moseley in 1913. Moseley, after discussions with Bohr who was at the same lab (and who had used Van den Broek's hypothesis in his Bohr model of the atom), decided to test Van den Broek's and Bohr's hypothesis directly, by seeing if spectral lines emitted from excited atoms fitted the Bohr theory's postulation that the frequency of the spectral lines be proportional to the square of \"Z\".\n\nTo do this, Moseley measured the wavelengths of the innermost photon transitions (K and L lines) produced by the elements from aluminum (\"Z\" = 13) to gold (\"Z\" = 79) used as a series of movable anodic targets inside an x-ray tube. The square root of the frequency of these photons (x-rays) increased from one target to the next in an arithmetic progression. This led to the conclusion (Moseley's law) that the atomic number does closely correspond (with an offset of one unit for K-lines, in Moseley's work) to the calculated electric charge of the nucleus, i.e. the element number \"Z\". Among other things, Moseley demonstrated that the lanthanide series (from lanthanum to lutetium inclusive) must have 15 members—no fewer and no more—which was far from obvious from the chemistry at that time.\n\nAfter Moseley's death in 1915, the atomic numbers of all known elements from hydrogen to uranium (\"Z\" = 92) were examined by his method. There were seven elements (with \"Z\" < 92) which were not found and therefore identified as still undiscovered, corresponding to atomic numbers 43, 61, 72, 75, 85, 87 and 91. From 1918 to 1947, all seven of these missing elements were discovered. By this time the first four transuranium elements had also been discovered, so that the periodic table was complete with no gaps as far as curium (\"Z\" = 96).\n\nIn 1915 the reason for nuclear charge being quantized in units of \"Z\", which were now recognized to be the same as the element number, was not understood. An old idea called Prout's hypothesis had postulated that the elements were all made of residues (or \"protyles\") of the lightest element hydrogen, which in the Bohr-Rutherford model had a single electron and a nuclear charge of one. However, as early as 1907 Rutherford and Thomas Royds had shown that alpha particles, which had a charge of +2, were the nuclei of helium atoms, which had a mass four times that of hydrogen, not two times. If Prout's hypothesis were true, something had to be neutralizing some of the charge of the hydrogen nuclei present in the nuclei of heavier atoms.\n\nIn 1917 Rutherford succeeded in generating hydrogen nuclei from a nuclear reaction between alpha particles and nitrogen gas, and believed he had proven Prout's law. He called the new heavy nuclear particles protons in 1920 (alternate names being proutons and protyles). It had been immediately apparent from the work of Moseley that the nuclei of heavy atoms have more than twice as much mass as would be expected from their being made of hydrogen nuclei, and thus there was required a hypothesis for the neutralization of the extra protons presumed present in all heavy nuclei. A helium nucleus was presumed to be composed of four protons plus two \"nuclear electrons\" (electrons bound inside the nucleus) to cancel two of the charges. At the other end of the periodic table, a nucleus of gold with a mass 197 times that of hydrogen, was thought to contain 118 nuclear electrons in the nucleus to give it a residual charge of + 79, consistent with its atomic number.\n\nAll consideration of nuclear electrons ended with James Chadwick's discovery of the neutron in 1932. An atom of gold now was seen as containing 118 neutrons rather than 118 nuclear electrons, and its positive charge now was realized to come entirely from a content of 79 protons. After 1932, therefore, an element's atomic number \"Z\" was also realized to be identical to the proton number of its nuclei.\n\nThe conventional symbol \"Z\" possibly comes from the German word (atomic number). However, prior to 1915, the word \"Zahl\" (simply \"number\") was used for an element's assigned number in the periodic table.\n\nEach element has a specific set of chemical properties as a consequence of the number of electrons present in the neutral atom, which is \"Z\" (the atomic number). The configuration of these electrons follows from the principles of quantum mechanics. The number of electrons in each element's electron shells, particularly the outermost valence shell, is the primary factor in determining its chemical bonding behavior. Hence, it is the atomic number alone that determines the chemical properties of an element; and it is for this reason that an element can be defined as consisting of \"any\" mixture of atoms with a given atomic number.\n\nThe quest for new elements is usually described using atomic numbers. As of 2010, all elements with atomic numbers 1 to 118 have been observed. Synthesis of new elements is accomplished by bombarding target atoms of heavy elements with ions, such that the sum of the atomic numbers of the target and ion elements equals the atomic number of the element being created. In general, the half-life becomes shorter as atomic number increases, though an \"island of stability\" may exist for undiscovered isotopes with certain numbers of protons and neutrons.\n\n"}
{"id": "674", "url": "https://en.wikipedia.org/wiki?curid=674", "title": "Anatomy", "text": "Anatomy\n\nAnatomy is the branch of biology concerned with the study of the structure of organisms and their parts. Anatomy (Greek anatomē, “dissection”) is a branch of natural science dealing with the structural organization of living things. It is an old science, having its beginnings in prehistoric times. Anatomy is inherently tied to embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated over immediate (embryology) and long (evolution) timescales. Human anatomy is one of the basic essential sciences of medicine.\n\nThe discipline of anatomy is divided into macroscopic and microscopic anatomy. Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.\n\nThe history of anatomy is characterized by a progressive understanding of the functions of the organs and structures of the human body. Methods have also improved dramatically, advancing from the examination of animals by dissection of carcasses and cadavers (corpses) to 20th century medical imaging techniques including X-ray, ultrasound, and magnetic resonance imaging.\n\nAnatomy and physiology, which study (respectively) the structure and function of organisms and their parts, make a natural pair of related disciplines, and they are often studied together.\n\nDerived from the Greek \"anatomē\" \"dissection\" (from \"anatémnō\" \"I cut up, cut open\" from ἀνά \"aná\" \"up\", and τέμνω \"témnō\" \"I cut\"), anatomy is the scientific study of the structure of organisms including their systems, organs and tissues. It includes the appearance and position of the various parts, the materials from which they are composed, their locations and their relationships with other parts. Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved. For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.\n\nThe discipline of anatomy can be subdivided into a number of branches including gross or macroscopic anatomy and microscopic anatomy. Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, including histology (the study of tissues), and embryology (the study of an organism in its immature condition).\n\nAnatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures. Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.\n\nThe term \"anatomy\" is commonly taken to refer to human anatomy. However, substantially the same structures and tissues are found throughout the rest of the animal kingdom and the term also includes the anatomy of other animals. The term \"zootomy\" is also sometimes used to specifically refer to animals. The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.\n\nThe kingdom Animalia or metazoa, contains multicellular organisms that are heterotrophic and motile (although some have secondarily adopted a sessile lifestyle). Most animals have bodies differentiated into separate tissues and these animals are also known as eumetazoans. They have an internal digestive chamber, with one or two openings; the gametes are produced in multicellular sex organs, and the zygotes include a blastula stage in their embryonic development. Metazoans do not include the sponges, which have undifferentiated cells.\n\nUnlike plant cells, animal cells have neither a cell wall nor chloroplasts. Vacuoles, when present, are more in number and much smaller than those in the plant cell. The body tissues are composed of numerous types of cell, including those found in muscles, nerves and skin. Each typically has a cell membrane formed of phospholipids, cytoplasm and a nucleus. All of the different cells of an animal are derived from the embryonic germ layers. Those simpler invertebrates which are formed from two germ layers of ectoderm and endoderm are called diploblastic and the more developed animals whose structures and organs are formed from three germ layers are called triploblastic. All of a triploblastic animal's tissues and organs are derived from the three germ layers of the embryo, the ectoderm, mesoderm and endoderm.\n\nAnimal tissues can be grouped into four basic types: connective, epithelial, muscle and nervous tissue.\nConnective tissues are fibrous and made up of cells scattered among inorganic material called the extracellular matrix. Connective tissue gives shape to organs and holds them in place. The main types are loose connective tissue, adipose tissue, fibrous connective tissue, cartilage and bone. The extracellular matrix contains proteins, the chief and most abundant of which is collagen. Collagen plays a major part in organizing and maintaining tissues. The matrix can be modified to form a skeleton to support or protect the body. An exoskeleton is a thickened, rigid cuticle which is stiffened by mineralization, as in crustaceans or by the cross-linking of its proteins as in insects. An endoskeleton is internal and present in all developed animals, as well as in many of those less developed.\n\nEpithelial tissue is composed of closely packed cells, bound to each other by cell adhesion molecules, with little intercellular space. Epithelial cells can be squamous (flat), cuboidal or columnar and rest on a basal lamina, the upper layer of the basement membrane, the lower layer is the reticular lamina lying next to the connective tissue in the extracellular matrix secreted by the epithelial cells. There are many different types of epithelium, modified to suit a particular function. In the respiratory tract there is a type of ciliated epithelial lining; in the small intestine there are microvilli on the epithelial lining and in the large intestine there are intestinal villi. Skin consists of an outer layer of keratinized stratified squamous epithelium that covers the exterior of the vertebrate body. Keratinocytes make up to 95% of the cells in the skin. The epithelial cells on the external surface of the body typically secrete an extracellular matrix in the form of a cuticle. In simple animals this may just be a coat of glycoproteins. In more advanced animals, many glands are formed of epithelial cells.\n\nMuscle cells (myocytes) form the active contractile tissue of the body. Muscle tissue functions to produce force and cause motion, either locomotion or movement within internal organs. Muscle is formed of contractile filaments and is separated into three main types; smooth muscle, skeletal muscle and cardiac muscle. Smooth muscle has no striations when examined microscopically. It contracts slowly but maintains contractibility over a wide range of stretch lengths. It is found in such organs as sea anemone tentacles and the body wall of sea cucumbers. Skeletal muscle contracts rapidly but has a limited range of extension. It is found in the movement of appendages and jaws. Obliquely striated muscle is intermediate between the other two. The filaments are staggered and this is the type of muscle found in earthworms that can extend slowly or make rapid contractions. In higher animals striated muscles occur in bundles attached to bone to provide movement and are often arranged in antagonistic sets. Smooth muscle is found in the walls of the uterus, bladder, intestines, stomach, oesophagus, respiratory airways, and blood vessels. Cardiac muscle is found only in the heart, allowing it to contract and pump blood round the body.\n\nNervous tissue is composed of many nerve cells known as neurons which transmit information. In some slow-moving radially symmetrical marine animals such as ctenophores and cnidarians (including sea anemones and jellyfish), the nerves form a nerve net, but in most animals they are organized longitudinally into bundles. In simple animals, receptor neurons in the body wall cause a local reaction to a stimulus. In more complex animals, specialized receptor cells such as chemoreceptors and photoreceptors are found in groups and send messages along neural networks to other parts of the organism. Neurons can be connected together in ganglia. In higher animals, specialized receptors are the basis of sense organs and there is a central nervous system (brain and spinal cord) and a peripheral nervous system. The latter consists of sensory nerves that transmit information from sense organs and motor nerves that influence target organs. The peripheral nervous system is divided into the somatic nervous system which conveys sensation and controls voluntary muscle, and the autonomic nervous system which involuntarily controls smooth muscle, certain glands and internal organs, including the stomach.\n\nAll vertebrates have a similar basic body plan and at some point in their lives, mostly in the embryonic stage, share the major chordate characteristics; a stiffening rod, the notochord; a dorsal hollow tube of nervous material, the neural tube; pharyngeal arches; and a tail posterior to the anus. The spinal cord is protected by the vertebral column and is above the notochord and the gastrointestinal tract is below it. Nervous tissue is derived from the ectoderm, connective tissues are derived from mesoderm, and gut is derived from the endoderm. At the posterior end is a tail which continues the spinal cord and vertebrae but not the gut. The mouth is found at the anterior end of the animal, and the anus at the base of the tail. The defining characteristic of a vertebrate is the vertebral column, formed in the development of the segmented series of vertebrae. In most vertebrates the notochord becomes the nucleus pulposus of the intervertebral discs. However, a few vertebrates, such as the sturgeon and the coelacanth retain the notochord into adulthood. Jawed vertebrates are typified by paired appendages, fins or legs, which may be secondarily lost. The limbs of vertebrates are considered to be homologous because the same underlying skeletal structure was inherited from their last common ancestor. This is one of the arguments put forward by Charles Darwin to support his theory of evolution.\n\nThe body of a fish is divided into a head, trunk and tail, although the divisions between the three are not always externally visible. The skeleton, which forms the support structure inside the fish, is either made of cartilage, in cartilaginous fish, or bone in bony fish. The main skeletal element is the vertebral column, composed of articulating vertebrae which are lightweight yet strong. The ribs attach to the spine and there are no limbs or limb girdles. The main external features of the fish, the fins, are composed of either bony or soft spines called rays, which with the exception of the caudal fins, have no direct connection with the spine. They are supported by the muscles which compose the main part of the trunk. The heart has two chambers and pumps the blood through the respiratory surfaces of the gills and on round the body in a single circulatory loop. The eyes are adapted for seeing underwater and have only local vision. There is an inner ear but no external or middle ear. Low frequency vibrations are detected by the lateral line system of sense organs that run along the length of the sides of fish, and these respond to nearby movements and to changes in water pressure.\n\nSharks and rays are basal fish with numerous primitive anatomical features similar to those of ancient fish, including skeletons composed of cartilage. Their bodies tend to be dorso-ventrally flattened, they usually have five pairs of gill slits and a large mouth set on the underside of the head. The dermis is covered with separate dermal placoid scales. They have a cloaca into which the urinary and genital passages open, but not a swim bladder. Cartilaginous fish produce a small number of large, yolky eggs. Some species are ovoviviparous and the young develop internally but others are oviparous and the larvae develop externally in egg cases.\n\nThe bony fish lineage shows more derived anatomical traits, often with major evolutionary changes from the features of ancient fish. They have a bony skeleton, are generally laterally flattened, have five pairs of gills protected by an operculum, and a mouth at or near the tip of the snout. The dermis is covered with overlapping scales. Bony fish have a swim bladder which helps them maintain a constant depth in the water column, but not a cloaca. They mostly spawn a large number of small eggs with little yolk which they broadcast into the water column.\n\nAmphibians are a class of animals comprising frogs, salamanders and caecilians. They are tetrapods, but the caecilians and a few species of salamander have either no limbs or their limbs are much reduced in size. Their main bones are hollow and lightweight and are fully ossified and the vertebrae interlock with each other and have articular processes. Their ribs are usually short and may be fused to the vertebrae. Their skulls are mostly broad and short, and are often incompletely ossified. Their skin contains little keratin and lacks scales, but contains many mucous glands and in some species, poison glands. The hearts of amphibians have three chambers, two atria and one ventricle. They have a urinary bladder and nitrogenous waste products are excreted primarily as urea. Amphibians breathe by means of buccal pumping, a pump action in which air is first drawn into the buccopharyngeal region through the nostrils. These are then closed and the air is forced into the lungs by contraction of the throat. They supplement this with gas exchange through the skin which needs to be kept moist.\n\nIn frogs the pelvic girdle is robust and the hind legs are much longer and stronger than the forelimbs. The feet have four or five digits and the toes are often webbed for swimming or have suction pads for climbing. Frogs have large eyes and no tail. Salamanders resemble lizards in appearance; their short legs project sideways, the belly is close to or in contact with the ground and they have a long tail. Caecilians superficially resemble earthworms and are limbless. They burrow by means of zones of muscle contractions which move along the body and they swim by undulating their body from side to side.\n\nReptiles are a class of animals comprising turtles, tuataras, lizards, snakes and crocodiles. They are tetrapods, but the snakes and a few species of lizard either have no limbs or their limbs are much reduced in size. Their bones are better ossified and their skeletons stronger than those of amphibians. The teeth are conical and mostly uniform in size. The surface cells of the epidermis are modified into horny scales which create a waterproof layer. Reptiles are unable to use their skin for respiration as do amphibians and have a more efficient respiratory system drawing air into their lungs by expanding their chest walls. The heart resembles that of the amphibian but there is a septum which more completely separates the oxygenated and deoxygenated bloodstreams. The reproductive system has evolved for internal fertilization, with a copulatory organ present in most species. The eggs are surrounded by amniotic membranes which prevents them from drying out and are laid on land, or develop internally in some species. The bladder is small as nitrogenous waste is excreted as uric acid.\n\nTurtles are notable for their protective shells. They have an inflexible trunk encased in a horny carapace above and a plastron below. These are formed from bony plates embedded in the dermis which are overlain by horny ones and are partially fused with the ribs and spine. The neck is long and flexible and the head and the legs can be drawn back inside the shell. Turtles are vegetarians and the typical reptile teeth have been replaced by sharp, horny plates. In aquatic species, the front legs are modified into flippers.\n\nTuataras superficially resemble lizards but the lineages diverged in the Triassic period. There is one living species, \"Sphenodon punctatus\". The skull has two openings (fenestrae) on either side and the jaw is rigidly attached to the skull. There is one row of teeth in the lower jaw and this fits between the two rows in the upper jaw when the animal chews. The teeth are merely projections of bony material from the jaw and eventually wear down. The brain and heart are more primitive than those of other reptiles, and the lungs have a single chamber and lack bronchi. The tuatara has a well-developed parietal eye on its forehead.\n\nLizards have skulls with only one fenestra on each side, the lower bar of bone below the second fenestra having been lost. This results in the jaws being less rigidly attached which allows the mouth to open wider. Lizards are mostly quadrupeds, with the trunk held off the ground by short, sideways-facing legs, but a few species have no limbs and resemble snakes. Lizards have moveable eyelids, eardrums are present and some species have a central parietal eye.\n\nSnakes are closely related to lizards, having branched off from a common ancestral lineage during the Cretaceous period, and they share many of the same features. The skeleton consists of a skull, a hyoid bone, spine and ribs though a few species retain a vestige of the pelvis and rear limbs in the form of pelvic spurs. The bar under the second fenestra has also been lost and the jaws have extreme flexibility allowing the snake to swallow its prey whole. Snakes lack moveable eyelids, the eyes being covered by transparent \"spectacle\" scales. They do not have eardrums but can detect ground vibrations through the bones of their skull. Their forked tongues are used as organs of taste and smell and some species have sensory pits on their heads enabling them to locate warm-blooded prey.\n\nCrocodilians are large, low-slung aquatic reptiles with long snouts and large numbers of teeth. The head and trunk are dorso-ventrally flattened and the tail is laterally compressed. It undulates from side to side to force the animal through the water when swimming. The tough keratinized scales provide body armour and some are fused to the skull. The nostrils, eyes and ears are elevated above the top of the flat head enabling them to remain above the surface of the water when the animal is floating. Valves seal the nostrils and ears when it is submerged. Unlike other reptiles, crocodilians have hearts with four chambers allowing complete separation of oxygenated and deoxygenated blood.\n\nBirds are tetrapods but though their hind limbs are used for walking or hopping, their front limbs are wings covered with feathers and adapted for flight. Birds are endothermic, have a high metabolic rate, a light skeletal system and powerful muscles. The long bones are thin, hollow and very light. Air sac extensions from the lungs occupy the centre of some bones. The sternum is wide and usually has a keel and the caudal vertebrae are fused. There are no teeth and the narrow jaws are adapted into a horn-covered beak. The eyes are relatively large, particularly in nocturnal species such as owls. They face forwards in predators and sideways in ducks.\n\nThe feathers are outgrowths of the epidermis and are found in localized bands from where they fan out over the skin. Large flight feathers are found on the wings and tail, contour feathers cover the bird's surface and fine down occurs on young birds and under the contour feathers of water birds. The only cutaneous gland is the single uropygial gland near the base of the tail. This produces an oily secretion that waterproofs the feathers when the bird preens. There are scales on the legs, feet and claws on the tips of the toes.\n\nMammals are a diverse class of animals, mostly terrestrial but some are aquatic and others have evolved flapping or gliding flight. They mostly have four limbs but some aquatic mammals have no limbs or limbs modified into fins and the forelimbs of bats are modified into wings. The legs of most mammals are situated below the trunk, which is held well clear of the ground. The bones of mammals are well ossified and their teeth, which are usually differentiated, are coated in a layer of prismatic enamel. The teeth are shed once (milk teeth) during the animal's lifetime or not at all, as is the case in cetaceans. Mammals have three bones in the middle ear and a cochlea in the inner ear. They are clothed in hair and their skin contains glands which secrete sweat. Some of these glands are specialized as mammary glands, producing milk to feed the young. Mammals breathe with lungs and have a muscular diaphragm separating the thorax from the abdomen which helps them draw air into the lungs. The mammalian heart has four chambers and oxygenated and deoxygenated blood are kept entirely separate. Nitrogenous waste is excreted primarily as urea.\n\nMammals are amniotes, and most are viviparous, giving birth to live young. The exception to this are the egg-laying monotremes, the platypus and the echidnas of Australia. Most other mammals have a placenta through which the developing foetus obtains nourishment, but in marsupials, the foetal stage is very short and the immature young is born and finds its way to its mother's pouch where it latches on to a nipple and completes its development.\n\nHumans have the overall body plan of a mammal. Humans have a head, neck, trunk (which includes the thorax and abdomen), two arms and hands, and two legs and feet.\n\nGenerally, students of certain biological sciences, paramedics, prosthetists and orthotists, physiotherapists, occupational therapists, nurses, and medical students learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures and tutorials, and in addition, medical students generally also learn gross anatomy through practical experience of dissection and inspection of cadavers. The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope.\nHuman anatomy, physiology and biochemistry are complementary basic medical sciences, which are generally taught to medical students in their first year at medical school. Human anatomy can be taught regionally or systemically; that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems. The major anatomy textbook, Gray's Anatomy, has been reorganized from a systems format to a regional format, in line with modern teaching methods. A thorough working knowledge of anatomy is required by physicians, especially surgeons and doctors working in some diagnostic specialties, such as histopathology and radiology.\nAcademic anatomists are usually employed by universities, medical schools or teaching hospitals. They are often involved in teaching anatomy, and research into certain systems, organs, tissues or cells.\n\nInvertebrates constitute a vast array of living organisms ranging from the simplest unicellular eukaryotes such as \"Paramecium\" to such complex multicellular animals as the octopus, lobster and dragonfly. They constitute about 95% of the animal species. By definition, none of these creatures has a backbone. The cells of single-cell protozoans have the same basic structure as those of multicellular animals but some parts are specialized into the equivalent of tissues and organs. Locomotion is often provided by cilia or flagella or may proceed via the advance of pseudopodia, food may be gathered by phagocytosis, energy needs may be supplied by photosynthesis and the cell may be supported by an endoskeleton or an exoskeleton. Some protozoans can form multicellular colonies.\n\nMetazoans are multicellular organism, different groups of cells of which have separate functions. The most basic types of metazoan tissues are epithelium and connective tissue, both of which are present in nearly all invertebrates. The outer surface of the epidermis is normally formed of epithelial cells and secretes an extracellular matrix which provides support to the organism. An endoskeleton derived from the mesoderm is present in echinoderms, sponges and some cephalopods. Exoskeletons are derived from the epidermis and is composed of chitin in arthropods (insects, spiders, ticks, shrimps, crabs, lobsters). Calcium carbonate constitutes the shells of molluscs, brachiopods and some tube-building polychaete worms and silica forms the exoskeleton of the microscopic diatoms and radiolaria. Other invertebrates may have no rigid structures but the epidermis may secrete a variety of surface coatings such as the pinacoderm of sponges, the gelatinous cuticle of cnidarians (polyps, sea anemones, jellyfish) and the collagenous cuticle of annelids. The outer epithelial layer may include cells of several types including sensory cells, gland cells and stinging cells. There may also be protrusions such as microvilli, cilia, bristles, spines and tubercles.\n\nMarcello Malpighi, the father of microscopical anatomy, discovered that plants had tubules similar to those he saw in insects like the silk worm. He observed that when a ring-like portion of bark was removed on a trunk a swelling occurred in the tissues above the ring, and he unmistakably interpreted this as growth stimulated by food coming down from the leaves, and being captured above the ring.\n\nArthropods comprise the largest phylum in the animal kingdom with over a million known invertebrate species.\n\nInsects possess segmented bodies supported by a hard-jointed outer covering, the exoskeleton, made mostly of chitin. The segments of the body are organized into three distinct parts, a head, a thorax and an abdomen. The head typically bears a pair of sensory antennae, a pair of compound eyes, one to three simple eyes (ocelli) and three sets of modified appendages that form the mouthparts. The thorax has three pairs of segmented legs, one pair each for the three segments that compose the thorax and one or two pairs of wings. The abdomen is composed of eleven segments, some of which may be fused and houses the digestive, respiratory, excretory and reproductive systems. There is considerable variation between species and many adaptations to the body parts, especially wings, legs, antennae and mouthparts.\n\nSpiders a class of arachnids have four pairs of legs; a body of two segments—a cephalothorax and an abdomen. Spiders have no wings and no antennae. They have mouthparts called chelicerae which are often connected to venom glands as most spiders are venomous. They have a second pair of appendages called pedipalps attached to the cephalothorax. These have similar segmentation to the legs and function as taste and smell organs. At the end of each male pedipalp is a spoon-shaped cymbium that acts to support the copulatory organ.\n\n\nIn 1600 BCE, the Edwin Smith Papyrus, an Ancient Egyptian medical text, described the heart, its vessels, liver, spleen, kidneys, hypothalamus, uterus and bladder, and showed the blood vessels diverging from the heart. The Ebers Papyrus (c. 1550 BCE) features a \"treatise on the heart\", with vessels carrying all the body's fluids to or from every member of the body.\n\nAncient Greek anatomy and physiology underwent great changes and advances throughout the early medieval world. Over time, this medical practice expanded by a continually developing understanding of the functions of organs and structures in the body. Phenomenal anatomical observations of the human body were made, which have contributed towards the understanding of the brain, eye, liver, reproductive organs and the nervous system.\n\nThe Hellenistic Egyptian city of Alexandria was the stepping-stone for Greek anatomy and physiology. Alexandria not only housed the biggest library for medical records and books of the liberal arts in the world during the time of the Greeks, but was also home to many medical practitioners and philosophers. Great patronage of the arts and sciences from the Ptolemy rulers helped raise Alexandria up, further rivalling the cultural and scientific achievements of other Greek states.\n\nSome of the most striking advances in early anatomy and physiology took place in Hellenistic Alexandria. Two of the most famous anatomists and physiologists of the third century were Herophilus and Erasistratus. These two physicians helped pioneer human dissection for medical research. They also conducted vivisections on the cadavers of condemned criminals, which was considered taboo until the Renaissance – Herophilus was recognized as the first person to perform systematic dissections. Herophilus became known for his anatomical works making impressing contributions to many branches of anatomy and many other aspects of medicine. Some of the works included classifying the system of the pulse, the discovery that human arteries had thicker walls then veins, and that the atria were parts of the heart. Herophilus’s knowledge of the human body has provided vital input towards understanding the brain, eye, liver, reproductive organs and nervous system, and characterizing the course of disease. Erasistratus accurately described the structure of the brain, including the cavities and membranes, and made a distinction between its cerebrum and cerebellum During his study in Alexandria, Erasistratus was particularly concerned with studies of the circulatory and nervous systems. He was able to distinguish the sensory and the motor nerves in the human body and believed that air entered the lungs and heart, which was then carried throughout the body. His distinction between the arteries and veins – the arteries carrying the air through the body, while the veins carried the blood from the heart was a great anatomical discovery. Erasistratus was also responsible for naming and describing the function of the epiglottis and the valves of the heart, including the tricuspid. During the third century, Greek physicians were able to differentiate nerves from blood vessels and tendons and to realize that the nerves convey neural impulses. It was Herophilus who made the point that damage to motor nerves induced paralysis. Herophilus named the meninges and ventricles in the brain, appreciated the division between cerebellum and cerebrum and recognized that the brain was the \"seat of intellect\" and not a \"cooling chamber\" as propounded by Aristotle Herophilus is also credited with describing the optic, oculomotor, motor division of the trigeminal, facial, vestibulocochlear and hypoglossal nerves.\n\nGreat feats were made during the third century in both the digestive and reproductive systems. Herophilus was able to discover and describe not only the salivary glands, but the small intestine and liver. He showed that the uterus is a hollow organ and described the ovaries and uterine tubes. He recognized that spermatozoa were produced by the testes and was the first to identify the prostate gland.\n\nThe anatomy of the muscles and skeleton is described in the \"Hippocratic Corpus\", an Ancient Greek medical work written by unknown authors. Aristotle described vertebrate anatomy based on animal dissection. Praxagoras identified the difference between arteries and veins. Also in the 4th century BCE, Herophilos and Erasistratus produced more accurate anatomical descriptions based on vivisection of criminals in Alexandria during the Ptolemaic dynasty.\n\nIn the 2nd century, Galen of Pergamum, an anatomist, clinician, writer and philosopher, wrote the final and highly influential anatomy treatise of ancient times. He compiled existing knowledge and studied anatomy through dissection of animals. He was one of the first experimental physiologists through his vivisection experiments on animals. Galen's drawings, based mostly on dog anatomy, became effectively the only anatomical textbook for the next thousand years. His work was known to Renaissance doctors only through Islamic Golden Age medicine until it was translated from the Greek some time in the 15th century.\n\nAnatomy developed little from classical times until the sixteenth century; as the historian Marie Boas writes, \"Progress in anatomy before the sixteenth century is as mysteriously slow as its development after 1500 is startlingly rapid\". Between 1275 and 1326, the anatomists Mondino de Luzzi, Alessandro Achillini and Antonio Benivieni at Bologna carried out the first systematic human dissections since ancient times. Mondino's \"Anatomy\" of 1316 was the first textbook in the medieval rediscovery of human anatomy. It describes the body in the order followed in Mondino's dissections, starting with the abdomen, then the thorax, then the head and limbs. It was the standard anatomy textbook for the next century.\n\nLeonardo da Vinci (1452–1519) was trained in anatomy by Andrea del Verrocchio. He made use of his anatomical knowledge in his artwork, making many sketches of skeletal structures, muscles and organs of humans and other vertebrates that he dissected.\n\nAndreas Vesalius (1514–1564) (Latinized from Andries van Wezel), professor of anatomy at the University of Padua, is considered the founder of modern human anatomy. Originally from Brabant, Vesalius published the influential book \"De humani corporis fabrica\" (\"the structure of the human body\"), a large format book in seven volumes, in 1543. The accurate and intricately detailed illustrations, often in allegorical poses against Italianate landscapes, are thought to have been made by the artist Jan van Calcar, a pupil of Titian.\n\nIn England, anatomy was the subject of the first public lectures given in any science; these were given by the Company of Barbers and Surgeons in the 16th century, joined in 1583 by the Lumleian lectures in surgery at the Royal College of Physicians.\n\nIn the United States, medical schools began to be set up towards the end of the 18th century. Classes in anatomy needed a continual stream of cadavers for dissection and these were difficult to obtain. Philadelphia, Baltimore and New York were all renowned for body snatching activity as criminals raided graveyards at night, removing newly buried corpses from their coffins. A similar problem existed in Britain where demand for bodies became so great that grave-raiding and even anatomy murder were practised to obtain cadavers. Some graveyards were in consequence protected with watchtowers. The practice was halted in Britain by the Anatomy Act of 1832, while in the United States, similar legislation was enacted after the physician William S. Forbes of Jefferson Medical College was found guilty in 1882 of \"complicity with resurrectionists in the despoliation of graves in Lebanon Cemetery\".\n\nThe teaching of anatomy in Britain was transformed by Sir John Struthers, Regius Professor of Anatomy at the University of Aberdeen from 1863 to 1889. He was responsible for setting up the system of three years of \"pre-clinical\" academic teaching in the sciences underlying medicine, including especially anatomy. This system lasted until the reform of medical training in 1993 and 2003. As well as teaching, he collected many vertebrate skeletons for his museum of comparative anatomy, published over 70 research papers, and became famous for his public dissection of the Tay Whale. From 1822 the Royal College of Surgeons regulated the teaching of anatomy in medical schools. Medical museums provided examples in comparative anatomy, and were often used in teaching. Ignaz Semmelweis investigated puerperal fever and he discovered how it was caused. He noticed that the frequently fatal fever occurred more often in mothers examined by medical students than by midwives. The students went from the dissecting room to the hospital ward and examined women in childbirth. Semmelweis showed that when the trainees washed their hands in chlorinated lime before each clinical examination, the incidence of puerperal fever among the mothers could be reduced dramatically.\n\nBefore the era of modern medical procedures, the main means for studying the internal structure of the body were palpation and dissection. It was the advent of microscopy that opened up an understanding of the building blocks that constituted living tissues. Technical advances in the development of achromatic lenses increased the resolving power of the microscope and around 1839, Matthias Jakob Schleiden and Theodor Schwann identified that cells were the fundamental unit of organization of all living things. Study of small structures involved passing light through them and the microtome was invented to provide sufficiently thin slices of tissue to examine. Staining techniques using artificial dyes were established to help distinguish between different types of tissue. The fields of cytology and histology developed from here in the late 19th century. The invention of the electron microscope brought a great advance in resolution power and allowed research into the ultrastructure of cells and the organelles and other structures within them. About the same time, in the 1950s, the use of X-ray diffraction for studying the crystal structures of proteins, nucleic acids and other biological molecules gave rise to a new field of molecular anatomy.\n\nShort wavelength electromagnetic radiation such as X-rays can be passed through the body and used in medical radiography to view interior structures that have different degrees of opaqueness. Nowadays, modern techniques such as magnetic resonance imaging, computed tomography, fluoroscopy and ultrasound imaging have enabled researchers and practitioners to examine organs, living or dead, in unprecedented detail. They are used for diagnostic and therapeutic purposes and provide information on the internal structures and organs of the body to a degree far beyond the imagination of earlier generations.\n\n\n\n"}
{"id": "675", "url": "https://en.wikipedia.org/wiki?curid=675", "title": "Affirming the consequent", "text": "Affirming the consequent\n\nAffirming the consequent, sometimes called converse error, fallacy of the converse or confusion of necessity and sufficiency, is a formal fallacy of inferring the converse from the original statement. The corresponding argument has the general form:\n\nAn argument of this form is invalid, i.e., the conclusion can be false even when statements 1 and 2 are true. Since \"P\" was never asserted as the \"only\" sufficient condition for \"Q\", other factors could account for \"Q\" (while \"P\" was false).\n\nTo put it differently, if \"P\" implies \"Q\", the only inference that can be made is \"non-Q\" implies \"non-P\". (\"Non-P\" and \"non-Q\" designate the opposite propositions to \"P\" and \"Q\".) This is known as logical contraposition. Symbolically:\n\nformula_2\n\nThe name \"affirming the consequent\" derives from the premise \"Q\", which affirms the \"then\" clause of the conditional premise.\n\nExample 1\n\nOne way to demonstrate the invalidity of this argument form is with a counterexample with true premises but an obviously false conclusion. For example:\n\nOwning Fort Knox is not the \"only\" way to be rich. Any number of other ways exist to be rich.\n\nHowever, one can affirm with certainty that \"if someone is not rich\" (\"non-Q\"), then \"this person does not own Fort Knox\" (\"non-P\"). This is the contrapositive of the first statement, and it must be true if and only if the original statement is true.\n\nExample 2\n\nArguments of the same form can sometimes seem superficially convincing, as in the following example:\n\nBut having the flu is not the \"only\" cause of a sore throat since many illnesses cause sore throat, such as the common cold or strep throat.\n\nAffirming the consequent is commonly used in rationalization, and thus appears as a coping mechanism in some people.\n\nExample 3\n\nIn Joseph Heller's \"Catch-22\", the chaplain is interrogated for supposedly being 'Washinton Irving'/'Irving Washington', who has been blocking out large portions of soldier's letters home. The colonel has found such a letter, but with the Chaplain's name signed.\n\n\"P\" in this case is 'The chaplain signs his own name', and \"Q\" 'The chaplain's name is written'. The chaplain's name may be written, but he did not necessarily write it, as the colonel falsely concludes \"(and in fact he did not, as in the novel, Yossarian signed the name\"\")\".\n\n"}
{"id": "676", "url": "https://en.wikipedia.org/wiki?curid=676", "title": "Andrei Tarkovsky", "text": "Andrei Tarkovsky\n\nAndrei Arsenyevich Tarkovsky (; 4 April 1932 – 29 December 1986) was a Soviet filmmaker, writer, film editor, film theorist, theatre and opera director.\n\nTarkovsky's films include \"Ivan's Childhood\" (1962), \"Andrei Rublev\" (1966), \"Solaris\" (1972), \"Mirror\" (1975), and \"Stalker\" (1979). He directed the first five of his seven feature films in the Soviet Union; his last two films, \"Nostalghia\" (1983) and \"The Sacrifice\" (1986), were produced in Italy and Sweden, respectively. His work is characterized by long takes, unconventional dramatic structure, distinctly authored use of cinematography, and spiritual and metaphysical themes.\n\nTarkovsky's works \"Andrei Rublev\", \"Mirror\", and \"Stalker\" are regularly listed among the greatest films of all time. His contribution to cinema was so influential that works done in a similar way are described as Tarkovskian. Ingmar Bergman said of him:\n\nTarkovsky for me is the greatest (director), the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream.\n\nAndrei Tarkovsky was born in the village of Zavrazhye in the Yuryevetsky District of the Ivanovo Industrial Oblast to the poet and translator Arseny Alexandrovich Tarkovsky, a native of Yelisavetgrad, Kherson Governorate, and Maria Ivanova Vishnyakova, a graduate of the Maxim Gorky Literature Institute who later worked as a corrector; she was born in Moscow in the Dubasov family estate. Andrei's paternal grandfather Aleksandr Karlovich Tarkovsky (in ) was a Polish nobleman who worked as a bank clerk. His wife Maria Danilovna Rachkovskaya was a Romanian teacher who arrived from Iași. Andrei's maternal grandmother Vera Nikolaevna Vishnyakova (née Dubasova) belonged to an old Dubasov family of Russian nobility that traces its history back to the 17th century; among her relatives was Admiral Fyodor Dubasov, a fact she had to conceal during the Soviet days. She was married to Ivan Ivanovich Vishnyakov, a native of the Kaluga Governorate who studied law at the Moscow University and served as a judge in Kozelsk. According to the family legend, Tarkovsky's ancestors on his father's side were princes from the Shamkhalate of Tarki, Dagestan, although his sister Marina Tarkovskaya who did a detailed research on their genealogy called it «a myth, even a prank of sorts», stressing that none of the documents confirms this version.\n\nTarkovsky spent his childhood in Yuryevets. He was described by childhood friends as active and popular, having many friends and being typically in the center of action. His father left the family in 1937, subsequently volunteering for the army in 1941. Tarkovsky stayed with his mother, moving with her and his sister Marina to Moscow, where she worked as a proofreader at a printing press. In 1939 Tarkovsky enrolled at the Moscow School № 554. During the war, the three evacuated to Yuryevets, living with his maternal grandmother. In 1943 the family returned to Moscow. Tarkovsky continued his studies at his old school, where the poet Andrey Voznesensky was one of his classmates. He studied piano at a music school and attended classes at an art school. The family lived on Shchipok Street in the Zamoskvorechye District in Moscow. From November 1947 to spring 1948 he was in the hospital with tuberculosis. Many themes of his childhood—the evacuation, his mother and her two children, the withdrawn father, the time in the hospital—feature prominently in his film \"Mirror\".\n\nIn his school years, Tarkovsky was a troublemaker and a poor student. He still managed to graduate, and from 1951 to 1952 studied Arabic at the Oriental Institute in Moscow, a branch of the Academy of Sciences of the USSR. Although he already spoke some Arabic and was a successful student in his first semesters, he did not finish his studies and dropped out to work as a prospector for the Academy of Science Institute for Non-Ferrous Metals and Gold. He participated in a year-long research expedition to the river Kureikye near Turukhansk in the Krasnoyarsk Province. During this time in the taiga, Tarkovsky decided to study film.\n\nUpon returning from the research expedition in 1954, Tarkovsky applied at the State Institute of Cinematography (VGIK) and was admitted to the film-directing program. He was in the same class as Irma Raush whom he married in April 1957.\n\nThe early Khrushchev era offered good opportunities for young film directors. Before 1953, annual film production was low and most films were directed by veteran directors. After 1953, more films were produced, many of them by young directors. The Khrushchev Thaw relaxed Soviet social restrictions a bit and permitted a limited influx of European and North American literature, films and music. This allowed Tarkovsky to see films of the Italian neorealists, French New Wave, and of directors such as Kurosawa, Buñuel, Bergman, Bresson, Andrzej Wajda (whose film \"Ashes and Diamonds\" influenced Tarkovsky) and Mizoguchi. Tarkovsky absorbed the idea of the auteur as a necessary condition for creativity.\n\nTarkovsky's teacher and mentor was Mikhail Romm, who taught many film students who would later become influential film directors. In 1956 Tarkovsky directed his first student short film, \"The Killers\", from a short story of Ernest Hemingway. The short film \"There Will Be No Leave Today\" and the screenplay \"Concentrate\" followed in 1958 and 1959.\n\nAn important influence on Tarkovsky was the film director Grigori Chukhrai, who was teaching at the VGIK. Impressed by the talent of his student, Chukhrai offered Tarkovsky a position as assistant director for his film \"Clear Skies\". Tarkovsky initially showed interest but then decided to concentrate on his studies and his own projects.\n\nDuring his third year at the VGIK, Tarkovsky met Andrei Konchalovsky. They found much in common as they liked the same film directors and shared ideas on cinema and films. In 1959 they wrote the script \"Antarctica – Distant Country\", which was later published in the \"Moskovskij Komsomolets\". Tarkovsky submitted the script to Lenfilm, but it was rejected. They were more successful with the script \"The Steamroller and the Violin\", which they sold to Mosfilm. This became Tarkovsky's graduation project, earning him his diploma in 1960 and winning First Prize at the New York Student Film Festival in 1961.\n\nTarkovsky's first feature film was \"Ivan's Childhood\" in 1962. He had inherited the film from director Eduard Abalov, who had to abort the project. The film earned Tarkovsky international acclaim and won the Golden Lion award at the Venice Film Festival in the year 1962. In the same year, on 30 September, his first son Arseny (called Senka in Tarkovsky's diaries) Tarkovsky was born.\n\nIn 1965, he directed the film \"Andrei Rublev\" about the life of Andrei Rublev, the fifteenth-century Russian icon painter. \"Andrei Rublev\" was not, except for a single screening in Moscow in 1966, immediately released after completion due to problems with Soviet authorities. Tarkovsky had to cut the film several times, resulting in several different versions of varying lengths. A version of the film was presented at the Cannes Film Festival in 1969 and won the FIPRESCI prize. The film was widely released in the Soviet Union in a cut version in 1971.\n\nHe divorced his wife, Irma Raush, in June 1970. In the same year, he married Larissa Kizilova (née Egorkina), who had been a production assistant for the film \"Andrei Rublev\" (they had been living together since 1965). Their son, Andrei Andreyevich Tarkovsky, was born in the same year on 7 August.\n\nIn 1972, he completed \"Solaris\", an adaptation of the novel \"Solaris\" by Stanisław Lem. He had worked on this together with screenwriter Fridrikh Gorenshtein as early as 1968. The film was presented at the Cannes Film Festival, won the Grand Prix Spécial du Jury and the FIPRESCI prize, and was nominated for the Palme d'Or. From 1973 to 1974, he shot the film \"Mirror\", a highly autobiographical and unconventionally structured film drawing on his childhood and incorporating some of his father's poems. Tarkovsky had worked on the screenplay for this film since 1967, under the consecutive titles \"Confession\", \"White day\" and \"A white, white day\". From the beginning the film was not well received by Soviet authorities due to its content and its perceived elitist nature. Russian authorities placed the film in the \"third category,\" a severely limited distribution, and only allowed it to be shown in third-class cinemas and workers' clubs. Few prints were made and the film-makers received no returns. Third category films also placed the film-makers in danger of being accused of wasting public funds, which could have serious effects on their future productivity. These difficulties are presumed to have made Tarkovsky play with the idea of going abroad and producing a film outside the Soviet film industry.\n\nDuring 1975, Tarkovsky also worked on the screenplay \"Hoffmanniana\", about the German writer and poet E. T. A. Hoffmann. In December 1976, he directed \"Hamlet\", his only stage play, at the Lenkom Theatre in Moscow. The main role was played by Anatoly Solonitsyn, who also acted in several of Tarkovsky's films. At the end of 1978, he also wrote the screenplay \"Sardor\" together with the writer Aleksandr Misharin.\n\nThe last film Tarkovsky completed in the Soviet Union was \"Stalker\", inspired by the novel \"Roadside Picnic\" by the brothers Arkady and Boris Strugatsky. Tarkovsky had met the brothers first in 1971 and was in contact with them until his death in 1986. Initially he wanted to shoot a film based on their novel \"Dead Mountaineer's Hotel\" and he developed a raw script. Influenced by a discussion with Arkady Strugatsky he changed his plan and began to work on the script based on \"Roadside Picnic\". Work on this film began in 1976. The production was mired in troubles; improper development of the negatives had ruined all the exterior shots. Tarkovsky's relationship with cinematographer Georgy Rerberg deteriorated to the point where he hired Alexander Knyazhinsky as a new first cinematographer. Furthermore, Tarkovsky suffered a heart attack in April 1978, resulting in further delay. The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival.\n\nIn the same year Tarkovsky also began the production of the film \"The First Day\" (Russian: Первый День \"Pervyj Dyen′\"), based on a script by his friend and long-term collaborator Andrei Konchalovsky. The film was set in 18th-century Russia during the reign of Peter the Great and starred Natalya Bondarchuk and Anatoli Papanov. To get the project approved by Goskino, Tarkovsky submitted a script that was different from the original script, omitting several scenes that were critical of the official atheism in the Soviet Union. After shooting roughly half of the film the project was stopped by Goskino after it became apparent that the film differed from the script submitted to the censors. Tarkovsky was reportedly infuriated by this interruption and destroyed most of the film.\n\nDuring the summer of 1979, Tarkovsky traveled to Italy, where he shot the documentary \"Voyage in Time\" together with his long-time friend Tonino Guerra. Tarkovsky returned to Italy in 1980 for an extended trip during which he and Guerra completed the script for the film \"Nostalghia\".\n\nTarkovsky returned to Italy in 1982 to start shooting \"Nostalghia\". He did not return to his home country. As Mosfilm withdrew from the project, he had to complete the film with financial support provided by the Italian RAI. Tarkovsky completed the film in 1983. \"Nostalghia\" was presented at the Cannes Film Festival and won the FIPRESCI prize and the Prize of the Ecumenical Jury. Tarkovsky also shared a special prize called \"Grand Prix du cinéma de creation\" with Robert Bresson. Soviet authorities prevented the film from winning the Palme d'Or, a fact that hardened Tarkovsky's resolve to never work in the Soviet Union again. In the same year, he also staged the opera \"Boris Godunov\" at the Royal Opera House in London under the musical direction of Claudio Abbado.\n\nHe spent most of 1984 preparing the film \"The Sacrifice\". At a press conference in Milan on 10 July 1984, he announced that he would never return to the Soviet Union and would remain in Europe. At that time, his son Andrei Jr. was still in the Soviet Union and not allowed to leave the country. On 28 August 1985, Tarkovsky arrived at Latina Refugee Camp in Latina, where he was registered with the serial number 13225/379.\n\nDuring 1985, he shot the film \"The Sacrifice\" in Sweden. At the end of the year he was diagnosed with terminal lung cancer. In January 1986, he began treatment in Paris and was joined there by his son, who was finally allowed to leave the Soviet Union. \"The Sacrifice\" was presented at the Cannes Film Festival and received the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury. As Tarkovsky was unable to attend due to his illness, the prizes were collected by his son, Andrei Jr.\n\nIn Tarkovsky's last entry (15 December 1986), he wrote: \"But now I have no strength left – that is the problem\". The diaries are sometimes also known as \"\" and were published posthumously in 1989 and in English in 1991.\n\nTarkovsky died in Paris on 29 December 1986. His funeral ceremony was held at the Alexander Nevsky Cathedral. He was buried on 3 January 1987 in the Russian Cemetery in Sainte-Geneviève-des-Bois in France. The inscription on his gravestone, which was created by the Russian sculptor Ernst Neizvestny, reads: \"To the man who saw the Angel\".\n\nA conspiracy theory emerged in Russia in the early 1990s when it was alleged that Tarkovsky did not die of natural causes but was assassinated by the KGB. Evidence for this hypothesis includes testimonies by former KGB agents who claim that Viktor Chebrikov gave the order to eradicate Tarkovsky to curtail what the Soviet government and the KGB saw as anti-Soviet propaganda by Tarkovsky. Other evidence includes several memoranda that surfaced after the 1991 coup and the claim by one of Tarkovsky's doctors that his cancer could not have developed from a natural cause.\n\nAs with Tarkovsky, his wife Larisa Tarkovskaya and actor Anatoli Solonitsyn all died from the very same type of lung cancer. Vladimir Sharun, sound designer in \"Stalker\", is convinced that they were all poisoned by the chemical plant where they were shooting the film.\n\nTarkovsky is mainly known as a film director. During his career he directed only seven feature films, as well as three shorts from his time at VGIK. He also wrote several screenplays. He furthermore directed the play \"Hamlet\" for the stage in Moscow, directed the opera \"Boris Godunov\" in London, and he directed a radio production of the short story \"Turnabout\" by William Faulkner. He also wrote \"Sculpting in Time\", a book on film theory.\n\nTarkovsky's first feature film was \"Ivan's Childhood\" in 1962. He then directed \"Andrei Rublev\" in 1966, \"Solaris\" in 1972, \"Mirror\" in 1975 and \"Stalker\" in 1979. The documentary \"Voyage in Time\" was produced in Italy in 1982, as was \"Nostalghia\" in 1983. His last film \"The Sacrifice\" was produced in Sweden in 1986. Tarkovsky was personally involved in writing the screenplays for all his films, sometimes with a cowriter. Tarkovsky once said that a director who realizes somebody else's screenplay without being involved in it becomes a mere illustrator, resulting in dead and monotonous films.\n\nA book of 60 photos, \"Instant Light, Tarkovsky Polaroids\", taken by Tarkovsky in Russia and Italy between 1979 and 1984 was published in 2006. The collection was selected by Italian photographer Giovanni Chiaramonte and Tarkovsky's son Andrey A. Tarkovsky.\n\nNumerous awards were bestowed on Tarkovsky throughout his lifetime. At the Venice Film Festival he was awarded the Golden Lion for \"Ivan's Childhood\". At the Cannes Film Festival, he won the FIPRESCI prize four times, the Prize of the Ecumenical Jury three times (more than any other director), and the Grand Prix Spécial du Jury twice. He was also nominated for the Palme d'Or two times. In 1987, the British Academy of Film and Television Arts awarded the BAFTA Award for Best Foreign Language Film to \"The Sacrifice\".\n\nUnder the influence of Glasnost and Perestroika, Tarkovsky was finally recognized in the Soviet Union in the Autumn of 1986, shortly before his death, by a retrospective of his films in Moscow. After his death, an entire issue of the film magazine \"Iskusstvo Kino\" was devoted to Tarkovsky. In their obituaries, the film committee of the Council of Ministers of the USSR and the Union of Soviet Film Makers expressed their sorrow that Tarkovsky had to spend the last years of his life in exile.\n\nPosthumously, he was awarded the Lenin Prize in 1990, one of the highest state honors in the Soviet Union. In 1989 the \"Andrei Tarkovsky Memorial Prize\" was established, with its first recipient being the Russian animator Yuriy Norshteyn. In three consecutive events, the Moscow International Film Festival awards the annual \"Andrei Tarkovsky Award\" in the years of 1993, 1995 and 1997. In 1996 the Andrei Tarkovsky Museum opened in Yuryevets, his childhood town. A minor planet, 3345 Tarkovskij, discovered by Soviet astronomer Lyudmila Georgievna Karachkina in 1982, has also been named after him.\n\nTarkovsky has been the subject of several documentaries. Most notable is the 1988 documentary \"Moscow Elegy\", by Russian film director Alexander Sokurov. Sokurov's own work has been heavily influenced by Tarkovsky. The film consists mostly of narration over stock footage from Tarkovsky's films. \"Directed by Andrei Tarkovsky\" is 1988 documentary film by Michal Leszczylowski, an editor of the film \"The Sacrifice\". Film director Chris Marker produced the television documentary \"One Day in the Life of Andrei Arsenevich\" as an homage to Andrei Tarkovsky in 2000.\n\nIngmar Bergman was quoted as saying: \"Tarkovsky for me is the greatest [of us all], the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream\". Film historian Steven Dillon says that much of subsequent film was deeply influenced by the films of Tarkovsky.\n\nAt the entrance to the Gerasimov Institute of Cinematography in Moscow, Russia there is a monument that includes statues of Tarkovsky, Gennady Shpalikov and Vasily Shukshin.\n\nConcentrate (, \"Konsentrat\") is a never-filmed 1958 screenplay by Russian film director Andrei Tarkovsky. The screenplay is based on Tarkovsky's year in the taiga as a member of a research expedition, prior to his enrollment in film school.\n\n\"Concentrate\" is about the leader of a geological expedition, who waits for the boat that brings back the concentrates collected by the expedition. The expedition is surrounded by mystery, and its purpose is a state secret. This screenplay refers to Tarkovsky's year in the taiga, where he was a member of a research expedition prior to enrolling at the film school.\n\nAlthough some authors claim that the screenplay was filmed, according to Marina Tarkovskaya, Tarkovsky's sister (and wife of Aleksandr Gordon, a fellow student of Tarvosky during his film school years) the screenplay was never filmed. Tarkovsky wrote the screenplay during his entrance examination at the State Institute of Cinematography (VGIK) in a single sitting. He earned the highest possible grade, excellent () for this work. In 1994 fragments of the \"Concentrate\" were filmed and used in the documentary \"Andrei Tarkovsky's Taiga Summer\" by Marina Tarkovskaya and Aleksandr Gordon.\n\nHoffmanniana () is a never-filmed 1974 screenplay by Russian film director Andrei Tarkovsky. The screenplay is based on the life and work of German author E. T. A. Hoffmann. In 1974 an acquaintance from Tallinnfilm approached Tarkovsky to write a screenplay on a German theme. Tarkovsky considered Thomas Mann and E.T.A. Hoffmann, and also thought about Ibsen's \"Peer Gynt\". In the end Tarkovsky signed a contract for a script based on the life and work of Hoffmann. Tarkovsky planned to write the script during the summer of 1974 at his dacha. Writing was not without difficulty, less than a month before the deadline he had not written a single page. He finally finished the project in late 1974 and submitted the final script to Tallinnfilm in October.\n\nAlthough the script was well received by the officials at Tallinnfilm, it was the consensus that no one but Tarkovsky would be able to direct it. The script was sent to Goskino in February 1976, and although approval was granted for proceeding with making the film the screenplay was never realized. In 1984, during the time of his exile in the West, Tarkovsky revisited the screenplay and made a few changes. He also considered to finally direct a film based on the screenplay but ultimately dropped this idea.\n\nTarkovsky became a film director during the mid and late 1950s, a period referred to as the Khrushchev Thaw, during which Soviet society opened to foreign films, literature and music, among other things. This allowed Tarkovsky to see films of European, American and Japanese directors, an experience which influenced his own film making. His teacher and mentor at the film school, Mikhail Romm, allowed his students considerable freedom and emphasized the independence of the film director.\n\nTarkovsky was, according to fellow student Shavkat Abdusalmov, fascinated by Japanese films. He was amazed by how every character on the screen is exceptional and how everyday events such as a Samurai cutting bread with his sword are elevated to something special and put into the limelight. Tarkovsky has also expressed interest in the art of Haiku and its ability to create \"images in such a way that they mean nothing beyond themselves.\"\n\nTarkovsky perceived that the art of cinema has only been truly mastered by very few filmmakers, stating in a 1970 interview with Naum Abramov that \"they can be counted on the fingers of one hand.\" In 1972, Tarkovsky told film historian Leonid Kozlov his ten favorite films. The list includes: \"Diary of a Country Priest\" and \"Mouchette\" by Robert Bresson; \"Winter Light\", \"Wild Strawberries\", and \"Persona\" by Ingmar Bergman; \"Nazarín\" by Luis Buñuel; \"City Lights\" by Charlie Chaplin; \"Ugetsu\" by Kenji Mizoguchi; \"Seven Samurai\" by Akira Kurosawa, and \"Woman in the Dunes\" by Hiroshi Teshigahara. Among his favorite directors were Buñuel, Mizoguchi, Bergman, Bresson, Kurosawa, Michelangelo Antonioni, Jean Vigo, and Carl Theodor Dreyer.\n\nWith the exception of \"City Lights\", the list does not contain any films of the early silent era. The reason is that Tarkovsky saw film as an art as only a relatively recent phenomenon, with the early film-making forming only a prelude. The list has also no films or directors from Tarkovsky's native Russia, although he rated Soviet directors such as Boris Barnet, Sergei Parajanov and Alexander Dovzhenko highly.\n\nAlthough strongly opposed to commercial cinema, in a famous exception Tarkovsky praised the blockbuster film \"The Terminator\", saying its \"vision of the future and the relation between man and its destiny is pushing the frontier of cinema as an art\". He was critical of the \"brutality and low acting skills\", but nevertheless impressed by this film.\n\nIn a 1962 interview, Tarkovsky argued, \"All art, of course, is intellectual, but for me, all the arts, and cinema even more so, must above all be emotional and act upon the heart.\" His films are characterized by metaphysical themes, extremely long takes, and images often considered by critics to be of exceptional beauty. Recurring motifs are dreams, memory, childhood, running water accompanied by fire, rain indoors, reflections, levitation, and characters re-appearing in the foreground of long panning movements of the camera. He once said, \"Juxtaposing a person with an environment that is boundless, collating him with a countless number of people passing by close to him and far away, relating a person to the whole world, that is the meaning of cinema.”\n\nTarkovsky incorporated levitation scenes into several of his films, most notably \"Solaris\". To him these scenes possess great power and are used for their photogenic value and magical inexplicability. Water, clouds, and reflections were used by him for their surreal beauty and photogenic value, as well as their symbolism, such as waves or the forms of brooks or running water. Bells and candles are also frequent symbols. These are symbols of film, sight and sound, and Tarkovsky's film frequently has themes of self-reflection.\n\nTarkovsky developed a theory of cinema that he called \"sculpting in time\". By this he meant that the unique characteristic of cinema as a medium was to take our experience of time and alter it. Unedited movie footage transcribes time in real time. By using long takes and few cuts in his films, he aimed to give the viewers a sense of time passing, time lost, and the relationship of one moment in time to another.\n\nUp to, and including, his film \"Mirror\", Tarkovsky focused his cinematic works on exploring this theory. After \"Mirror\", he announced that he would focus his work on exploring the dramatic unities proposed by Aristotle: a concentrated action, happening in one place, within the span of a single day.\n\nSeveral of Tarkovsky's films have color or black and white sequences. This first occurs in the otherwise monochrome \"Andrei Rublev\", which features a color epilogue of Rublev's authentic religious icon paintings. All of his films afterwards contain monochrome, and in \"Stalker's\" case sepia sequences, while otherwise being in color. In 1966, in an interview conducted shortly after finishing \"Andrei Rublev\", Tarkovsky dismissed color film as a \"commercial gimmick\" and cast doubt on the idea that contemporary films meaningfully use color. He claimed that in everyday life one does not consciously notice colors most of the time, and that color should therefore be used in film mainly to emphasize certain moments, but not all the time, as this distracts the viewer. To him, films in color were like moving paintings or photographs, which are too beautiful to be a realistic depiction of life.\n\nTarkovsky worked in close collaboration with cinematographer Vadim Yusov from 1958 to 1972, and much of the visual style of Tarkovsky's films can be attributed to this collaboration. Tarkovsky would spend two days preparing for Yusov to film a single long take, and due to the preparation, usually only a single take was needed.\n\nIn his last film, \"The Sacrifice\", Tarkovsky worked with cinematographer Sven Nykvist, who had worked closely with director Ingmar Bergman on many of Ingmar Bergman's films – multiple people who worked with Bergman worked on the production, notably lead actor Erland Josephson, who had acted for Tarkovsky in \"Nostalghia\". Nykvist complained that Tarkovsky would frequently look through the camera and even direct actors through it.\n\n\nNotes\nBibliography\n"}
{"id": "677", "url": "https://en.wikipedia.org/wiki?curid=677", "title": "Ambiguity", "text": "Ambiguity\n\nAmbiguity is a type of uncertainty of meaning in which several interpretations are plausible. It is thus an attribute of any idea or statement whose intended meaning cannot be definitively resolved according to a rule or process with a finite number of steps. (The \"ambi-\" part of the term reflects an idea of \"two\", as in \"two meanings\".)\n\nThe concept of ambiguity is generally contrasted with vagueness. In ambiguity, specific and distinct interpretations are permitted (although some may not be immediately obvious), whereas with information that is vague, it is difficult to form any interpretation at the desired level of specificity.\n\nContext may play a role in resolving ambiguity. For example, the same piece of information may be ambiguous in one context and unambiguous in another.\n\nThe lexical ambiguity of a word or phrase pertains to its having more than one meaning in the language to which the word belongs. \"Meaning\" here refers to whatever should be captured by a good dictionary. For instance, the word \"bank\" has several distinct lexical definitions, including \"financial institution\" and \"edge of a river\". Or consider \"apothecary\". One could say \"I bought herbs from the apothecary\". This could mean one actually spoke to the apothecary (pharmacist) or went to the apothecary (pharmacy).\n\nThe context in which an ambiguous word is used often makes it evident which of the meanings is intended. If, for instance, someone says \"I buried $100 in the bank\", most people would not think someone used a shovel to dig in the mud. However, some linguistic contexts do not provide sufficient information to disambiguate a used word.\n\nLexical ambiguity can be addressed by algorithmic methods that automatically associate the appropriate meaning with a word in context, a task referred to as word sense disambiguation.\n\nThe use of multi-defined words requires the author or speaker to clarify their context, and sometimes elaborate on their specific intended meaning (in which case, a less ambiguous term should have been used). The goal of clear concise communication is that the receiver(s) have no misunderstanding about what was meant to be conveyed. An exception to this could include a politician whose \"weasel words\" and obfuscation are necessary to gain support from multiple constituents with mutually exclusive conflicting desires from their candidate of choice. Ambiguity is a powerful tool of political science.\n\nMore problematic are words whose senses express closely related concepts. \"Good\", for example, can mean \"useful\" or \"functional\" (\"That's a good hammer\"), \"exemplary\" (\"She's a good student\"), \"pleasing\" (\"This is good soup\"), \"moral\" (\"a good person\" versus \"the lesson to be learned from a story\"), \"righteous\", etc. \" I have a good daughter\" is not clear about which sense is intended. The various ways to apply prefixes and suffixes can also create ambiguity (\"unlockable\" can mean \"capable of being unlocked\" or \"impossible to lock\").\n\nSyntactic ambiguity arises when a sentence can have two (or more) different meanings because of the structure of the sentence—its syntax. This is often due to a modifying expression, such as a prepositional phrase, the application of which is unclear. \"He ate the cookies on the couch\", for example, could mean that he ate those cookies that were on the couch (as opposed to those that were on the table), or it could mean that he was sitting on the couch when he ate the cookies. \"To get in, you will need an entrance fee of $10 or your voucher and your drivers' license.\" This could mean that you need EITHER ten dollars OR BOTH your voucher and your license. Or it could mean that you need your license AND you need EITHER ten dollars OR a voucher. Only rewriting the sentence, or placing appropriate punctuation can resolve a syntactic ambiguity.\nFor the notion of, and theoretic results about, syntactic ambiguity in artificial, formal languages (such as computer programming languages), see Ambiguous grammar.\n\nSpoken language can contain many more types of ambiguities which are called phonological ambiguities, where there is more than one way to compose a set of sounds into words. For example, \"ice cream\" and \"I scream\". Such ambiguity is generally resolved according to the context. A mishearing of such, based on incorrectly resolved ambiguity, is called a mondegreen.\n\nSemantic ambiguity happens when a sentence contains an ambiguous word or phrase—a word or phrase that has more than one meaning. In \"We saw her duck\" (example due to Richard Nordquist), the word \"duck\" can refer either\n\nFor example, \"You could do with a new automobile. How about a test drive?\" The clause \"You could do with\" presents a statement with such wide possible interpretation as to be essentially meaningless. Lexical ambiguity is contrasted with semantic ambiguity. The former represents a choice between a finite number of known and meaningful context-dependent interpretations. The latter represents a choice between any number of possible interpretations, none of which may have a standard agreed-upon meaning. This form of ambiguity is closely related to vagueness.\n\nLinguistic ambiguity can be a problem in law, because the interpretation of written documents and oral agreements is often of paramount importance.\n\nPhilosophers (and other users of logic) spend a lot of time and effort searching for and removing (or intentionally adding) ambiguity in arguments because it can lead to incorrect conclusions and can be used to deliberately conceal bad arguments. For example, a politician might say, \"I oppose taxes which hinder economic growth\", an example of a glittering generality. Some will think he opposes taxes in general because they hinder economic growth. Others may think he opposes only those taxes that he believes will hinder economic growth. In writing, the sentence can be rewritten to reduce possible misinterpretation, either by adding a comma after \"taxes\" (to convey the first sense) or by changing \"which\" to \"that\" (to convey the second sense) or by rewriting it in other ways. The devious politician hopes that each constituent will interpret the statement in the most desirable way, and think the politician supports everyone's opinion. However, the opposite can also be true – an opponent can turn a positive statement into a bad one if the speaker uses ambiguity (intentionally or not). The logical fallacies of amphiboly and equivocation rely heavily on the use of ambiguous words and phrases.\n\nIn continental philosophy (particularly phenomenology and existentialism), there is much greater tolerance of ambiguity, as it is generally seen as an integral part of the human condition. Martin Heidegger argued that the relation between the subject and object is ambiguous, as is the relation of mind and body, and part and whole.[3] In Heidegger's phenomenology, Dasein is always in a meaningful world, but there is always an underlying background for every instance of signification. Thus, although some things may be certain, they have little to do with Dasein's sense of care and existential anxiety, e.g., in the face of death. In calling his work Being and Nothingness an \"essay in phenomenological ontology\" Jean-Paul Sartre follows Heidegger in defining the human essence as ambiguous, or relating fundamentally to such ambiguity. Simone de Beauvoir tries to base an ethics on Heidegger's and Sartre's writings (The Ethics of Ambiguity), where she highlights the need to grapple with ambiguity: \"as long as philosophers and they [men] have thought, most of them have tried to mask it...And the ethics which they have proposed to their disciples have always pursued the same goal. It has been a matter of eliminating the ambiguity by making oneself pure inwardness or pure externality, by escaping from the sensible world or being engulfed by it, by yielding to eternity or enclosing oneself in the pure moment.\" Ethics cannot be based on the authoritative certainty given by mathematics and logic, or prescribed directly from the empirical findings of science. She states: \"Since we do not succeed in fleeing it, let us, therefore, try to look the truth in the face. Let us try to assume our fundamental ambiguity. It is in the knowledge of the genuine conditions of our life that we must draw our strength to live and our reason for acting\". Other continental philosophers suggest that concepts such as life, nature, and sex are ambiguous. Corey Anton has argued that we cannot be certain what is separate from or unified with something else: language, he asserts, divides what is not, in fact, separate. Following Ernest Becker, he argues that the desire to 'authoritatively disambiguate' the world and existence have led to numerous ideologies and historical events such as genocide. On this basis, he argues that ethics must focus on 'dialectically integrating opposites' and balancing tension, rather than seeking a priori validation or certainty. Like the existentialists and phenomenologists, he sees the ambiguity of life as the basis of creativity.\n\nIn literature and rhetoric, ambiguity can be a useful tool. Groucho Marx's classic joke depends on a grammatical ambiguity for its humor, for example: \"Last night I shot an elephant in my pajamas. How he got in my pajamas, I'll never know\". Songs and poetry often rely on ambiguous words for artistic effect, as in the song title \"Don't It Make My Brown Eyes Blue\" (where \"blue\" can refer to the color, or to sadness).\n\nIn narrative, ambiguity can be introduced in several ways: motive, plot, character. F. Scott Fitzgerald uses the latter type of ambiguity with notable effect in his novel \"The Great Gatsby\".\n\nChristianity and Judaism employ the concept of paradox synonymously with 'ambiguity'. Many Christians and Jews endorse Rudolf Otto's description of the sacred as 'mysterium tremendum et fascinans', the awe-inspiring mystery which fascinates humans.[dubious – discuss] The orthodox Catholic writer G. K. Chesterton regularly employed paradox to tease out the meanings in common concepts which he found ambiguous, or to reveal meaning often overlooked or forgotten in common phrases. (The title of one of his most famous books, Orthodoxy, itself employing such a paradox.)\n\nMetonymy involves the use of the name of a subcomponent part as an abbreviation, or jargon, for the name of the whole object (for example \"wheels\" to refer to a car, or \"flowers\" to refer to beautiful offspring, an entire plant, or a collection of blooming plants). In modern vocabulary critical semiotics,[9] metonymy encompasses any potentially ambiguous word substitution that is based on contextual contiguity (located close together), or a function or process that an object performs, such as \"sweet ride\" to refer to a nice car. Metonym miscommunication is considered a primary mechanism of linguistic humour.\n\nIn music, pieces or sections which confound expectations and may be or are interpreted simultaneously in different ways are ambiguous, such as some polytonality, polymeter, other ambiguous meters or rhythms, and ambiguous phrasing, or (Stein 2005, p. 79) any aspect of music. The music of Africa is often purposely ambiguous. To quote Sir Donald Francis Tovey (1935, p. 195), \"Theorists are apt to vex themselves with vain efforts to remove uncertainty just where it has a high aesthetic value.\"\n\nIn visual art, certain images are visually ambiguous, such as the Necker cube, which can be interpreted in two ways. Perceptions of such objects remain stable for a time, then may flip, a phenomenon called multistable perception.\nThe opposite of such ambiguous images are impossible objects.\n\nPictures or photographs may also be ambiguous at the semantic level: the visual image is unambiguous, but the meaning and narrative may be ambiguous: is a certain facial expression one of excitement or fear, for instance?\n\nSome languages have been created with the intention of avoiding ambiguity, especially lexical ambiguity. Lojban and Loglan are two related languages which have been created for this, focusing chiefly on syntactic ambiguity as well. The languages can be both spoken and written. These languages are intended to provide a greater technical precision over big natural languages, although historically, such attempts at language improvement have been criticized. Languages composed from many diverse sources contain much ambiguity and inconsistency. The many exceptions to syntax and semantic rules are time-consuming and difficult to learn.\n\nIn computer science, the SI prefixes kilo-, mega- and giga- were historically used in certain contexts to mean either the first three powers of 1024 (1024, 1024 and 1024) contrary to the metric system in which these units unambiguously mean one thousand, one million, and one billion. This usage is particularly prevalent with electronic memory devices (e.g. DRAM) addressed directly by a binary machine register where a decimal interpretation makes no practical sense.\n\nSubsequently, the Ki, Mi, and Gi prefixes were introduced so that binary prefixes could be written explicitly, also rendering k, M, and G \"unambiguous\" in texts conforming to the new standard — this led to a \"new\" ambiguity in engineering documents lacking outward trace of the binary prefixes (necessarily indicating the new style) as to whether the usage of k, M, and G remains ambiguous (old style) or not (new style). Note also that 1 M (where M is ambiguously 1,000,000 or 1,048,576) is \"less\" uncertain than the engineering value 1.0e6 (defined to designate the interval 950,000 to 1,050,000), and that as non-volatile storage devices began to commonly exceed 1 GB in capacity (where the ambiguity begins to routinely impact the second significant digit), GB and TB almost always mean 10 and 10 bytes.\n\nMathematical notation, widely used in physics and other sciences, avoids many ambiguities compared to expression in natural language. However, for various reasons, several lexical, syntactic and semantic ambiguities remain.\n\nThe ambiguity in the style of writing a function should not be confused with a multivalued function, which can (and should) be defined in a deterministic and unambiguous way. Several special functions still do not have established notations. Usually, the conversion to another notation requires to scale the argument or the resulting value; sometimes, the same name of the function is used, causing confusions. Examples of such underestablished functions:\n\nAmbiguous expressions often appear in physical and mathematical texts.\nIt is common practice to omit multiplication signs in mathematical expressions. Also, it is common to give the same name to a variable and a function, for example, formula_1. Then, if one sees formula_2, there is no way to distinguish whether it means formula_1 multiplied by formula_4, or function formula_5 evaluated at argument equal to formula_4. In each case of use of such notations, the reader is supposed to be able to perform the deduction and reveal the true meaning.\n\nCreators of algorithmic languages try to avoid ambiguities. Many algorithmic languages (C++ and Fortran) require the character * as symbol of multiplication. The Wolfram Language used in Mathematica allows the user to omit the multiplication symbol, but requires square brackets to indicate the argument of a function; square brackets are not allowed for grouping of expressions. Fortran, in addition, does not allow use of the same name (identifier) for different objects, for example, function and variable; in particular, the expression f=f(x) is qualified as an error.\n\nThe order of operations may depend on the context. In most programming languages, the operations of division and multiplication have equal priority and are executed from left to right. Until the last century, many editorials assumed that multiplication is performed first, for example, formula_7 is interpreted as formula_8; in this case, the insertion of parentheses is required when translating the formulas to an algorithmic language. In addition, it is common to write an argument of a function without parenthesis, which also may lead to ambiguity.\nSometimes, one uses \"italics\" letters to denote elementary functions.\nIn the scientific journal style, the expression\nformula_9\nmeans\nproduct of variables\nformula_10,\nformula_11,\nformula_12 and\nformula_13, although in a slideshow, it may mean formula_14.\n\nA comma in subscripts and superscripts sometimes is omitted; it is also ambiguous notation.\nIf it is written formula_15, the reader should guess from the context, does it mean a single-index object, evaluated while the subscript is equal to product of variables\nformula_16, formula_12 and formula_18, or it is indication to a trivalent tensor.\nThe writing of formula_15 instead of formula_20 may mean that the writer either is stretched in space (for example, to reduce the publication fees) or aims to increase number of publications without considering readers. The same may apply to any other use of ambiguous notations.\n\nSubscripts are also used to denote the argument to a function, as in formula_21.\nformula_22, which could be understood to mean either formula_23 or formula_24. In addition, formula_25 may mean formula_26, as formula_27 means formula_28 (see tetration).\n\nformula_29, which by convention means formula_30, though it might be thought to mean formula_31, since formula_32 means formula_33.\n\nformula_34, which arguably should mean formula_35 but would commonly be understood to mean formula_36 .\n\nIt is common to define the coherent states in quantum optics with formula_37 and states with fixed number of photons with formula_38. Then, there is an \"unwritten rule\": the state is coherent if there are more Greek characters than Latin characters in the argument, and formula_39photon state if the Latin characters dominate. The ambiguity becomes even worse, if formula_40 is used for the states with certain value of the coordinate, and formula_41 means the state with certain value of the momentum, which may be used in books on quantum mechanics. Such ambiguities easily lead to confusions, especially if some normalized adimensional, dimensionless variables are used. Expression formula_42 may mean a state with single photon, or the coherent state with mean amplitude equal to 1, or state with momentum equal to unity, and so on. The reader is supposed to guess from the context.\n\nSome physical quantities do not yet have established notations; their value (and sometimes even dimension, as in the case of the Einstein coefficients), depends on the system of notations. Many terms are ambiguous. Each use of an ambiguous term should be preceded by the definition, suitable for a specific case. Just like Ludwig Wittgenstein states in Tractatus Logico-Philosophicus: \"... Only in the context of a proposition has a name meaning.\"\n\nA highly confusing term is \"gain\". For example, the sentence \"the gain of a system should be doubled\", without context, means close to nothing.\nIt may mean that the ratio of the output voltage of an electric circuit to the input voltage should be doubled.\nIt may mean that the ratio of the output power of an electric or optical circuit to the input power should be doubled.\nIt may mean that the gain of the laser medium should be doubled, for example, doubling the population of the upper laser level in a quasi-two level system (assuming negligible absorption of the ground-state).\n\nThe term \"intensity\" is ambiguous when applied to light. The term can refer to any of irradiance, luminous intensity, radiant intensity, or radiance, depending on the background of the person using the term.\n\nAlso, confusions may be related with the use of atomic percent as measure of concentration of a dopant, or resolution of an imaging system, as measure of the size of the smallest detail which still can be resolved at the background of statistical noise. See also Accuracy and precision and its talk.\n\nThe Berry paradox arises as a result of systematic ambiguity in the meaning of terms such as \"definable\" or \"nameable\". Terms of this kind give rise to vicious circle fallacies. Other terms with this type of ambiguity are: satisfiable, true, false, function, property, class, relation, cardinal, and ordinal.\n\nIn mathematics and logic, ambiguity can be considered to be an instance of the logical concept of underdetermination—for example, formula_43 leaves open what the value of \"X\" is—while its opposite is a self-contradiction, also called inconsistency, paradoxicalness, or oxymoron, or in mathematics an inconsistent system—such as formula_44, which has no solution.\n\nLogical ambiguity and self-contradiction is analogous to visual ambiguity and impossible objects, such as the Necker cube and impossible cube, or many of the drawings of M. C. Escher.\n\n\n\n"}
{"id": "679", "url": "https://en.wikipedia.org/wiki?curid=679", "title": "Animal (disambiguation)", "text": "Animal (disambiguation)\n\nAn animal is a multicellular, eukaryotic organism of the kingdom Animalia or Metazoa.\n\nAnimal or Animals or The Animal may also refer to:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "680", "url": "https://en.wikipedia.org/wiki?curid=680", "title": "Aardvark", "text": "Aardvark\n\nThe aardvark ( ; \"Orycteropus afer\") is a medium-sized, burrowing, nocturnal mammal native to Africa. It is the only living species of the order Tubulidentata, although other prehistoric species and genera of Tubulidentata are known. Unlike other insectivores, it has a long pig-like snout, which is used to sniff out food. It roams over most of the southern two-thirds of the African continent, avoiding areas that are mainly rocky. A nocturnal feeder, it subsists on ants and termites, which it will dig out of their hills using its sharp claws and powerful legs. It also digs to create burrows in which to live and rear its young. It receives a \"least concern\" rating from the IUCN, although its numbers seem to be decreasing.\n\nThe aardvark is sometimes colloquially called \"African ant bear\", \"anteater\" (not to be confused with the South American anteater), or the \"Cape anteater\" after the Cape of Good Hope. The name \"aardvark\" () comes from earlier Afrikaans (erdvark) and means \"earth pig\" or \"ground pig\" (\"aarde\": earth/ground, \"vark\": pig), because of its burrowing habits (similar origin to the name groundhog). The name \"Orycteropus\" means burrowing foot, and the name \"afer\" refers to Africa. The name of the aardvarks's order, \"Tubulidentata,\" comes from the tubule-style teeth.\n\nThe aardvark is not closely related to the pig; rather, it is the sole extant representative of the obscure mammalian order Tubulidentata, in which it is usually considered to form one variable species of the genus \"Orycteropus\", the sole surviving genus in the family Orycteropodidae. The aardvark is not closely related to the South American anteater, despite sharing some characteristics and a superficial resemblance. The similarities are based on convergent evolution. The closest living relatives of the aardvark are the elephant shrews, tenrecs and golden moles. Along with the sirenians, hyraxes, elephants, and their extinct relatives, these animals form the superorder Afrotheria. Studies of the brain have shown the similarities with Condylarthra, and given the clade's status as a wastebasket taxon it may mean some species traditionally classified as \"condylarths\" are actually stem-aardvarks.\n\nBased on fossils, Bryan Patterson has concluded that early relatives of the aardvark appeared in Africa around the end of the Paleocene. The ptolemaiidans, a mysterious clade of mammals with uncertain affinities, may actually be stem-aardvarks, either as a sister clade to Tubulidentata or as a grade leading to true tubulidentates.\n\nThe first unambiguous tubulidentate was probably \"Myorycteropus africanus\" from Kenyan Miocene deposits. The earliest example from the \"Orycteropus\" genus was the \"Orycteropus mauritanicus\" found in Algeria in deposits from the middle Miocene, with an equally aged version found in Kenya. Fossils from the aardvark have been dated to 5 million years, and have been located throughout Europe and the Near East.\n\nThe mysterious Pleistocene \"Plesiorycteropus\" from Madagascar was originally thought to be a tubulidentate that was descended from ancestors that entered the island during the Eocene. However, a number of subtle anatomical differences coupled with recent molecular evidence now lead researchers to believe that \"Plesiorycteropus\" is a relative of golden moles and tenrecs that achieved an aardvark-like appearance and ecological niche through convergent evolution.\n\nThe aardvark has seventeen poorly defined subspecies listed:\n\n\nThe 1911 Encyclopædia Britannica also mentions \"O. a. capensis\" or Cape ant-bear from South Africa.\n\nThe aardvark is vaguely pig-like in appearance. Its body is stout with a prominently arched back and is sparsely covered with coarse hairs. The limbs are of moderate length, with the rear legs being longer than the forelegs. The front feet have lost the pollex (or 'thumb'), resulting in four toes, while the rear feet have all five toes. Each toe bears a large, robust nail which is somewhat flattened and shovel-like, and appears to be intermediate between a claw and a hoof. Whereas the aardvark is considered digitigrade, it appears at time to be plantigrade. This confusion happens because when it squats it stands on its soles.\n\nAn aardvark's weight is typically between . An aardvark's length is usually between , and can reach lengths of when its tail (which can be up to ) is taken into account. It is tall at the shoulder, and has a girth of about . It is the largest member of the proposed clade Afroinsectiphilia. The aardvark is pale yellowish-gray in color and often stained reddish-brown by soil. The aardvark's coat is thin, and the animal's primary protection is its tough skin. Its hair is short on its head and tail; however its legs tend to have longer hair. The hair on the majority of its body is grouped in clusters of 3-4 hairs. The hair surrounding its nostrils is dense to help filter particulate matter out as it digs. Its tail is very thick at the base and gradually tapers.\n\nThe greatly elongated head is set on a short, thick neck, and the end of the snout bears a disc, which houses the nostrils. It contains a thin but complete zygomatic arch. The head of the aardvark contains many unique and different features. One of the most distinctive characteristics of the Tubulidentata is their teeth. Instead of having a pulp cavity, each tooth has a cluster of thin, hexagonal, upright, parallel tubes of vasodentin (a modified form of dentine), with individual pulp canals, held together by cementum. The number of columns is dependent on the size of the tooth, with the largest having about 1,500. The teeth have no enamel coating and are worn away and regrow continuously. The aardvark is born with conventional incisors and canines at the front of the jaw, which fall out and are not replaced. Adult aardvarks have only cheek teeth at the back of the jaw, and have a dental formula of: These remaining teeth are peg-like and rootless and are of unique composition. The teeth consist of 14 upper and 12 lower jaw molars. The nasal area of the aardvark is another unique area, as it contains ten nasal conchae, more than any other placental mammal.\n\nThe sides of the nostrils are thick with hair. The tip of the snout is highly mobile and is moved by modified mimetic muscles. The fleshy dividing tissue between its nostrils probably has sensory functions, but it is uncertain whether they are olfactory or vibratory in nature. Its nose is made up of more turbinate bones than any other mammal, with between 9 and 11, compared to dogs with 4 to 5. With a large quantity of turbinate bones, the aardvark has more space for the moist epithelium, which is the location of the olfactory bulb. The nose contains nine olfactory bulbs, more than any other mammal. Its keen sense of smell is not just from the quantity of bulbs in the nose but also in the development of the brain, as its olfactory lobe is very developed. The snout resembles an elongated pig snout. The mouth is small and tubular, typical of species that feed on ants and termites. The aardvark has a long, thin, snakelike, protruding tongue (as much as long) and elaborate structures supporting a keen sense of smell. The ears, which are very effective, are disproportionately long, about long. The eyes are small for its head, and consist only of rods.\n\nThe aardvark's stomach has a muscular pyloric area that acts as a gizzard to grind swallowed food up, thereby rendering chewing unnecessary. Its cecum is large. Both sexes emit a strong smelling secretion from an anal gland. Its salivary glands are highly developed and almost completely ring the neck; their output is what causes the tongue to maintain its tackiness. The female has two pairs of teats in the inguinal region.\n\nGenetically speaking, the aardvark is a living fossil, as its chromosomes are highly conserved, reflecting much of the early eutherian arrangement before the divergence of the major modern taxa.\n\nAardvarks are found in sub-Saharan Africa, where suitable habitat (savannas, grasslands, woodlands and bushland) and food (i.e., ants and termites) is available. They spend the daylight hours in dark underground burrows to avoid the heat of the day. The only major habitat that they are not present in is swamp forest, as the high water table precludes digging to a sufficient depth. They also avoid terrain rocky enough to cause problems with digging. They have been documented as high as in Ethiopia. They are present throughout sub-Saharan Africa all the way to South Africa with few exceptions. These exceptions include the coastal areas of Namibia, Ivory Coast, and Ghana. They are not found in Madagascar.\n\nAardvarks live for up to 23 years in captivity. Its keen hearing warns it of predators: lions, leopards, cheetahs, hunting dogs, hyenas, and pythons. Some humans also hunt aardvarks for meat. Aardvarks can dig fast or run in zigzag fashion to elude enemies, but if all else fails, they will strike with their claws, tail and shoulders, sometimes flipping onto their backs lying motionless except to lash out with all four feet. They are capable of causing substantial damage to unprotected areas of an attacker. They will also dig to escape as they can, when pressed, dig extremely quickly. Their thick skin also protects them to some extent.\n\nThe aardvark is nocturnal and is a solitary creature that feeds almost exclusively on ants and termites (formivore); the only fruit eaten by aardvarks is the aardvark cucumber. In fact, the cucumber and the aardvark have a symbiotic relationship as they eat the subterranean fruit, then defecate the seeds near their burrows, which then grow rapidly due to the loose soil and fertile nature of the area. The time spent in the intestine of the aardvark helps the fertility of the seed, and the fruit provides needed moisture for the aardvark. They avoid eating the African driver ant and red ants. Due to their stringent diet requirements, they require a large range to survive. An aardvark emerges from its burrow in the late afternoon or shortly after sunset, and forages over a considerable home range encompassing . While foraging for food, the aardvark will keep its nose to the ground and its ears pointed forward, which indicates that both smell and hearing are involved in the search for food. They zig-zag as they forage and will usually not repeat a route for 5–8 days as they appear to allow time for the termite nests to recover before feeding on it again.\n\nDuring a foraging period, they will stop and dig a \"V\" shaped trench with their forefeet and then sniff it profusely as a means to explore their location. When a concentration of ants or termites is detected, the aardvark digs into it with its powerful front legs, keeping its long ears upright to listen for predators, and takes up an astonishing number of insects with its long, sticky tongue—as many as 50,000 in one night have been recorded. Its claws enable it to dig through the extremely hard crust of a termite or ant mound quickly. It avoids inhaling the dust by sealing the nostrils. When successful, the aardvark's long (up to ) tongue licks up the insects; the termites' biting, or the ants' stinging attacks are rendered futile by the tough skin. After an aardvark visit at a termite mound, other animals will visit to pick up all the leftovers. Termite mounds alone don't provide enough food for the aardvark, so they look for termites that are on the move. When these insects move, they can form columns long and these tend to provide easy pickings with little effort exerted by the aardvark. These columns are more common in areas of livestock or other hoofed animals. The trampled grass and dung attract termites from Odontotermes, Microtermes, and Pseudacanthotermes genera.\n\nOn a nightly basis they tend to be more active during the first portion of the night time (20:00-00:00); however, they don't seem to prefer bright or dark nights over the other. During adverse weather or if disturbed they will retreat to their burrow systems. They cover between per night; however, some studies have shown that they may traverse as far as in a night.\n\nThe aardvark is a rather quiet animal. However, it does make soft grunting sounds as it forages and loud grunts as it makes for its tunnel entrance. It makes a bleating sound if frightened. When it is threatened it will make for one of its burrows. If one is not close it will dig a new one rapidly. This new one will be short and require the aardvark to back out when the coast is clear.\n\nThe aardvark is known to be a good swimmer and has been witnessed successfully swimming in strong currents. It can dig a yard of tunnel in about five minutes, but otherwise moves fairly slowly.\n\nWhen leaving the burrow at night, they pause at the entrance for about ten minutes, sniffing and listening. After this period of watchfulness, it will bound out and within seconds it will be away. It will then pause, prick its ears, twisting its head to listen, then jump and move off to start foraging.\n\nAside from digging out ants and termites, the aardvark also excavates burrows in which to live; of which they generally fall into three categories: burrows made while foraging, refuge and resting location, and permanent homes. Temporary sites are scattered around the home range and are used as refuges, while the main burrow is also used for breeding. Main burrows can be deep and extensive, have several entrances and can be as long as . These burrows can be large enough for a man to enter. The aardvark changes the layout of its home burrow regularly, and periodically moves on and makes a new one. The old burrows are an important part of the African wildlife scene. As they are vacated, then they are inhabited by smaller animals like the African wild dog, ant-eating chat, \"Nycteris thebaica\" and warthogs. Other animals that use them are hares, mongooses, hyenas, owls, pythons, and lizards. Without these refuges many animals would die during wildfire season. Only mothers and young share burrows; however, the aardvark is known to live in small family groups or as a solitary creature. If attacked in the tunnel, it will escape by digging out of the tunnel thereby placing the fresh fill between it and its predator, or if it decides to fight it will roll onto its back, and attack with its claws. The aardvark has been known to sleep in a recently excavated ant nest, which also serves as protection from its predators.\n\nAardvarks pair only during the breeding season; after a gestation period of seven months, one cub weighing around is born during May–July. When born, the young has flaccid ears and many wrinkles. When nursing, it will nurse off each teat in succession. After two weeks, the folds of skin disappear and after three, the ears can be held upright. After 5–6 weeks, body hair starts growing. It is able to leave the burrow to accompany its mother after only two weeks, is eating termites at 9 weeks, and is weaned between three months and 16 weeks. At six months of age, it is able to dig its own burrows, but it will often remain with the mother until the next mating season, and is sexually mature from approximately two years of age.\n\nAardvarks were thought to have declining numbers, however, this is possibly due to the fact that they are not readily seen. There are no definitive counts because of their nocturnal and secretive habits; however, their numbers seem to be stable overall. They are not considered common anywhere in Africa, but due to their large range, they maintain sufficient numbers. There may be a slight decrease in numbers in eastern, northern, and western Africa. Southern African numbers are not decreasing. It receives an official designation from the IUCN as least concern. However, they are a species in a precarious situation, as they are so dependent on such specific food; therefore if a problem arises with the abundance of termites, the species as a whole would be affected drastically.\n\nAardvarks handle captivity well. The first zoo to have one was London Zoo in 1869, which had an animal from South Africa.\n\nIn African folklore, the aardvark is much admired because of its diligent quest for food and its fearless response to soldier ants. Hausa magicians make a charm from the heart, skin, forehead, and nails of the aardvark, which they then proceed to pound together with the root of a certain tree. Wrapped in a piece of skin and worn on the chest, the charm is said to give the owner the ability to pass through walls or roofs at night. The charm is said to be used by burglars and those seeking to visit young girls without their parents' permission. Also, some tribes, such as the Margbetu, Ayanda, and Logo, will use aardvark teeth to make bracelets, which are regarded as good luck charms. The meat, which has a resemblance to pork, is eaten in certain cultures.\n\nThe Egyptian god Set is usually depicted with the head of an unidentified animal, whose similarity to an aardvark has been noted in scholarship.\n\nThe titular character of \"Arthur\", an animated television series for children based on a book series and produced by WGBH, shown in more than 180 countries, is an aardvark.\n\nAn aardvark features as the antagonist in the cartoon \"The Ant and the Aardvark\" as well as in the Canadian animated series \"The Raccoons\".\n\nIn the military, the Air Force supersonic fighter-bomber F-111/FB-111 was nicknamed the Aardvark because of its long nose resembling the animal. It also had similarities with its nocturnal missions flown at a very low level employing ordnance that could penetrate deep into the ground. In the US Navy, the squadron VF-114 was nicknamed the Aardvarks, flying F-4s and then F-14s. The squadron mascot was adapted from the animal in the comic strip \"B.C.\", which the F-4 was said to resemble.\n\n\n"}
{"id": "681", "url": "https://en.wikipedia.org/wiki?curid=681", "title": "Aardwolf", "text": "Aardwolf\n\nThe aardwolf (\"Proteles cristata\") is a small, insectivorous mammal, native to East and Southern Africa. Its name means \"earth wolf\" in Afrikaans and Dutch. It is also called \"maanhaar jackal\" (Afrikaans for \"mane jackal\") or \"civet hyena\", based on its habit of secreting substances from its anal gland, a characteristic shared with the civet. The aardwolf is in the same family as the hyena. Unlike many of its relatives in the order Carnivora, the aardwolf does not hunt large animals. It eats insects, mainly termites – one aardwolf can eat about 250,000 termites during a single night, using its long, sticky tongue to capture them. The aardwolf lives in the shrublands of eastern and southern Africa – open lands covered with stunted trees and shrubs. It is nocturnal, resting in burrows during the day and emerging at night to seek food. Its diet consists mainly of termites and insect larvae.\n\nThe aardwolf is the only surviving species in the mammalian subfamily Protelinae. There is disagreement as to whether the species is monotypic. or can be divided into subspecies \"P. c. cristatus\" of Southern Africa and \"P. c. septentrionalis\" of East Africa. Recent studies have shown that the aardwolf probably broke away from the rest of the hyena family early on; however, how early is still unclear, as the fossil record and genetic studies disagree by 10 million years.\n\nThe aardwolf is generally classified with the Hyaenidae, though it was formerly placed into the family Protelidae. Early on, scientists felt that it was merely mimicking the striped hyena, which subsequently led to the creation of Protelidae.\n\nThe generic name \"proteles\" comes from two words both of Greek origin, \"protos\" and \"teleos\" which combined means \"complete in front\" based on the fact that they have five toes on their front feet and four on the rear. The specific name, \"cristatus\", comes from Latin and means \"provided with a comb\", relating to their mane.\n\nThe aardwolf resembles a very thin striped hyena, but with a more slender muzzle, black vertical stripes on a coat of yellowish fur, and a long, distinct mane down the midline of the neck and back. It also has one or two diagonal stripes down the fore- and hind-quarters, along with several stripes on its legs. The mane is raised during confrontations to make the aardwolf appear larger. It is missing the throat spot that others in the family have. Its lower leg (from the knee down) is all black, and its tail is bushy with a black tip. The aardwolf is about long, excluding its bushy tail, which is about long, and stands about tall at the shoulders. An adult aardwolf weighs approximately , sometimes reaching . The aardwolves in the south of the continent tend to be smaller (about ), whereas the eastern version weighs more (around ). The front feet have five toes each, unlike the four-toed hyena. The teeth and skull are similar to those of other hyenas, though smaller, and its cheek teeth are specialised for eating insects. It does still have canines, but, unlike other hyenas, these teeth are used primarily for fighting and defense. Its ears, which are large, are very similar to those of the striped hyena.\n\nAs an aardwolf ages, it will normally lose some of its teeth, though this has little impact on its feeding habits due to the softness of the insects that it eats.\nAardwolves live in open, dry plains and bushland, avoiding mountainous areas. Due to their specific food requirements, they are only found in regions where termites of the family Hodotermitidae occur. Termites of this family depend on dead and withered grass and are most populous in heavily grazed grasslands and savannahs, including farmland. For most of the year, aardwolves spend time in shared territories consisting of up to a dozen dens, which are occupied for six weeks at a time.\n\nThere are two distinct populations: one in Southern Africa, and another in East and Northeast Africa. The species does not occur in the intermediary miombo forests.\n\nAn adult pair, along with their most recent offspring, occupies a territory of .\n\nAardwolves are shy and nocturnal, sleeping in underground burrows by day. They will, on occasion during the winter, become diurnal feeders. This happens during the coldest periods as they then stay in at night to conserve heat.\n\nThey have often been mistaken for solitary animals. In fact, they live as monogamous pairs with their young. If their territory is infringed upon, they will chase the intruder up to or to the border. If the intruder is caught, which rarely happens, a fight will occur, which is accompanied by soft clucking, hoarse barking, and a type of roar. The majority of incursions occur during mating season, when they can occur once or twice per week. When food is scarce, the stringent territorial system may be abandoned and as many as three pairs may occupy a \"single territory\".\n\nThe territory is marked by both sexes, as they both have developed anal glands from which they extrude a black substance that is smeared on rocks or grass stalks in -long streaks. Aardwolves also have scent glands on the forefoot and penile pad. They often mark near termite mounds within their territory every 20 minutes or so. If they are patrolling their territorial boundaries, the marking frequency increases drastically, to once every . At this rate, an individual may mark 60 marks per hour, and upwards of 200 per night.\n\nAn aardwolf pair may have up to 10 dens, and numerous middens, within their territory. When they deposit feces at their middens, they dig a small hole and then cover it with sand. Their dens are usually abandoned aardvark, springhare, or porcupine dens, or on occasion they are crevices in rocks. They will also dig their own dens, or enlarge dens started by springhares. They typically will only use one or two dens at a time, rotating through all of their dens every six months. During the summer, they may rest outside their den during the night, and sleep underground during the heat of the day.\n\nAardwolves are not fast runners nor are they particularly adept at fighting off predators. Therefore, when threatened, the aardwolf may attempt to mislead its foe by doubling back on its tracks. If confronted, it may raise its mane in an attempt to appear more menacing. It also emits a foul-smelling liquid from its anal glands.\n\nThe aardwolf feeds primarily on termites and more specifically on \"Trinervitermes\". This genus of termites has different species throughout the aardwolf's range. In East Africa, they eat \"T. bettonianus\", and in central Africa, they eat \"T. rhodesiensis\", and finally in southern Africa, they eat \"T. trinervoides\". Their technique consists of licking them off the ground as opposed to the aardvark, which digs into the mound. They locate their food by sound and also from the scent secreted by the soldier termites. An aardwolf may consume up to 250,000 termites per night using its sticky, long tongue. They do not destroy the termite mound or consume the entire colony, thus ensuring that the termites can rebuild and provide a continuous supply of food. They often memorize the location of such nests and return to them every few months. During certain seasonal events, such as the onset of the rainy season and the cold of midwinter, the primary termites become scarce, so the need for other foods becomes pronounced. During these times, the southern aardwolf will seek out \"Hodotermes mossambicus\", a type of harvester termite active in the afternoon, which explains some of their diurnal behavior in the winter. The eastern aardwolf, during the rainy season, subsists on termites from the genera \"Odontotermes\" and \"Macrotermes\". They are also known to feed on other insects, larvae, eggs, and, some sources say, occasionally small mammals and birds, but these constitute a very small percentage of their total diet. Unlike other hyenas, aardwolves do not scavenge or kill larger animals. Contrary to popular myths, aardwolves do not eat carrion, and if they are seen eating while hunched over a dead carcass, they are actually eating larvae and beetles. Also, contrary to some sources, they do not like meat, unless it is finely ground or cooked for them. The adult aardwolf was formerly assumed to forage in small groups, but more recent research has shown that they are primarily solitary foragers, necessary because of the scarcity of their insect prey. Their primary source, \"Trinervitermes\", forages in small but dense patches of . While foraging, the aardwolf can cover about per hour, which translates to per summer night and per winter night.\n\nThe breeding season varies depending on location, but normally takes place during autumn or spring. In South Africa, breeding occurs in early July. During the breeding season, unpaired male aardwolves search their own territory, as well as others, for a female to mate with. Dominant males also mate opportunistically with the females of less dominant neighboring aardwolves, which can result in conflict between rival males. Dominant males even go a step further and as the breeding season approaches, they make increasingly greater and greater incursions onto weaker males' territories. As the female comes into oestrus, they add pasting to their tricks inside of the other territories, sometimes doing so more in rivals' territories than their own. Females will also, when given the opportunity, mate with the dominant male, which increases the chances of the dominant male guarding \"his\" cubs with her. Copulation lasts between 1 and 4.5 hours. Gestation lasts between 89 and 92 days, producing two to five cubs (most often two or three) during the rainy season (November–December), when termites are more active. They are born with their eyes open, but initially are helpless, and weigh around . The first six to eight weeks are spent in the den with their parents. The male may spend up to six hours a night watching over the cubs while the mother is out looking for food. After three months, they begin supervised foraging, and by four months are normally independent, though they often share a den with their mother until the next breeding season. By the time the next set of cubs is born, the older cubs have moved on. Aardwolves generally achieve sexual maturity at one and a half to two years of age.\n\nThe aardwolf has not seen decreasing numbers and they are relatively widespread throughout eastern Africa. They are not common throughout their range, as they maintain a density of no more than 1 per square kilometer, if the food is good. Because of these factors, the IUCN has rated the aardwolf as least concern. In some areas, they are persecuted by man because of the mistaken belief that they prey on livestock; however, they are actually beneficial to the farmers because they eat termites that are detrimental. In other areas, the farmers have recognized this, but they are still killed, on occasion, for their fur. Dogs and insecticides are also common killers of the aardwolf.\n\nAardwolfs are common sights at zoos. Frankfurt Zoo in Germany was home to the oldest recorded aardwolf in captivity at 18 years and 11 months.\n\n\n"}
{"id": "682", "url": "https://en.wikipedia.org/wiki?curid=682", "title": "Adobe", "text": "Adobe\n\nAdobe (, ; ) is a building material made from earth and often organic material. Adobe means mudbrick in Spanish, but in some English speaking regions of Spanish heritage it refers to any kind of earth construction, as most adobe buildings are similar in appearance to cob and rammed earth buildings. Adobe is among the earliest building materials, and is used throughout the world.\n\nAdobe bricks are most often made into units weighing less than 100 pounds and small enough that they can quickly air dry individually without cracking and be subsequently assembled, with the application of adobe mud, to bond the individual bricks into a structure. Modern methods of construction allow the pouring of whole adobe walls that are reinforced with steel.\n\nIn dry climates, adobe structures are extremely durable, and account for some of the oldest existing buildings in the world. Adobe buildings offer significant advantages due to their greater thermal mass, but they are known to be particularly susceptible to earthquake damage if they are not somehow reinforced. Cases where adobe structures were widely damaged during earthquakes include the 1976 Guatemala earthquake, the 2003 Bam earthquake and the 2010 Chile earthquake.\n\nBuildings made of sun-dried earth are common throughout the world (Middle East, Western Asia, North Africa, West Africa, South America, southwestern North America, Spain, and Eastern Europe.) Adobe had been in use by indigenous peoples of the Americas in the Southwestern United States, Mesoamerica, and the Andes for several thousand years. Puebloan peoples built their adobe structures with handfuls or basketfuls of adobe, until the Spanish introduced them to making bricks. Adobe bricks were used in Spain from the Late Bronze and Iron Ages (eighth century BCE onwards). Its wide use can be attributed to its simplicity of design and manufacture, and economics.\n\nA distinction is sometimes made between the smaller \"adobes\", which are about the size of ordinary baked bricks, and the larger \"adobines\", some of which may be one to two yards (1–2 m) long.\n\nThe word \"adobe\" has existed for around 4000 years with relatively little change in either pronunciation or meaning. The word can be traced from the Middle Egyptian (c. 2000 BC) word \"ɟbt\" \"mudbrick.\" Middle Egyptian evolved into Late Egyptian, Demotic or \"pre-Coptic\", and finally to Coptic (c. 600 BC), where it appeared as τωωβε . This was borrowed into Arabic as \"aṭ-ṭawbu\" or \"aṭ-ṭūbu\", with the definite article \"al-\" attached, \"tuba\", which was assimilated into the Old Spanish language as \"adobe\" , probably via Mozarabic. English borrowed the word from Spanish in the early 18th century.\nIn more modern English usage, the term \"adobe\" has come to include a style of architecture popular in the desert climates of North America, especially in New Mexico.\n\nAn adobe brick is a composite material made of earth mixed with water and an organic material such as straw or dung. The soil composition typically contains sand, silt and clay. Straw is useful in binding the brick together and allowing the brick to dry evenly, thereby preventing cracking due to uneven shrinkage rates through the brick. Dung offers the same advantage. The most desirable soil texture for producing the mud of adobe is 15% clay, 10–30% silt and 55–75% fine sand. Another source quotes 15–25% clay and the remainder sand and coarser particles up to cobbles with no deleterious effect. Modern adobe is stabilized with either emulsified asphalt or Portland cement up to 10% by weight.\n\nNo more than half the clay content should be expansive clays with the remainder non-expansive illite or kaolinite. Too much expansive clay results in uneven drying through the brick resulting in cracking, while too much kaolinite will make a weak brick. Typically the soils of the Southwest United States, where such construction is in use, are an adequate composition.\n\nAdobe walls are load bearing, i.e. they carry their own weight into the foundation rather than by another structure, hence the adobe must have sufficient compressive strength. In the United States, most building codes call for a minimum compressive strength of 300 lbf/in (2.07 newton/mm) for the adobe block. Adobe construction should be designed so as to avoid lateral structural loads that would cause bending loads. The building codes require the building sustain a 1 g lateral acceleration earthquake load. Such an acceleration will cause lateral loads on the walls, resulting in shear and bending and inducing tensile stresses. To withstand such loads, the codes typically call for a tensile modulus of rupture strength of at least 50 lbf/in (0.345 newton/mm) for the finished block.\n\nIn addition to being an inexpensive material with a small resource cost, adobe can serve as a significant heat reservoir due to the thermal properties inherent in the massive walls typical in adobe construction. In climates typified by hot days and cool nights, the high thermal mass of adobe mediates the high and low temperatures of the day, moderating the living space temperature. The massive walls require a large and relatively long input of heat from the sun (radiation) and from the surrounding air (convection) before they warm through to the interior. After the sun sets and the temperature drops, the warm wall will then continue to transfer heat to the interior for several hours due to the time-lag effect. Thus, a well-planned adobe wall of the appropriate thickness is very effective at controlling inside temperature through the wide daily fluctuations typical of desert climates, a factor which has contributed to its longevity as a building material.\n\nThermodynamic material properties are sparsely quoted. The thermal resistance of adobe is quoted as having an R-value of R = 0.41 h ft °F/(Btu in) and a conductivity of 0.57 W/(m K) quoted from another source. A third source provides the following properties: conductivity=0.30 Btu/(h ft °F); heat capacity=0.24 Btu/(lb °F); density=106 lb/ft (1700 kg/m). To determine the total R-value of a wall for example, multiply R by the thickness of the wall. From knowledge of the adobe density, heat capacity and a diffusivity value, the conductivity is found to be k = 0.20 Btu/(h ft °F) or 0.35 W/(m K). The heat capacity is commonly quoted as c = 0.20 Btu/(lb F) or 840 joules/(kg K). The density is 95 lb/ft or 1520 kg/m. The thermal diffusivity is calculated to be 0.0105 ft/h or 2.72x10 m/s.\n\nPoured and puddled adobe (puddled clay, piled earth) today called \"cob\", is made by placing soft adobe in layers, rather than making by individual dried bricks or using a form. Puddle is a general term for a clay or clay and sand based material worked into a dense, plastic state. These are the oldest methods of building with adobe in the Americas until holes in the ground were used as forms and then later wooden forms used to make individual bricks were introduced by the Spanish.\n\nBricks made from adobe are usually made by pressing the mud mixture into an open timber frame. In North America, the brick is typically about in size. The mixture is molded into the frame, which is then is removed after initial setting. After drying for a few hours, the bricks are turned on edge to finish drying. Slow drying in shade reduces cracking.\n\nThe same mixture, without straw, is used to make mortar and often plaster on interior and exterior walls. Some ancient cultures used lime-based cement for the plaster to protect against rain damage.\n\nDepending on the form into which the mixture is pressed, adobe can encompass nearly any shape or size, provided drying is even and the mixture includes reinforcement for larger bricks. Reinforcement can include manure, straw, cement, rebar or wooden posts. Experience has shown straw, cement, or manure added to a standard adobe mixture can all produce a stronger, more crack-resistant brick. A test is done on the soil content first. To do so, a sample of the soil is mixed into a clear container with some water, creating an almost completely saturated liquid. The container is shaken vigorously for one minute. It is then allowed to settle for a day until the soil has settled into layers. Heavier particles settle out first, sand above, silt above that and very fine clay and organic matter will stay in suspension for days. After the water has cleared, percentages of the various particles can be determined. Fifty to 60 percent sand and 35 to 40 percent clay will yield strong bricks. The Cooperative State Research, Education, and Extension Service at New Mexico State University recommends a mix of not more than 1/3 clay, not less than 1/2 sand, and never more than 1/3 silt.\n\nThe ground supporting an adobe structure should be compressed, as the weight of adobe wall is significant and foundation settling may cause cracking of the wall. Footing depth is to below the ground frost level. The footing and stem wall are commonly 24 and 14 inches thick, respectively. Modern construction codes call for the use of reinforcing steel in the footing and stem wall. Adobe bricks are laid by course. Adobe walls usually never rise above two stories as they are load bearing and adobe has low structural strength. When creating window and door openings, a lintel is placed on top of the opening to support the bricks above. Atop the last courses of brick, bond beams made of heavy wood beams or modern reinforced concrete are laid to provide a horizontal bearing plate for the roof beams and to redistribute lateral earthquake loads to shear walls more able to carry the forces. To protect the interior and exterior adobe walls, finishes such as mud plaster, whitewash or stucco can be applied. These protect the adobe wall from water damage, but need to be reapplied periodically. Alternatively, the walls can be finished with other nontraditional plasters that provide longer protection. Bricks made with stabilized adobe generally do not need protection of plasters.\n\nThe traditional adobe roof has been constructed using a mixture of soil/clay, water, sand and organic materials. The mixture was then formed and pressed into wood forms, producing rows of dried earth bricks that would then be laid across a support structure of wood and plastered into place with more adobe.\n\nDepending on the materials available, a roof may be assembled using wood or metal beams to create a framework to begin layering adobe bricks. Depending on the thickness of the adobe bricks, the framework has been preformed using a steel framing and a layering of a metal fencing or wiring over the framework to allow an even load as masses of adobe are spread across the metal fencing like cob and allowed to air dry accordingly. This method was demonstrated with an adobe blend heavily impregnated with cement to allow even drying and prevent cracking.\n\nThe more traditional flat adobe roofs are functional only in dry climates that are not exposed to snow loads. The heaviest wooden beams, called vigas, lie atop the wall. Across the vigas lie smaller members called latillas and upon those brush is then laid. Finally, the adobe layer is applied.\n\nTo construct a flat adobe roof, beams of wood were laid to span the building, the ends of which were attached to the tops of the walls. Once the vigas, latillas and brush are laid, adobe bricks are placed. An adobe roof is often laid with bricks slightly larger in width to ensure a greater expanse is covered when placing the bricks onto the roof. Following each individual brick should be a layer of adobe mortar, recommended to be at least thick to make certain there is ample strength between the brick’s edges and also to provide a relative moisture barrier during rain. \n\nDepending on the materials, adobe roofs can be inherently fire-proof. The construction of a chimney can greatly influence the construction of the roof supports, creating an extra need for care in choosing the materials. The builders can make an adobe chimney by stacking simple adobe bricks in a similar fashion as the surrounding walls\n\nThe largest structure ever made from adobe is the Arg-é Bam built by the Achaemenid Empire. Other large adobe structures are the Huaca del Sol in Peru, with 100 million signed bricks and the \"ciudellas\" of Chan Chan and Tambo Colorado, both in Peru.\n\n\n"}
{"id": "683", "url": "https://en.wikipedia.org/wiki?curid=683", "title": "Adventure", "text": "Adventure\n\nAn adventure is an exciting or unusual experience. It may also be a bold, usually risky undertaking, with an uncertain outcome. Adventures may be activities with some potential for physical danger such as traveling, exploring, skydiving, mountain climbing, scuba diving, river rafting or participating in extreme sports. The term also broadly refers to any enterprise that is potentially fraught with physical, financial or psychological risk, such as a business venture, or other major life undertakings.\n\nThe word adventuress can mean a female who enjoys or partakes in adventures, but (particularly in older literature) it can also have the negative connotation of one who schemes for material advancement by the use her sexuality; a gold digger As an instance of the latter the \"Oxford English Dictionary\" cites \"Our Adventuress had the pickings of a few Feathers from an old Gentleman who fell in Love with her\".\n\nAdventurous experiences create psychological arousal, which can be interpreted as negative (e.g. fear) or positive (e.g. flow). For some people, adventure becomes a major pursuit in and of itself. According to adventurer André Malraux, in his \"La Condition Humaine\" (1933), \"If a man is not ready to risk his life, where is his dignity?\". Similarly, Helen Keller stated that \"Life is either a daring adventure or nothing.\"\n\nOutdoor adventurous activities are typically undertaken for the purposes of recreation or excitement: examples are adventure racing and adventure tourism. Adventurous activities can also lead to gains in knowledge, such as those undertaken by explorers and pioneers – the British adventurer Jason Lewis, for example, uses adventures to draw global sustainability lessons from living within finite environmental constraints on expeditions to share with schoolchildren. Adventure education intentionally uses challenging experiences for learning.\n\nSome of the oldest and most widespread stories in the world are stories of adventure such as Homer's \"The Odyssey\". \n\nThe knight errant was the form the \"adventure seeker\" character took in the late Middle Ages.\n\nThe adventure novel exhibits these \"protagonist on adventurous journey\" characteristics as do many popular feature films, such as \"Star Wars\" and \"Raiders of the Lost Ark\".\n\nAdventure books may have the theme of the hero or main character going to face the wilderness or Mother Nature. Examples include books such as Hatchet or My Side of the Mountain. These books are less about \"questing\", such as in mythology or other adventure novels, but more about surviving on their own, living off the land, gaining new experiences, and becoming closer to the natural world.\nMany adventures are based on the idea of a quest: the hero goes off in pursuit of a reward, whether it be a skill, prize, or perhaps the safety of a person. On the way, the hero must overcome various obstacles. Mythologist Joseph Campbell discussed his notion of the monomyth in his book, \"The Hero with a Thousand Faces\". Campbell proposed that the heroic mythological stories from culture to culture followed a similar underlying pattern, starting with the \"call to adventure\", followed by a hazardous journey, and eventual triumph.\nMany video games are adventure games.\n\nFrom ancient times, travelers and explorers have written about their adventures. Journals which became best-sellers in their day were written, such as Marco Polo's journal \"The Travels of Marco Polo\" or Mark Twain's \"Roughing It\". Others were personal journals, only later published, such as the journals of Lewis and Clark or Captain James Cook's journals. There are also books written by those not directly a part of the adventure in question, such as The Right Stuff by Tom Wolfe, or books written by those participating in the adventure but in a format other than that of a journal, such as Conquistadors of the Useless by Lionel Terray. Documentaries often use the theme of adventure as well.\n\nThere are many sports classified as adventure sports, due to their inherent danger and excitement. Some of these include mountain climbing, skydiving, or other extreme sports.\n\n\n"}
{"id": "689", "url": "https://en.wikipedia.org/wiki?curid=689", "title": "Asia", "text": "Asia\n\nAsia () is Earth's largest and most populous continent, located primarily in the Eastern and Northern Hemispheres and sharing the continental landmass of Eurasia with the continent of Europe and shares the continental landmass of Afro-Eurasia with both Europe and Africa. Asia covers an area of , about 30% of Earth's total land area and 8.7% of the Earth's total surface area. The continent, which has long been home to the majority of the human population, was the site of many of the first civilizations. Asia is notable for not only its overall large size and population, but also dense and large settlements as well as vast barely populated regions within the continent of 4.4 billion people.\n\nIn general terms, Asia is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. The western boundary with Europe is a historical and cultural construct, as there is no clear physical and geographical separation between them. The most commonly accepted boundaries place Asia to the east of the Suez Canal, the Ural River, and the Ural Mountains, and south of the Caucasus Mountains and the Caspian and Black Seas.\n\nChina and India alternated in being the largest economies in the world from 1 to 1800 CE. China was a major economic power and attracted many to the east, and for many the legendary wealth and prosperity of the ancient culture of India personified Asia, attracting European commerce, exploration and colonialism. The accidental discovery of America by Columbus in search for India demonstrates this deep fascination. The Silk Road became the main East-West trading route in the Asian hinterlands while the Straits of Malacca stood as a major sea route. Asia has exhibited economic dynamism (particularly East Asia) as well as robust population growth during the 20th century, but overall population growth has since fallen. Asia was the birthplace of most of the world's mainstream religions including Christianity, Islam, Judaism, Hinduism, Buddhism, Confucianism, Taoism (or Daoism), Jainism, Sikhism, Zoroastranism, as well as many other religions.\n\nGiven its size and diversity, the concept of Asia—a name dating back to classical antiquity—may actually have more to do with human geography than physical geography. Asia varies greatly across and within its regions with regard to ethnic groups, cultures, environments, economics, historical ties and government systems. It also has a mix of many different climates ranging from the equatorial south via the hot desert in the Middle East, temperate areas in the east and the continental centre to vast subarctic and polar areas in Siberia.\n\nThe boundary between Asia and Africa is the Red Sea, the Gulf of Suez, and the Suez Canal. This makes Egypt a transcontinental country, with the Sinai peninsula in Asia and the remainder of the country in Africa.\n\nThe border between Asia and Europe was historically defined by European academics. The Don River became unsatisfactory to northern Europeans when Peter the Great, king of the Tsardom of Russia, defeating rival claims of Sweden and the Ottoman Empire to the eastern lands, and armed resistance by the tribes of Siberia, synthesized a new Russian Empire extending to the Ural Mountains and beyond, founded in 1721. The major geographical theorist of the empire was actually a former Swedish prisoner-of-war, taken at the Battle of Poltava in 1709 and assigned to Tobolsk, where he associated with Peter's Siberian official, Vasily Tatishchev, and was allowed freedom to conduct geographical and anthropological studies in preparation for a future book.\n\nIn Sweden, five years after Peter's death, in 1730 Philip Johan von Strahlenberg published a new atlas proposing the Urals as the border of Asia. The Russians were enthusiastic about the concept, which allowed them to keep their European identity in geography. Tatishchev announced that he had proposed the idea to von Strahlenberg. The latter had suggested the Emba River as the lower boundary. Over the next century various proposals were made until the Ural River prevailed in the mid-19th century. The border had been moved perforce from the Black Sea to the Caspian Sea into which the Ural River projects. The border between the Black Sea and the Caspian is usually placed along the crest of the Caucasus Mountains, although it is sometimes placed further north.\n\nThe border between Asia and the region of Oceania is usually placed somewhere in the Malay Archipelago. The Maluku Islands in Indonesia are often considered to lie on the border of southeast Asia, with New Guinea, to the east of the islands, being wholly part of Oceania. The terms Southeast Asia and Oceania, devised in the 19th century, have had several vastly different geographic meanings since their inception. The chief factor in determining which islands of the Malay Archipelago are Asian has been the location of the colonial possessions of the various empires there (not all European). Lewis and Wigen assert, \"The narrowing of 'Southeast Asia' to its present boundaries was thus a gradual process.\"\n\nGeographical Asia is a cultural artifact of European conceptions of the world, beginning with the Ancient Greeks, being imposed onto other cultures, an imprecise concept causing endemic contention about what it means. Asia is larger and more culturally diverse than Europe. It does not exactly correspond to the cultural borders of its various types of constituents.\n\nFrom the time of Herodotus a minority of geographers have rejected the three-continent system (Europe, Africa, Asia) on the grounds that there is no or is no substantial physical separation between them. For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely \"the western excrescence of the continent of Asia\". \n\nGeographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass. Asia, Europe and Africa make up a single continuous landmass - Afro-Eurasia (except for the Suez Canal) and share a common continental shelf. Almost all of Europe and the better part of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Chersky Range) on the North American Plate.\n\nThe English name \"Asia\" was originally a concept of Greek civilization. The place name \"Asia\" in various forms in a large number of modern languages is of unknown ultimate provenience. Its etymology and language of origin are uncertain. It appears to be one of the most ancient of recorded names. A number of theories have been published. English Asia can be traced through the formation of English literature to Latin literature, where it has the same form, Asia. Whether all uses and all forms of the name derive also from the Latin of the Roman Empire is much less certain. One of the first classical writers to use Asia as a name of the whole continent was Pliny. This metonymical change in meaning is common and can be observed in some other geographical names, such as Skandinavia (from Scania).\n\nBefore Greek poetry, the Aegean Sea area was in a Greek Dark Age, at the beginning of which syllabic writing was lost and alphabetic writing had not begun. Prior to then in the Bronze Age the records of the Assyrian Empire, the Hittite Empire and the various Mycenaean states of Greece mention a region undoubtedly Asia, certainly in Anatolia, including if not identical to Lydia. These records are administrative and do not include poetry.\n\nThe Mycenaean states were destroyed about 1200 BCE by unknown agents although one school of thought assigns the Dorian invasion to this time. The burning of the palaces baked clay diurnal administrative records written in a Greek syllabic script called Linear B, deciphered by a number of interested parties, most notably by a young World War II cryptographer, Michael Ventris, subsequently assisted by the scholar, John Chadwick. A major cache discovered by Carl Blegen at the site of ancient Pylos included hundreds of male and female names formed by different methods.\n\nSome of these are of women held in servitude (as study of the society implied by the content reveals). They were used in trades, such as cloth-making, and usually came with children. The epithet, lawiaiai, \"captives,\" associated with some of them identifies their origin. Some are ethnic names. One in particular, aswiai, identifies \"women of Asia.\" Perhaps they were captured in Asia, but some others, Milatiai, appear to have been of Miletus, a Greek colony, which would not have been raided for slaves by Greeks. Chadwick suggests that the names record the locations where these foreign women were purchased. The name is also in the singular, Aswia, which refers both to the name of a country and to a female of it. There is a masculine form, aswios. This Aswia appears to have been a remnant of a region known to the Hittites as Assuwa, centered on Lydia, or \"Roman Asia.\" This name, \"Assuwa\", has been suggested as the origin for the name of the continent \"Asia\". The Assuwa league was a confederation of states in western Anatolia, defeated by the Hittites under Tudhaliya I around 1400 BCE.\n\nAlternatively, the etymology of the term may be from the Akkadian word \"(w)aṣû(m)\", which means 'to go outside' or 'to ascend', referring to the direction of the sun at sunrise in the Middle East and also likely connected with the Phoenician word \"asa\" meaning east. This may be contrasted to a similar etymology proposed for \"Europe\", as being from Akkadian \"erēbu(m)\" 'to enter' or 'set' (of the sun).\n\nT.R. Reid supports this alternative etymology, noting that the ancient Greek name must have derived from \"asu\", meaning 'east' in Assyrian (\"ereb\" for \"Europe\" meaning 'west'). The ideas of \"Occidental\" (form Latin \"Occidens\" 'setting') and \"Oriental\" (from Latin \"Oriens\" for 'rising') are also European invention, synonymous with \"Western\" and \"Eastern\". Reid further emphasizes that it explains the Western point of view of placing all the peoples and cultures of Asia into a single classification, almost as if there were a need for setting the distinction between Western and Eastern civilizations on the Eurasian continent. Ogura Kazuo and Tenshin Okakura are two outspoken Japanese figures on the subject.\n\nLatin Asia and Greek Ἀσία appear to be the same word. Roman authors translated Ἀσία as Asia. The Romans named a province Asia, which roughly corresponds with modern-day central-western Turkey. There was an Asia Minor and an Asia Major located in modern-day Iraq. As the earliest evidence of the name is Greek, it is likely circumstantially that Asia came from Ἀσία, but ancient transitions, due to the lack of literary contexts, are difficult to catch in the act. The most likely vehicles were the ancient geographers and historians, such as Herodotus, who were all Greek. Ancient Greek certainly evidences early and rich uses of the name.\n\nThe first continental use of Asia is attributed to Herodotus (about 440 BCE), not because he innovated it, but because his \"Histories\" are the earliest surviving prose to describe it in any detail. He defines it carefully, mentioning the previous geographers whom he had read, but whose works are now missing. By it he means Anatolia and the Persian Empire, in contrast to Greece and Egypt. Herodotus comments that he is puzzled as to why three women's names were \"given to a tract which is in reality one\" (Europa, Asia, and Libya, referring to Africa), stating that most Greeks assumed that Asia was named after the wife of Prometheus (i.e. Hesione), but that the Lydians say it was named after Asies, son of Cotys, who passed the name on to a tribe at Sardis. In Greek mythology, \"Asia\" (\"Ἀσία\") or \"Asie\" (\"Ἀσίη\") was the name of a \"Nymph or Titan goddess of Lydia.\"\n\nIn ancient Greek religion, places were under the care of female divinities, parallel to guardian angels. The poets detailed their doings and generations in allegoric language salted with entertaining stories, which subsequently playwrights transformed into classical Greek drama and became \"Greek mythology.\" For example, Hesiod mentions the daughters of Tethys and Ocean, among whom are a \"holy company\", \"who with the Lord Apollo and the Rivers have youths in their keeping.\" Many of these are geographic: Doris, Rhodea, Europa, Asia. Hesiod explains:\"For there are three-thousand neat-ankled daughters of Ocean who are dispersed far and wide, and in every place alike serve the earth and the deep waters.\" The Iliad (attributed by the ancient Greeks to Homer) mentions two Phrygians (the tribe that replaced the Luvians in Lydia) in the Trojan War named Asios (an adjective meaning \"Asian\"); and also a marsh or lowland containing a marsh in Lydia as ασιος.\n\nThe history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia and the Middle East, linked by the interior mass of the Central Asian steppes.\n\nThe coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley and the Yellow River shared many similarities. These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel. Other innovations, such as writing, seem to have been developed individually in each area. Cities, states and empires developed in these lowlands.\n\nThe central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes. The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into the Middle East, South Asia, and the borders of China, where the Tocharians resided. The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra. These areas remained very sparsely populated.\nThe center and the peripheries were mostly kept separated by mountains and deserts. The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty. While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.\n\nThe Islamic Caliphate took over the Middle East and Central Asia during the Muslim conquests of the 7th century. The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Before the Mongol invasion, Song dynasty reportedly had approximately 120 million citizens; the 1300 census which followed the invasion reported roughly 60 million people.\n\nThe Black Death, one of the most devastating pandemics in human history, is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road.\n\nThe Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, most of the Middle East, North Africa and the Balkans from the mid 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing Dynasty. The Islamic Mughal Empire and the Hindu Maratha Empire controlled much of India in the 16th and 18th centuries respectively.\n\nAsia is the largest continent on Earth. It covers 8.8% of the Earth's total surface area (or 30% of its land area), and has the largest coastline, at . Asia is generally defined as comprising the eastern four-fifths of Eurasia. It is located to the east of the Suez Canal and the Ural Mountains, and south of the Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Asia is subdivided into 48 countries, three of them (Russia, Kazakhstan and Turkey) having part of their land in Europe.\n\nAsia has extremely diverse climates and geographic features. Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia. It is moist across southeast sections, and dry across much of the interior. Some of the largest daily temperature ranges on Earth occur in western sections of Asia. The monsoon circulation dominates across southern and eastern sections, due to the presence of the Himalayas forcing the formation of a thermal low which draws in moisture during the summer. Southwestern sections of the continent are hot. Siberia is one of the coldest places in the Northern Hemisphere, and can act as a source of arctic air masses for North America. The most active place on Earth for tropical cyclone activity lies northeast of the Philippines and south of Japan. The Gobi Desert is in Mongolia and the Arabian Desert stretches across much of the Middle East. The Yangtze River in China is the longest river in the continent. The Himalayas between Nepal and China is the tallest mountain range in the world. Tropical rainforests stretch across much of southern Asia and coniferous and deciduous forests lie farther north.\n\nA survey carried out in 2010 by global risk analysis farm Maplecroft identified 16 countries that are extremely vulnerable to climate change. Each nation's vulnerability was calculated using 42 socio, economic and environmental indicators, which identified the likely climate change impacts during the next 30 years. The Asian countries of Bangladesh, India, Vietnam, Thailand, Pakistan and Sri Lanka were among the 16 countries facing extreme risk from climate change. Some shifts are already occurring. For example, in tropical parts of India with a semi-arid climate, the temperature increased by 0.4 °C between 1901 and 2003.\nA 2013 study by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) aimed to find science-based, pro-poor approaches and techniques that would enable Asia's agricultural systems to cope with climate change, while benefitting poor and vulnerable farmers. The study's recommendations ranged from improving the use of climate information in local planning and strengthening weather-based agro-advisory services, to stimulating diversification of rural household incomes and providing incentives to farmers to adopt natural resource conservation measures to enhance forest cover, replenish groundwater and use renewable energy.\n\nAsia has the second largest nominal GDP of all continents, after Europe, but the largest when measured in purchasing power parity. As of 2011, the largest economies in Asia are China, Japan, India, South Korea and Indonesia. Based on Global Office Locations 2011, Asia dominated the office locations with 4 of the top 5 being in Asia: Hong Kong, Singapore, Tokyo, Seoul and Shanghai. Around 68 percent of international firms have office in Hong Kong.\n\nIn the late 1990s and early 2000s, the economies of China and India have been growing rapidly, both with an average annual growth rate of more than 8%. Other recent very-high-growth nations in Asia include Israel, Malaysia, Indonesia, Bangladesh, Pakistan, Thailand, Vietnam, Mongolia, Uzbekistan, Cyprus and the Philippines, and mineral-rich nations such as Kazakhstan, Turkmenistan, Iran, Brunei, the United Arab Emirates, Qatar, Kuwait, Saudi Arabia, Bahrain and Oman.\n\nAccording to economic historian Angus Maddison in his book \"The World Economy: A Millennial Perspective\", India had the world's largest economy during 0 BCE and 1000 BCE. China was the largest and most advanced economy on earth for much of recorded history, until the British Empire (excluding India) overtook it in the mid-19th century. For several decades in the late twentieth century Japan was the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1986 and Germany in 1968. (NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC). This ended in 2010 when China overtook Japan to become the world's second largest economy.\n\nIn the late 1980s and early 1990s, Japan's GDP was almost as large (current exchange rate method) as that of the rest of Asia combined. In 1995, Japan's economy nearly equaled that of the USA as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/US$. Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which have now all received developed country status, having the highest GDP per capita in Asia.\nIt is forecasted that India will overtake Japan in terms of nominal GDP by 2020. By 2027, according to Goldman Sachs, China will have the largest economy in the world. Several trade blocs exist, with the most developed being the Association of Southeast Asian Nations.\n\nAsia is the largest continent in the world by a considerable margin, and it is rich in natural resources, such as petroleum, forests, fish, water, rice, copper and silver. Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in China, Taiwan, South Korea, Japan, India, the Philippines, and Singapore. Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly the PRC and India are making significant inroads. Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.\n\nAccording to Citigroup 9 of 11 Global Growth Generators countries came from Asia driven by population and income growth. They are Bangladesh, China, India, Indonesia, Iraq, Mongolia, Philippines, Sri Lanka and Vietnam. Asia has four main financial centers: Tokyo, Hong Kong, Singapore and Shanghai. Call centers and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly skilled, English-speaking workers. The increased use of outsourcing has assisted the rise of India and the China as financial centers. Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.\n\nIn 2010, Asia had 3.3 million millionaires (people with net worth over US$1 million excluding their homes), slightly below North America with 3.4 million millionaires. Last year Asia had toppled Europe.\nCitigroup in The Wealth Report 2012 stated that Asian centa-millionaire overtook North America's wealth for the first time as the world's \"economic center of gravity\" continued moving east. At the end of 2011, there were 18,000 Asian people mainly in Southeast Asia, China and Japan who have at least $100 million in disposable assets, while North America with 17,000 people and Western Europe with 14,000 people.\n\nWith growing Regional Tourism with domination of Chinese visitors, MasterCard has released Global Destination Cities Index 2013 with 10 of 20 are dominated by Asia and Pacific Region Cities and also for the first time a city of a country from Asia (Bangkok) set in the top-ranked with 15.98 international visitors.\n\nEast Asia had by far the strongest overall Human Development Index (HDI) improvement of any region in the world, nearly doubling average HDI attainment over the past 40 years, according to the report's analysis of health, education and income data. China, the second highest achiever in the world in terms of HDI improvement since\n1970, is the only country on the \"Top 10 Movers\" list due to income rather than health or education achievements. Its per capita income increased a stunning 21-fold over the last four decades, also lifting hundreds of millions out of income poverty. Yet it was not among the region's top performers in improving school enrollment and life expectancy.\n<br>Nepal, a South Asian country, emerges as one of the world's fastest movers since 1970 mainly due to health and education achievements. Its present life expectancy is 25 years longer than in the 1970s. More than four of every five children of school age in Nepal now attend primary school, compared to just one in five 40 years ago.\n<br> Japan and South Korea ranked highest among the countries grouped on the HDI (number 11 and 12 in the world, which are in the \"very high human development\" category), followed by Hong Kong (21) and Singapore (27). Afghanistan (155) ranked lowest amongst Asian countries out of the 169 countries assessed.\n\nAsia is home to several language families and many language isolates. Most Asian countries have more than one language that is natively spoken. For instance, according to Ethnologue, more than 600 languages are spoken in Indonesia, more than 800 languages spoken in India, and more than 100 are spoken in the Philippines. China has many languages and dialects in different provinces.\n\nMany of the world's major religions have their origins in Asia, including the five most practiced in the world (excluding irreligion), which are Christianity, Islam, Hinduism, Chinese folk religion (classified as Confucianism and Taoism), and Buddhism respectively. Asian mythology is complex and diverse. The story of the Great Flood for example, as presented to Christians in the Old Testament in the narrative of Noah, is first found in Mesopotamian mythology, in the \"Epic of Gilgamesh\". Likewise, the same story of Great Flood is presented to Muslims in the Holy Quran, again in the narrative of Noah, Who according to Islamic mythology was a Prophet and Built an Ark on Allah's Command to save the True Believers from the Great Flood (Great Calamity). Hindu mythology also tells about an Avatar of the God Vishnu in the form of a fish who warned Manu of a terrible flood. In ancient Chinese mythology, Shan Hai Jing, the Chinese ruler Da Yu, had to spend 10 years to control a deluge which swept out most of ancient China and was aided by the goddess Nüwa who literally fixed the broken sky through which huge rains were pouring.\n\nThe Abrahamic religions of Judaism, Christianity, Islam and Bahá'í Faith originated in West Asia.\n\nJudaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel, the birthplace and historical homeland of the Hebrew nation which today consists equally of those Israelites who remained in Asia/North Africa and those who returned from diaspora in Europe, North America, and other regions, though sizable communities continue to live abroad. Jews are the predominant ethnic group in Israel (75.6%) numbering at about 6.1 million, although the levels of adherence to Jewish religion are unspecified. Outside of Israel there are small ancient communities of Jewish still live in Turkey (17,400), Azerbaijan (9,100), Iran (8,756), India (5,000) and Uzbekistan (4,000).\n\nChristianity is a widespread religion in Asia with more than 286 million adherents according to Pew Research Center in 2010, and nearly 364 million according to Britannica Book of the Year 2014. Constituting around 12.6% of the total population of Asia. In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively. In Armenia, Cyprus, Georgia and Asian Russia, Eastern Orthodoxy is the predominant religion. Various Christian denominations have adherents in portions of the Middle East, as well as China and India. Saint Thomas Christians in India trace their origins to the evangelistic activity of Thomas the Apostle in the 1st century.\n\nIslam, which originated in Saudi Arabia, is the largest and most widely spread religion in Asia with at least 1 billion Muslims constituting around 23.8% of the total population of Asia. With 12.7% of the world Muslim population, the country currently with the largest Muslim population in the world is Indonesia, followed by Pakistan, India, Bangladesh, Iran and Turkey. Mecca, Medina and to a lesser extent Jerusalem are the holiest cities for Islam in all the world. These religious sites attract large numbers of devotees from all over the world, particularly during the Hajj and Umrah seasons. Iran is the largest Shi'a country.\n\nThe Bahá'í Faith originated in Asia, in Iran (Persia), and spread from there to the Ottoman Empire, Central Asia, India, and Burma during the lifetime of Bahá'u'lláh. Since the middle of the 20th century, growth has particularly occurred in other Asian countries, because Bahá'í activities in many Muslim countries has been severely suppressed by authorities. Lotus Temple is a big Baha'i Temple in India.\n\nAlmost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings. Indian philosophy includes Hindu philosophy and Buddhist philosophy. They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of the material world. The religions of Hinduism, Buddhism, Jainism and Sikhism originated in India, South Asia. In East Asia, particularly in China and Japan, Confucianism, Taoism and Zen Buddhism took shape.\n\nAs of 2012, Hinduism has around 1.1 billion adherents. The faith represents around 25% of Asia's population and is the second largest religion in Asia. However, it is mostly concentrated in South Asia. Over 80% of the populations of both India and Nepal adhere to Hinduism, alongside significant communities in Bangladesh, Pakistan, Bhutan, Sri Lanka and Bali, Indonesia. Many overseas Indians in countries such as Burma, Singapore and Malaysia also adhere to Hinduism.\n\nBuddhism has a great following in mainland Southeast Asia and East Asia. Buddhism is the religion of the majority of the populations of Cambodia (96%), Thailand (95%), Burma (80%–89%), Japan (36%–96%), Bhutan (75%–84%), Sri Lanka (70%), Laos (60%–67%) and Mongolia (53%–93%). Large Buddhist populations also exist in Singapore (33%–51%), Taiwan (35%–93%), South Korea (23%–50%), Malaysia (19%–21%), Nepal (9%–11%), Vietnam (10%–75%), China (20%–50%), North Korea (1.5%–14%), and small communities in India and Bangladesh. In many Chinese communities, Mahayana Buddhism is easily syncretized with Taoism, thus exact religious statistics is difficult to obtain and may be understated or overstated. The Communist-governed countries of China, Vietnam and North Korea are officially atheist, thus the number of Buddhists and other religious adherents may be under-reported.\n\nJainism is found mainly in India and in oversea Indian communities such as the United States and Malaysia.\nSikhism is found in Northern India and amongst overseas Indian communities in other parts of Asia, especially Southeast Asia.\nConfucianism is found predominantly in Mainland China, South Korea, Taiwan and in overseas Chinese populations.\nTaoism is found mainly in Mainland China, Taiwan, Malaysia and Singapore. Taoism is easily syncretized with Mahayana Buddhism for many Chinese, thus exact religious statistics is difficult to obtain and may be understated or overstated.\n\nSome of the events pivotal in the Asia territory related to the relationship with the outside world in the post-Second World War were:\n\n\nThe polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate. He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas. He is also the writer of the national anthems of Bangladesh and India.\n\nOther Asian writers who won Nobel Prize for literature include Yasunari Kawabata (Japan, 1968), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (China, 2000), Orhan Pamuk (Turkey, 2006), and Mo Yan (China, 2012). Some may consider the American writer, Pearl S. Buck, an honorary Asian Nobel laureate, having spent considerable time in China as the daughter of missionaries, and based many of her novels, namely \"The Good Earth\" (1931) and \"The Mother\" (1933), as well as the biographies of her parents of their time in China, \"The Exile\" and \"Fighting Angel\", all of which earned her the Literature prize in 1938.\n\nAlso, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children. Ebadi is the first Iranian and the first Muslim woman to receive the prize. Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma. She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma (Myanmar) and a noted prisoner of conscience. She is a Buddhist and was awarded the Nobel Peace Prize in 1991. Chinese dissident Liu Xiaobo was awarded the Nobel Peace Prize for \"his long and non-violent struggle for fundamental human rights in China\" on 8 October 2010. He is the first Chinese citizen to be awarded a Nobel Prize of any kind while residing in China. In 2014, Kailash Satyarthi from India and Malala Yousafzai from Pakistan were awarded the Nobel Peace Prize \"for their struggle against the suppression of children and young people and for the right of all children to education\".\n\nSir C. V. Raman is the first Asian to get a Nobel prize in Sciences. He won the Nobel Prize in Physics \"for his work on the scattering of light and for the discovery of the effect named after him\".\n\nJapan has won the most Nobel Prizes of any Asian nation with 24 followed by India which has won 13.\n\nAmartya Sen, (born 3 November 1933) is an Indian economist who was awarded the 1998 Nobel Memorial Prize in Economic Sciences for his contributions to welfare economics and social choice theory, and for his interest in the problems of society's poorest members.\n\nOther Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Ada Yonath, Yasser Arafat, José Ramos-Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and 13 Japanese scientists. Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Salam (Pakistan), Arafat (Palestinian Territories), Kim (South Korea), and Horta and Belo (Timor Leste).\n\nIn 2006, Dr. Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women in Bangladesh. Dr. Yunus received his PhD in economics from Vanderbilt University, United States. He is internationally known for the concept of micro credit which allows poor and destitute people with little or no collateral to borrow money. The borrowers typically pay back money within the specified period and the incidence of default is very low.\n\nThe Dalai Lama has received approximately eighty-four awards over his spiritual and political career. On 22 June 2006, he became one of only four people ever to be recognized with Honorary Citizenship by the Governor General of Canada. On 28 May 2005, he received the Christmas Humphreys Award from the Buddhist Society in the United Kingdom. Most notable was the Nobel Peace Prize, presented in Oslo, Norway on 10 December 1989.\n\nWithin the above-mentioned states are several partially recognized countries with limited to no international recognition. None of them are members of the UN:\nReferences to articles:\n\nSpecial topics:\n\nLists:\n\n\n\n"}
{"id": "690", "url": "https://en.wikipedia.org/wiki?curid=690", "title": "Aruba", "text": "Aruba\n\nAruba ( ; ) is a constituent country of the Kingdom of the Netherlands in the southern Caribbean Sea, located about west of the main part of the Lesser Antilles and north of the coast of Venezuela. It measures long from its northwestern to its southeastern end and across at its widest point. Together with Bonaire and Curaçao, Aruba forms a group referred to as the ABC islands. Collectively, Aruba and the other Dutch islands in the Caribbean are often called the Dutch Caribbean.\n\nAruba is one of the four countries that form the Kingdom of the Netherlands, along with the Netherlands, Curaçao, and Sint Maarten; the citizens of these countries are all Dutch nationals. Aruba has no administrative subdivisions, but, for census purposes, is divided into eight regions. Its capital is Oranjestad.\n\nUnlike much of the Caribbean region, Aruba has a dry climate and an arid, cactus-strewn landscape. This climate has helped tourism as visitors to the island can reliably expect warm, sunny weather. It has a land area of and is densely populated, with a total of 102,484 inhabitants at the 2010 Census. It lies outside Hurricane Alley.\n\nAruba's first inhabitants are thought to have been Caquetío Amerindians from the Arawak tribe, who migrated there from Venezuela to escape attacks by the Caribs. Fragments of the earliest known Indian settlements date back to 1000 AD. As sea currents made canoe travel to other Caribbean islands difficult, Caquetio culture remained more closely associated with that of mainland South America.\n\nEuropeans first learned of Aruba following the explorations for Spain by Amerigo Vespucci and Alonso de Ojeda in the summer of 1499. Both described Aruba as an \"island of giants\", remarking on the comparatively large stature of the native Caquetíos compared to Europeans. Gold was not discovered on Aruba for another 300 years. Vespucci returned to Spain with stocks of cotton and brazilwood from the island and described houses built into the ocean. Vespucci and Ojeda's tales spurred interest in Aruba, and Spaniards soon colonized the island.\n\nBecause it had low rainfall, Aruba was not considered profitable for the plantation system and the economics of the slave trade.\n\nAruba was colonized by Spain for over a century. \"Simas\", the \"Cacique\", or chief, in Aruba, welcomed the first Catholic priests in Aruba, who gave him a wooden cross as a gift. In 1508, the Spanish Crown appointed Alonso de Ojeda as its first Governor of Aruba, as part of \"Nueva Andalucía\". Arawaks spoke the \"broken Spanish\" which their ancestors had learned on Hispaniola.\n\nAnother governor appointed by Spain was Juan Martínez de Ampiés. A \"cédula real\" decreed in November 1525 gave Ampiés, factor of Española, the right to repopulate Aruba. In 1528, Ampiés was replaced by a representative of the House of Welser of Augsburg.\n\nThe Dutch statutes have applied to Aruba since 1629. The Netherlands acquired Aruba in 1636. Since 1636, Aruba has been under Dutch administration, initially governed by Peter Stuyvesant, later appointed to New Amsterdam (New York City). Stuyvesant was on a special mission in Aruba in November and December 1642. The island was included under the Dutch West India Company (W.I.C.) administration, as \"New Netherland and Curaçao\", from 1648 to 1664. In 1667 the Dutch administration appointed an Irishman as \"Commandeur\" in Aruba.\n\nThe Dutch took control 135 years after the Spanish, leaving the Arawaks to farm and graze livestock, and used the island as a source of meat for other Dutch possessions in the Caribbean.\n\nDuring the Napoleonic wars, the British Empire took control over the island, between 1799 and 1802, and between 1804 and 1816, before handing it back to the Dutch.\n\nDuring World War II with the occupation of the Netherlands in 1940 the oil facilities in Aruba came under the administration of the Dutch government-in-exile in London, and Aruba continued to supply oil to the British and their allies.\n\nIn August 1947, Aruba presented its first \"Staatsreglement\" (constitution), for Aruba's \"status aparte\" as an autonomous state within the Kingdom of the Netherlands. By 1954, the Charter of the Kingdom of the Netherlands was established, providing a framework for relations between Aruba and the rest of the Kingdom.\n\nIn 1972, at a conference in Suriname, Betico Croes (MEP), a politician from Aruba, proposed a \"sui-generis\" Dutch Commonwealth of four states: Aruba, the Netherlands, Suriname and the Netherlands Antilles, each to have its own nationality. C. Yarzagaray, a parliamentary member representing the AVP political party, proposed a referendum so that the people of Aruba could choose whether they wanted total independence or \"Status Aparte\" as a full autonomous state under the Crown.\n\nCroes worked in Aruba to inform and prepare the people of Aruba for independence. In 1976, he appointed a committee that chose the national flag and anthem, introducing them as symbols of Aruba's sovereignty and independence. He set 1981 as a target date for independence. In March 1977, the first Referendum for Self Determination was held with the support of the United Nations; 82% of the participants voted for independence.\n\nThe Island Government of Aruba assigned the Institute of Social Studies in The Hague to prepare a study for independence; it was titled \"Aruba en Onafhankelijkheid, achtergronden, modaliteiten en mogelijkheden; een rapport in eerste aanleg\" (Aruba and independence, backgrounds, modalities and opportunities; a preliminary report) (1978). At the conference in The Hague in 1981, Aruba's independence was set for the year 1991.\n\nIn March 1983, Aruba reached an official agreement within the Kingdom for its independence, to be developed in a series of steps as the Crown granted increasing autonomy. In August 1985 Aruba drafted a constitution that was unanimously approved. On 1 January 1986, after elections were held for its first parliament, Aruba seceded from the Netherlands Antilles; it officially became a country of the Kingdom of the Netherlands. Full independence was projected in 1996.\n\nAfter his death in 1986, Croes was proclaimed \"Libertador di Aruba\". At a convention in The Hague in 1990, at the request of Aruba's Prime Minister, the governments of Aruba, the Netherlands, and the Netherlands Antilles postponed indefinitely its transition to full independence. The article scheduling Aruba's complete independence was rescinded in 1995, although the process could be revived after another referendum.\n\nAruba is a generally flat, riverless island in the Leeward Antilles island arc of the Lesser Antilles in the southern part of the Caribbean. It has white sandy beaches on the western and southern coasts of the island, relatively sheltered from fierce ocean currents. This is where most tourist development has occurred. The northern and eastern coasts, lacking this protection, are considerably more battered by the sea and have been left largely untouched by humans.\n\nThe hinterland of the island features some rolling hills, the best known of which are called Hooiberg at and Mount Jamanota, the highest on the island at above sea level. Oranjestad, the capital, is located at .\n\nTo the east of Aruba are Bonaire and Curaçao, two island territories which once formed the southwest part of the Netherlands Antilles. This group of islands is sometimes called the ABC islands. They are located on the South American continental shelf and therefore geographically listed as part of South America.\n\nThe Natural Bridge was a large, naturally formed limestone bridge on the island's north shore. It was a popular tourist destination until its collapse in 2005.\n\nThe island, with a population of just over 100,000 inhabitants, does not have major cities. However, most of the island's population resides in or surrounding the two major city-like districts of Oranjestad (Capital) and San Nicolaas. Furthermore, the island is divided into six districts, which are:\n\nThe island of Aruba, being isolated from the main land of South America, has fostered the evolution of multiple endemic animals. The island provides a habitat for the endemic Aruban Whiptail and Aruba Rattlesnake, as well as endemic subspecies of Burrowing Owl and Brown-throated Parakeet.\n\nThe rattlesnake and the owl are printed on the Aruban currency.\n\nThe flora of Aruba differs from the typical tropical island vegetation. Xeric scrublands are common, with various forms of cacti, thorny shrubs and evergreens. The most known plant is the Aloe vera, which has a place on the Coat of Arms of Aruba.\n\nIn the Köppen climate classification, Aruba has a tropical semi-arid climate. Mean monthly temperature in Oranjestad varies little from to , moderated by constant trade winds from the Atlantic Ocean, which comes from north-east. Yearly precipitation barely exceeds in Oranjestad.\n\nThe population is estimated to be 75% mixed European/Amerindian/African, 15% Black and 10% other ethnicities.\n\nThe Arawak heritage is stronger on Aruba than on most Caribbean islands. Although no full-blooded Aboriginals remain, the features of the islanders clearly indicate their genetic Arawak heritage. Most of the population is descended from Caquetio Indians and Dutch and to a lesser extent of Africans, Spanish, Portuguese, English, French, and Sephardic Jewish ancestors.\n\nRecently, there has been substantial immigration to the island from neighboring American and Caribbean nations, possibly attracted by the higher paid jobs. In 2007, new immigration laws were introduced to help control the growth of the population by restricting foreign workers to a maximum of three years residency on the island.\n\nDemographically, Aruba has felt the impact of its proximity to Venezuela. Many of Aruba's families are descended from Venezuelan immigrants. There is a seasonal increase of Venezuelans living in second homes.\n\nThe official languages are Dutch and – since 2003 – Papiamento. Papiamento is the predominant language on Aruba. It is a creole language, spoken on Aruba, Bonaire, and Curaçao, that incorporates words from Portuguese, West African languages, Dutch, and Spanish. English is known by many; its usage has grown due to tourism. Other common languages spoken, based on the size of their community, are Portuguese, Chinese, German, Spanish, and French.\n\nIn recent years, the government of Aruba has shown an increased interest in acknowledging the cultural and historical importance of its native language. Although spoken Papiamento is fairly similar among the several Papiamento-speaking islands, there is a big difference in written Papiamento. The orthography differs per island and even per group of people. Some are more oriented towards Portuguese and use the equivalent spelling (e.g. \"y\" instead of \"j\"), where others are more oriented towards Dutch.\n\nThe book \"The Buccaneers of America\", first published in 1678, states through eyewitness account that the natives on Aruba spoke Spanish already. The oldest government official statement written in Papiamento dates from 1803. Around 12.6% of the population today speaks Spanish.\n\nAruba has four newspapers published in Papiamento: \"Diario\", \"Bon Dia\", \"Solo di Pueblo\" and \"Awe Mainta\"; and three in English: \"Aruba Daily\", \"Aruba Today\" and \"The News\". \"Amigoe\" is a newspaper published in Dutch. Aruba also has 18 radio stations (two AM and 16 FM) and two local television stations (Telearuba, and Channel 22).\n\nThree-quarters of the population is Roman Catholic.\n\nFor census purposes, Aruba is divided into eight regions, which have no administrative functions:\n\nAs a constituent country of the Kingdom of the Netherlands, Aruba's politics take place within a framework of a 21-member Parliament and an eight-member Cabinet. The governor of Aruba is appointed for a six-year term by the monarch, and the prime minister and deputy prime minister are elected by the Staten (or \"Parlamento\") for four-year terms. The Staten is made up of 21 members elected by direct, popular vote to serve a four-year term.\n\nTogether with the Netherlands, the countries of Aruba, Curaçao and Sint Maarten form the Kingdom of the Netherlands. As they share the same Dutch citizenship, these four countries still also share the Dutch passport as the Kingdom of the Netherlands passport. As Aruba, Curaçao and Sint Maarten have small populations, the three countries had to limit immigration. To protect their population, they have the right to control the admission and expulsion of people from the Netherlands.\n\nAruba is designated as a member of the Overseas Countries and Territories (OCT) and is thus officially not a part of the European Union, though Aruba can and does receive support from the European Development Fund.\n\nThe Aruban legal system is based on the Dutch model. In Aruba, legal jurisdiction lies with the \"Gerecht in Eerste Aanleg\" (Court of First Instance) on Aruba, the \"Gemeenschappelijk Hof van Justitie van Aruba, Curaçao, Sint Maarten en van Bonaire, Sint Eustatius en Saba\" (Joint Court of Justice of Aruba, Curaçao, Sint Maarten, and of Bonaire, Sint Eustatius and Saba) and the \"Hoge Raad der Nederlanden\" (Supreme Court of Justice of the Netherlands). The \"Korps Politie Aruba\" (Aruba Police Force) is the island's law enforcement agency and operates district precincts in Oranjestad, Noord, San Nicolaas, and Santa Cruz, where it is headquartered.\n\nDeficit spending has been a staple in Aruba's history, and modestly high inflation has been present as well. By 2006, the government's debt had grown to 1.883 billion Aruban florins. Aruba received some development aid from the Dutch government each year through 2009, as part of a deal (signed as \"Aruba's Financial Independence\") in which the Netherlands gradually reduced its financial help to the island each successive year.\n\nIn 2006, the Aruban government changed several tax laws to reduce the deficit. Direct taxes have been converted to indirect taxes as proposed by the IMF. A 3% tax has been introduced on sales and services, while income taxes have been lowered and revenue taxes for business reduced by 20%. The government compensated workers with 3.1% for the effect that the B.B.O. would have on the inflation for 2007.\n\nAruba's educational system is patterned after the Dutch system of education.\n\nThe Government of Aruba finances the public national education system.\n\nThere are private schools, including the International School of Aruba and Schakel College.\n\nThere are two medical schools, Aureus University School of Medicine and Xavier University School of Medicine, as well as its own national university, the University of Aruba.\n\nAruba has one of the highest standards of living in the Caribbean region. There is a low unemployment rate.\n\nThe GDP per capita for Aruba was estimated to be $28,924 in 2014; among the highest in the Caribbean and the Americas. Its main trading partners are Colombia, the United States, Venezuela, and the Netherlands.\n\nThe island's economy has been dominated by three main industries: tourism, aloe export, and petroleum refining (The Lago Oil and Transport Company and the Arend Petroleum Maatschappij Shell Co.). Before the \"Status Aparte\" (a separate completely autonomous country/state within the Kingdom), oil processing was the dominant industry in Aruba despite expansion of the tourism sector. Today, the influence of the oil processing business is minimal. The size of the agriculture and manufacturing sectors also remains minimal.\n\nThe official exchange rate of the Aruban florin is pegged to the US dollar at 1.79 florins to 1 USD. Because of this fact, and due to a large number of American tourists, many businesses operate using US dollars instead of florins, especially in the hotel and resort districts.\n\nAbout three quarters of the Aruban gross national product is earned through tourism or related activities. Most tourists are from the United States (predominantly from the north-east US), the Netherlands and South America, mainly Venezuela and Colombia.\n\nAs part of the Kingdom of the Netherlands, citizens of the Netherlands can travel with relative ease to Aruba and other islands of the Dutch Antilles. No visas are needed for Dutch citizens, only a passport, and although the currency used in Aruba is different (the Netherlands uses the Euro), money can be easily exchanged at a local bank for Aruban Florins.\n\nFor the facilitation of the passengers whose destination is the United States, the United States Department of Homeland Security (DHS), U.S. Customs and Border Protection (CBP) full pre-clearance facility in Aruba has been in effect since 1 February 2001 with the expansion in the Queen Beatrix Airport. United States and Aruba have had the agreement since 1986. It began as a USDA and Customs post. Since 2008, Aruba has been the only island to have this service for private flights.\n\nIn 1999, the U.S. Department of Defense established a Forward Operating Location (FOL) at the airport.\n\nOn 18 March, Aruba celebrates its National Day. In 1976, Aruba presented its National Anthem (Aruba Dushi Tera) and Flag.\n\nAruba has a varied culture. According to the \"Bureau Burgelijke Stand en Bevolkingsregister\" (BBSB), in 2005 there were ninety-two different nationalities living on the island. Dutch influence can still be seen, as in the celebration of \"Sinterklaas\" on 5 and 6 December and other national holidays like 27 April, when in Aruba and the rest of the Kingdom of the Netherlands the King's birthday or \"Dia di Rey\" (Koningsdag) is celebrated.\nChristmas and New Year's Eve are celebrated with the typical music and songs for gaitas for Christmas and the Dande for New Year, and \"ayaca\", \"ponche crema\", ham, and other typical foods and drinks. Millions of florins worth of fireworks are burnt at midnight on New Year's Eve. On 25 January, Betico Croes' birthday is celebrated. Dia di San Juan is celebrated on 24 June.\n\nBesides Christmas, the religious holy days of the Feast of the Ascension and Good Friday are holidays on the island.\n\nThe holiday of Carnaval is also an important one in Aruba, as it is in many Caribbean and Latin American countries, and, like Mardi Gras, that goes on for weeks. Its celebration in Aruba started, around the 1950s, influenced by the inhabitants from Venezuela and the nearby islands (Curaçao, St. Vincent, Trinidad, Barbados, St. Maarten and Anguilla) who came to work for the Oil refinery. Over the years the Carnival Celebration has changed and now starts from the beginning of January till the Tuesday before Ash Wednesday with a large parade on the last Sunday of the festivities (Sunday before Ash Wednesday).\n\nTourism from the United States has recently increased the visibility of American culture on the island, with such celebrations as Halloween and Thanksgiving Day in November.\n\nAruba's Queen Beatrix International Airport is located near Oranjestad. According to the Aruba Airport Authority, almost 1.7 million travelers used the airport in 2005, 61% of whom were Americans.\n\nAruba has two ports, Barcadera and Playa, which are located in Oranjestad and Barcadera. The Port of Playa services all the cruise-ship lines, including Royal Caribbean, Carnival Cruise Lines, NCL, Holland America Line, Disney Cruise Line and others. Nearly one million tourists enter this port per year. Aruba Ports Authority, owned and operated by the Aruban government, runs these seaports.\n\nArubus is a government-owned bus company. Its buses operate from 3:30 a.m. until 12:30 a.m., 365 days a year. Small private vans also provide transportation services in certain areas such Hotel Area, San Nicolaas, Santa Cruz and Noord.\n\nA street car service runs on rails on the Mainstreet.\n\nWater-en Energiebedrijf Aruba, N.V. (W.E.B.) produces potable industrial water at the world's third largest desalination plant. Average daily consumption in Aruba is about .\n\nThere are three telecommunications providers: Setar, a government-based company, Mio Wireless and Digicel, both of which are privately owned. Setar is the provider of services such as internet, video conferencing, GSM wireless technology and land lines. Digicel is Setar's competitor in wireless technology using the GSM platform, and Mio Wireless provides wireless technology and services using CDMA.\n\n\n\n"}
{"id": "691", "url": "https://en.wikipedia.org/wiki?curid=691", "title": "Articles of Confederation", "text": "Articles of Confederation\n\nThe Articles of Confederation, formally the Articles of Confederation and Perpetual Union, was an agreement among the 13 original states of the United States of America that served as its first constitution. Its drafting by a committee appointed by the Second Continental Congress began on July 12, 1776, and an approved version was sent to the states for ratification on November 15, 1777. The Articles of Confederation came into force on March 1, 1781, after being ratified by all 13 states. A guiding principle of the Articles was to preserve the independence and sovereignty of the states. The federal government received only those powers which the colonies had recognized as belonging to king and parliament.\n\nThe Articles formed a war-time confederation of states, with an extremely limited central government. While unratified, the document was used by the Congress to conduct business, direct the American Revolutionary War, conduct diplomacy with foreign nations, and deal with territorial issues and Native American relations. The adoption of the Articles made few perceptible changes in the federal government, because it did little more than legalize what the Continental Congress had been doing. That body was renamed the Congress of the Confederation; but Americans continued to call it the \"Continental Congress\", since its organization remained the same.\n\nAs the Confederation Congress attempted to govern the continually growing American states, delegates discovered that the limitations placed upon the central government rendered it ineffective at doing so. As the government's weaknesses became apparent, especially after Shays' Rebellion, individuals began asking for changes to the Articles. Their hope was to create a stronger national government. Initially, some states met to deal with their trade and economic problems. However, as more states became interested in meeting to change the Articles, a meeting was set in Philadelphia on May 25, 1787. This became the Constitutional Convention. It was quickly realized that changes would not work, and instead the entire Articles needed to be replaced. On March 4, 1789, the government under the Articles was replaced with the federal government under the Constitution. The new Constitution provided for a much stronger federal government by establishing a chief executive (the President), courts, and taxing powers.\n\nThe political push to increase cooperation among the then-loyal colonies began with the Albany Congress in 1754 and Benjamin Franklin's proposed Albany Plan of Union, an inter-colonial collaboration to help solve mutual local problems. The Articles of Confederation would bear some resemblance to it. Over the next two decades, some of the basic concepts it addressed would strengthen and others would weaken, particularly the degree of deserved loyalty to the crown. With civil disobedience resulting in coercive, and what the colonials perceived as intolerable acts of Parliament, and armed conflict resulting in dissidents being proclaimed rebels and outside the King's protection, any loyalty remaining shifted toward independence and how to achieve it. In 1775, with events outpacing communications, the Second Continental Congress began acting as the provisional government that would run the American Revolutionary War and gain the colonies their collective independence.\n\nIt was an era of constitution writing—most states were busy at the task—and leaders felt the new nation must have a written constitution, even though other nations did not. During the war, Congress exercised an unprecedented level of political, diplomatic, military and economic authority. It adopted trade restrictions, established and maintained an army, issued fiat money, created a military code and negotiated with foreign governments.\n\nTo transform themselves from outlaws into a legitimate nation, the colonists needed international recognition for their cause and foreign allies to support it. In early 1776, Thomas Paine argued in the closing pages of the first edition of \"Common Sense\" that the “custom of nations” demanded a formal declaration of American independence if any European power were to mediate a peace between the Americans and Great Britain. The monarchies of France and Spain in particular could not be expected to aid those they considered rebels against another legitimate monarch. Foreign courts needed to have American grievances laid before them persuasively in a “manifesto” which could also reassure them that the Americans would be reliable trading partners. Without such a declaration, Paine concluded, “[t]he custom of all courts is against us, and will be so, until, by an independence, we take rank with other nations.”\n\nBeyond improving their existing association, the records of the Second Continental Congress show that the need for a declaration of independence was intimately linked with the demands of international relations. On June 7, 1776, Richard Henry Lee introduced a resolution before the Continental Congress declaring the colonies independent; at the same time he also urged Congress to resolve “to take the most effectual measures for forming foreign Alliances” and to prepare a plan of confederation for the newly independent states. Congress then created three overlapping committees to draft the Declaration, a Model Treaty, and the Articles of Confederation. The Declaration announced the states' entry into the international system; the model treaty was designed to establish amity and commerce with other states; and the Articles of Confederation, which established “a firm league” among the thirteen free and independent states, constituted an international agreement to set up central institutions for the conduct of vital domestic and foreign affairs.\n\nOn June 12, 1776, a day after appointing a committee to prepare a draft of the Declaration of Independence, the Second Continental Congress resolved to appoint a committee of 13 to prepare a draft of a constitution for a union of the states. The committee met repeatedly, and chairman John Dickinson presented their results to the Congress on July 12, 1776. There were long debates on such issues as sovereignty, the exact powers to be given the confederate government, whether to have a judiciary, and voting procedures. The final draft of the Articles was prepared in the summer of 1777 and the Second Continental Congress approved them for ratification by the individual states on November 15, 1777, after a year of debate.\nIn practice, the Articles were in use beginning in 1777; the final draft of the Articles served as the de facto system of government used by the Congress (\"the United States in Congress assembled\") until it became de jure by final ratification on March 1, 1781; at which point Congress became the Congress of the Confederation. Under the Articles, the states retained sovereignty over all governmental functions not specifically relinquished to the national government. The individual articles set the rules for current and future operations of the United States government. It was made capable of making war and peace, negotiating diplomatic and commercial agreements with foreign countries, and deciding disputes between the states, including their additional and contested western territories. Article XIII stipulated that \"their provisions shall be inviolably observed by every state\" and \"the Union shall be perpetual\".\n\nJohn Dickinson's and Benjamin Franklin's handwritten drafts of the Articles of Confederation are housed at the National Archives in Washington, DC.\n\nThe Articles were created by delegates from the states in the Second Continental Congress out of a need to have \"a plan of confederacy for securing the freedom, sovereignty, and independence of the United States.\" After the war, nationalists, especially those who had been active in the Continental Army, complained that the Articles were too weak for an effective government. There was no president, no executive agencies, no judiciary and no tax base. The absence of a tax base meant that there was no way to pay off state and national debts from the war years except by requesting money from the states, which seldom arrived.\n\nIn 1788, with the approval of Congress, the Articles were replaced by the United States Constitution and the new government began operations in 1789.\n\nCongress began to move for ratification of the Articles of Confederation in 1777:\n\nThe document could not become officially effective until it was ratified by all 13 states. The first state to ratify was Virginia on December 16, 1777; the thirteenth state to ratify was Maryland on February 2, 1781. A ceremonial confirmation of this thirteenth, final ratification took place in the Congress on March 1, 1781, at high noon. Twelve states had ratified the Articles by February 1779, 14 months into the process. Maryland, however, refused to go along until the landed states, especially Virginia, had indicated they were prepared to cede their claims west of the Ohio River to the Union. It would be two years before the Maryland legislature became satisfied that they would follow through, and voted to ratify. The various states ratified the Articles of Confederation on the following dates:\n\nThe Articles of Confederation contain a preamble, thirteen articles, a conclusion, and a signatory section. The preamble declares that the states \"agree to certain articles of Confederation and perpetual Union.\"\nWhat follows here summarizes the purpose and content of each of the thirteen articles.\n\nWhile still at war with Britain, the revolution's leaders were divided between forming a national government with powers either strong and centralized (the \"federalists\"), or strictly limited (the \"anti federalists\"). The Continental Congress compromised by dividing sovereignty between the states and the central government, with a unicameral legislature that protected the liberty of the individual states. It empowered Congress to regulate military and monetary affairs, for example, but provided no mechanism to compel the States to comply with requests for either troops or funding. This left the military vulnerable to inadequate funding, supplies, or even food.\n\nThe Treaty of Paris (1783), which ended hostilities with Great Britain, languished in Congress for months because several state representatives failed to attend sessions of the national legislature to ratify it. Yet Congress had no power to enforce attendance. In September 1783, George Washington complained that Congress was paralyzed. Many revolutionaries had gone to their respective home countries after the war, and local government and self-rule seemed quite satisfactory.\n\nThe Articles supported the Congressional direction of the Continental Army, and allowed the states to present a unified front when dealing with the European powers. As a tool to build a centralized war-making government, they were largely a failure: Historian Bruce Chadwick wrote:\n\nThe Continental Congress, before the Articles were approved, had promised soldiers a pension of half pay for life. However Congress had no power to compel the states to fund this obligation, and as the war wound down after the victory at Yorktown the sense of urgency to support the military was no longer a factor. No progress was made in Congress during the winter of 1783–84. General Henry Knox, who would later become the first Secretary of War under the Constitution, blamed the weaknesses of the Articles for the inability of the government to fund the army. The army had long been supportive of a strong union. Knox wrote:\n\nAs Congress failed to act on the petitions, Knox wrote to Gouverneur Morris, four years before the Philadelphia Convention was convened, \"As the present Constitution is so defective, why do not you great men call the people together and tell them so; that is, to have a convention of the States to form a better Constitution.\"\n\nOnce the war had been won, the Continental Army was largely disbanded. A very small national force was maintained to man the frontier forts and to protect against Native American attacks. Meanwhile, each of the states had an army (or militia), and 11 of them had Navies. The wartime promises of bounties and land grants to be paid for service were not being met. In 1783, George Washington defused the Newburgh conspiracy, but riots by unpaid Pennsylvania veterans forced Congress to leave Philadelphia temporarily.\n\nThe Congress from time to time during the Revolutionary War requisitioned troops from the states. Any contributions were voluntary, and in the debates of 1788 the Federalists (who supported the proposed new Constitution) claimed that state politicians acted unilaterally, and contributed when the Continental army protected their state's interests. The Anti-Federalists claimed that state politicians understood their duty to the Union and contributed to advance its needs. Dougherty (2009) concludes that generally the States' behavior validated the Federalist analysis. This helps explain why the Articles of Confederation needed reforms.\n\nEven after peace had been achieved in 1783, the weakness of the Confederation government frustrated the ability of the government to conduct foreign policy. In 1789, Thomas Jefferson, concerned over the failure to fund an American naval force to confront the Barbary pirates, wrote to James Monroe, \"It will be said there is no money in the treasury. There never will be money in the treasury till the Confederacy shows its teeth. The states must see the rod.”\n\nFurthermore, the Jay–Gardoqui Treaty with Spain in 1789 also showed weakness in foreign policy. In this treaty — which was never ratified due to its immense unpopularity — the United States was to give up rights to use the Mississippi River for 25 years, which would have economically strangled the settlers west of the Appalachian Mountains. Finally, due to the Confederation's military weakness, it could not compel the British army to leave frontier forts which were on American soil — forts which, in 1783, the British promised to leave, but which they delayed leaving pending U.S. implementation of other provisions such as ending action against Loyalists and allowing them to seek compensation. This incomplete British implementation of the Treaty of Paris (1783) was superseded by the implementation of Jay's Treaty in 1795 under the new U.S. Constitution.\n\nUnder the Articles of Confederation, the central government's power was kept quite limited. The Confederation Congress could make decisions, but lacked enforcement powers. Implementation of most decisions, including modifications to the Articles, required unanimous approval of all thirteen state legislatures.\n\nCongress was denied any powers of taxation: it could only request money from the states. The states often failed to meet these requests in full, leaving both Congress and the Continental Army chronically short of money. As more money was printed by Congress, the continental dollars depreciated. In 1779, George Washington wrote to John Jay, who was serving as the president of the Continental Congress, \"that a wagon load of money will scarcely purchase a wagon load of provisions.\" Mr. Jay and the Congress responded in May by requesting $45 million from the States. In an appeal to the States to comply, Jay wrote that the taxes were \"the price of liberty, the peace, and the safety of yourselves and posterity.\" He argued that Americans should avoid having it said \"that America had no sooner become independent than she became insolvent\" or that \"her infant glories and growing fame were obscured and tarnished by broken contracts and violated faith.\" The States did not respond with any of the money requested from them.\n\nCongress had also been denied the power to regulate either foreign trade or interstate commerce and, as a result, all of the States maintained control over their own trade policies. The states and the Confederation Congress both incurred large debts during the Revolutionary War, and how to repay those debts became a major issue of debate following the War. Some States paid off their war debts and others did not. Federal assumption of the states' war debts became a major issue in the deliberations of the Constitutional Convention.\n\nNevertheless, the Confederation Congress did take two actions with long-lasting impact. The Land Ordinance of 1785 and Northwest Ordinance created territorial government, set up protocols for the admission of new states and the division of land into useful units, and set aside land in each township for public use. This system represented a sharp break from imperial colonization, as in Europe, and provided the basis for the rest of American continental expansion through the 19th Century.\n\nThe Land Ordinance of 1785 established both the general practices of land surveying in the west and northwest and the land ownership provisions used throughout the later westward expansion beyond the Mississippi River. Frontier lands were surveyed into the now-familiar squares of land called the township (36 square miles), the section (one square mile), and the quarter section (160 acres). This system was carried forward to most of the States west of the Mississippi (excluding areas of Texas and California that had already been surveyed and divided up by the Spanish Empire). Then, when the Homestead Act was enacted in 1867, the quarter section became the basic unit of land that was granted to new settler-farmers.\n\nThe Northwest Ordinance of 1787 noted the agreement of the original states to give up northwestern land claims, organized the Northwest Territory and laid the groundwork for the eventual creation of new states. While it didn't happen under the articles, the land north of the Ohio River and west of the (present) western border of Pennsylvania ceded by Massachusetts, Connecticut, New York, Pennsylvania, and Virginia, eventually became the states of: Ohio, Indiana, Illinois, Michigan, and Wisconsin, and the part of Minnesota east of the Mississippi River. The Northwest Ordinance of 1787 also made great advances in the abolition of slavery. New states admitted to the union in said territory would never be slave states.\n\nNo new states were admitted to the Union under the Articles of Confederation. The Articles provided for a blanket acceptance of the Province of Quebec (referred to as \"Canada\" in the Articles) into the United States if it chose to do so. It did not, and the subsequent Constitution carried no such special provision of admission. Additionally, ordinances to admit Frankland (later modified to Franklin), Kentucky, and Vermont to the Union were considered, but none were approved.\n\nThe peace treaty left the United States independent and at peace but with an unsettled governmental structure. The Articles envisioned a permanent confederation, but granted to the Congress—the only federal institution—little power to finance itself or to ensure that its resolutions were enforced. There was no president and no national court. Although historians generally agree that the Articles were too weak to hold the fast-growing nation together, they do give credit to the settlement of the western issue, as the states voluntarily turned over their lands to national control.\n\nBy 1783, with the end of the British blockade, the new nation was regaining its prosperity. However, trade opportunities were restricted by the mercantilism of the British and French empires. The ports of the British West Indies were closed to all staple products which were not carried in British ships. France and Spain established similar policies. Simultaneously, new manufacturers faced sharp competition from British products which were suddenly available again. Political unrest in several states and efforts by debtors to use popular government to erase their debts increased the anxiety of the political and economic elites which had led the Revolution. The apparent inability of the Congress to redeem the public obligations (debts) incurred during the war, or to become a forum for productive cooperation among the states to encourage commerce and economic development, only aggravated a gloomy situation. In 1786–87, Shays' Rebellion, an uprising of dissidents in western Massachusetts against the state court system, threatened the stability of state government.\n\nThe Continental Congress printed paper money which was so depreciated that it ceased to pass as currency, spawning the expression \"not worth a continental\". Congress could not levy taxes and could only make requisitions upon the States. Less than a million and a half dollars came into the treasury between 1781 and 1784, although the governors had been asked for two million in 1783 alone.\n\nWhen John Adams went to London in 1785 as the first representative of the United States, he found it impossible to secure a treaty for unrestricted commerce. Demands were made for favors and there was no assurance that individual states would agree to a treaty. Adams stated it was necessary for the States to confer the power of passing navigation laws to Congress, or that the States themselves pass retaliatory acts against Great Britain. Congress had already requested and failed to get power over navigation laws. Meanwhile, each State acted individually against Great Britain to little effect. When other New England states closed their ports to British shipping, Connecticut hastened to profit by opening its ports.\n\nBy 1787 Congress was unable to protect manufacturing and shipping. State legislatures were unable or unwilling to resist attacks upon private contracts and public credit. Land speculators expected no rise in values when the government could not defend its borders nor protect its frontier population.\n\nThe idea of a convention to revise the Articles of Confederation grew in favor. Alexander Hamilton realized while serving as Washington's top aide that a strong central government was necessary to avoid foreign intervention and allay the frustrations due to an ineffectual Congress. Hamilton led a group of like-minded nationalists, won Washington's endorsement, and convened the Annapolis Convention in 1786 to petition Congress to call a constitutional convention to meet in Philadelphia to remedy the long-term crisis.\n\nThe Second Continental Congress approved the Articles for distribution to the states on November 15, 1777. A copy was made for each state and one was kept by the Congress. On November 28, the copies sent to the states for ratification were unsigned, and the cover letter, dated November 17, had only the signatures of Henry Laurens and Charles Thomson, who were the President and Secretary to the Congress.\n\nThe Articles, however, were unsigned, and the date was blank. Congress began the signing process by examining their copy of the Articles on June 27, 1778. They ordered a final copy prepared (the one in the National Archives), and that delegates should inform the secretary of their authority for ratification.\n\nOn July 9, 1778, the prepared copy was ready. They dated it, and began to sign. They also requested each of the remaining states to notify its delegation when ratification was completed. On that date, delegates present from New Hampshire, Massachusetts, Rhode Island, Connecticut, New York, Pennsylvania, Virginia and South Carolina signed the Articles to indicate that their states had ratified. New Jersey, Delaware and Maryland could not, since their states had not ratified. North Carolina and Georgia also didn't sign that day, since their delegations were absent.\n\nAfter the first signing, some delegates signed at the next meeting they attended. For example, John Wentworth of New Hampshire added his name on August 8. John Penn was the first of North Carolina's delegates to arrive (on July 10), and the delegation signed the Articles on July 21, 1778.\n\nThe other states had to wait until they ratified the Articles and notified their Congressional delegation. Georgia signed on July 24, New Jersey on November 26, and Delaware on February 12, 1779. Maryland refused to ratify the Articles until every state had ceded its western land claims.\nOn February 2, 1781, the much-awaited decision was taken by the Maryland General Assembly in Annapolis. As the last piece of business during the afternoon Session, \"among engrossed Bills\" was \"signed and sealed by Governor Thomas Sim Lee in the Senate Chamber, in the presence of the members of both Houses... an Act to empower the delegates of this state in Congress to subscribe and ratify the articles of confederation\" and perpetual union among the states. The Senate then adjourned \"to the first Monday in August next.\" The decision of Maryland to ratify the Articles was reported to the Continental Congress on February 12. The confirmation signing of the Articles by the two Maryland delegates took place in Philadelphia at noon time on March 1, 1781, and was celebrated in the afternoon. With these events, the Articles were entered into force and the United States of America came into being as a sovereign federal state.\n\nCongress had debated the Articles for over a year and a half, and the ratification process had taken nearly three and a half years. Many participants in the original debates were no longer delegates, and some of the signers had only recently arrived. The Articles of Confederation and Perpetual Union were signed by a group of men who were never present in the Congress at the same time.\nThe signers and the states they represented were:\nConnecticut\n\nDelaware\n\nGeorgia\n\nMaryland\n\nMassachusetts Bay\n\nNew Hampshire\nNew Jersey\n\nNew York\n\nNorth Carolina\n\nPennsylvania\n\nRhode Island and Providence Plantations\n\nSouth Carolina\n\nVirginia\nRoger Sherman (Connecticut) was the only person to sign all four great state papers of the United States: the Continental Association, the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.\n\nRobert Morris (Pennsylvania) signed three of the great state papers of the United States: the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.\n\nJohn Dickinson (Delaware), Daniel Carroll (Maryland) and Gouverneur Morris (New York), along with Sherman and Robert Morris, were the only five people to sign both the Articles of Confederation and the United States Constitution (Gouverneur Morris represented Pennsylvania when signing the Constitution).\n\nThe following list is of those who led the Congress of the Confederation under the Articles of Confederation as the Presidents of the United States in Congress Assembled. Under the Articles, the president was the presiding officer of Congress, chaired the Committee of the States when Congress was in recess, and performed other administrative functions. He was not, however, an executive in the way the successor President of the United States is a chief executive, since all of the functions he executed were under the direct control of Congress.\n\n\"For a full list of Presidents of the Congress Assembled and Presidents under the two Continental Congresses before the Articles, see President of the Continental Congress.\"\n\nImages of an original draft of the Articles of Confederation stored at the United States National Archive.\nOn January 21, 1786, the Virginia Legislature, following James Madison's recommendation, invited all the states to send delegates to Annapolis, Maryland to discuss ways to reduce interstate conflict. At what came to be known as the Annapolis Convention, the few state delegates in attendance endorsed a motion that called for all states to meet in Philadelphia in May 1787 to discuss ways to improve the Articles of Confederation in a \"Grand Convention.\" Although the states' representatives to the Constitutional Convention in Philadelphia were only authorized to amend the Articles, the representatives held secret, closed-door sessions and wrote a new constitution. The new Constitution gave much more power to the central government, but characterization of the result is disputed. The general goal of the authors was to get close to a republic as defined by the philosophers of the Age of Enlightenment, while trying to address the many difficulties of the interstate relationships. Historian Forrest McDonald, using the ideas of James Madison from \"Federalist 39\", describes the change this way:\n\nIn May 1786, Charles Pinckney of South Carolina proposed that Congress revise the Articles of Confederation. Recommended changes included granting Congress power over foreign and domestic commerce, and providing means for Congress to collect money from state treasuries. Unanimous approval was necessary to make the alterations, however, and Congress failed to reach a consensus. The weakness of the Articles in establishing an effective unifying government was underscored by the threat of internal conflict both within and between the states, especially after Shays' Rebellion threatened to topple the state government of Massachusetts.\n\nHistorian Ralph Ketcham comments on the opinions of Patrick Henry, George Mason, and other Anti-Federalists who were not so eager to give up the local autonomy won by the revolution:\n\nHistorians have given many reasons for the perceived need to replace the articles in 1787. Jillson and Wilson (1994) point to the financial weakness as well as the norms, rules and institutional structures of the Congress, and the propensity to divide along sectional lines.\n\nRakove (1988) identifies several factors that explain the collapse of the Confederation. The lack of compulsory direct taxation power was objectionable to those wanting a strong centralized state or expecting to benefit from such power. It could not collect customs after the war because tariffs were vetoed by Rhode Island. Rakove concludes that their failure to implement national measures \"stemmed not from a heady sense of independence but rather from the enormous difficulties that all the states encountered in collecting taxes, mustering men, and gathering supplies from a war-weary populace.\" The second group of factors Rakove identified derived from the substantive nature of the problems the Continental Congress confronted after 1783, especially the inability to create a strong foreign policy. Finally, the Confederation's lack of coercive power reduced the likelihood for profit to be made by political means, thus potential rulers were uninspired to seek power.\n\nWhen the war ended in 1783, certain special interests had incentives to create a new \"merchant state,\" much like the British state people had rebelled against. In particular, holders of war scrip and land speculators wanted a central government to pay off scrip at face value and to legalize western land holdings with disputed claims. Also, manufacturers wanted a high tariff as a barrier to foreign goods, but competition among states made this impossible without a central government.\n\nPolitical scientist David C. Hendrickson writes that two prominent political leaders in the Confederation, John Jay of New York and Thomas Burke of North Carolina believed that \"the authority of the congress rested on the prior acts of the several states, to which the states gave their voluntary consent, and until those obligations were fulfilled, neither nullification of the authority of congress, exercising its due powers, nor secession from the compact itself was consistent with the terms of their original pledges.\"\n\nAccording to Article XIII of the Confederation, any alteration had to be approved unanimously: \n[T]he Articles of this Confederation shall be inviolably observed by every State, and the Union shall be perpetual; nor shall any alteration at any time hereafter be made in any of them; unless such alteration be agreed to in a Congress of the United States, and be afterwards confirmed by the legislatures of every State.\n\nOn the other hand, Article VII of the proposed Constitution stated that it would become effective after ratification by a mere nine states, without unanimity:\nThe Ratification of the Conventions of nine States, shall be sufficient for the Establishment of this Constitution between the States so ratifying the Same.\n\nThe apparent tension between these two provisions was addressed at the time, and remains a topic of scholarly discussion. In 1788, James Madison remarked (in \"Federalist No. 40\") that the issue had become moot: \"As this objection...has been in a manner waived by those who have criticised the powers of the convention, I dismiss it without further observation.\" Nevertheless, it is an interesting historical and legal question whether opponents of the Constitution could have plausibly attacked the Constitution on that ground. At the time, there were state legislators who argued that the Constitution was not an alteration of the Articles of Confederation, but rather would be a complete replacement so the unanimity rule did not apply. Moreover, the Confederation had proven woefully inadequate and therefore was supposedly no longer binding.\n\nModern scholars such as Francisco Forrest Martin agree that the Articles of Confederation had lost its binding force because many states had violated it, and thus \"other states-parties did not have to comply with the Articles' unanimous consent rule\". In contrast, law professor Akhil Amar suggests that there may not have really been any conflict between the Articles of Confederation and the Constitution on this point; Article VI of the Confederation specifically allowed side deals among states, and the Constitution could be viewed as a side deal until all states ratified it.\n\nOn July 3, 1788, the Congress received New Hampshire's all-important ninth ratification of the proposed Constitution, thus, according to its terms, establishing it as the new framework of governance for the ratifying states. The following day delegates considered a bill to admit Kentucky into the Union as a sovereign state. The discussion ended with Congress making the determination that, in light of this development, it would be \"unadvisable\" to admit Kentucky into the Union, as it could do so \"under the Articles of Confederation\" only, but not \"under the Constitution\".\n\nBy the end of July 1788, 11 of the 13 states had ratified the new Constitution. Congress continued to convene under the Articles with a quorum until October. On Saturday, September 13, 1788, the Confederation Congress voted the resolve to implement the new Constitution, and on Monday, September 15 published an announcement that the new Constitution had been ratified by the necessary nine states, set the first Wednesday in February 1789 for the presidential electors to meet and select a new president, and set the first Wednesday of March 1789 as the day the new government would take over and the government under the Articles of Confederation would come to an end.\n\nOn that same September 13, it determined that New York would remain the national capital.\n\n\n\n"}
{"id": "694", "url": "https://en.wikipedia.org/wiki?curid=694", "title": "Asia Minor (disambiguation)", "text": "Asia Minor (disambiguation)\n\nAsia Minor is an alternative name for Anatolia, the westernmost protrusion of Asia, comprising the majority of the Republic of Turkey. It may also refer to:\n"}
{"id": "698", "url": "https://en.wikipedia.org/wiki?curid=698", "title": "Atlantic Ocean", "text": "Atlantic Ocean\n\nThe Atlantic Ocean is the second largest of the world's oceans with a total area of about . It covers approximately 20 percent of the Earth's surface and about 29 percent of its water surface area. It separates the \"Old World\" from the \"New World\".\n\nThe Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Eurasia and Africa to the east, and the Americas to the west. As one component of the interconnected global ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south (other definitions describe the Atlantic as extending southward to Antarctica). The Equatorial Counter Current subdivides it into the North Atlantic Ocean and South Atlantic Ocean at about 8°N.\n\nScientific explorations of the Atlantic include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.\n\nThe oldest known mentions of an \"Atlantic\" sea come from Stesichorus around mid-sixth century BC (Sch. A. R. 1. 211): \"Atlantikoi pelágei\" (Greek: Ἀτλαντικῷ πελάγει; English: 'the Atlantic sea'; etym. 'Sea of Atlantis') and in \"The Histories\" of Herodotus around 450 BC (Hdt. 1.202.4): \"Atlantis thalassa\" (Greek: Ἀτλαντὶς θάλασσα; English: 'Sea of Atlantis' or 'the Atlantis sea') where the name refers to \"the sea beyond the pillars of Heracles\" which is said to be part of the ocean that surrounds all land. Thus, on one hand, the name refers to Atlas, the Titan in Greek mythology, who supported the heavens and who later appeared as a frontispiece in Medieval maps and also lent his name to modern atlases. On the other hand, to early Greek sailors and in Ancient Greek mythological literature such as the \"Iliad\" and the \"Odyssey\", this all-encompassing ocean was instead known as Oceanus, the gigantic river that encircled the world; in contrast to the enclosed seas well-known to the Greeks: the Mediterranean and the Black Sea.\nIn contrast, the term \"Atlantic\" originally referred specifically to the Atlas Mountains in Morocco and the sea off the Strait of Gibraltar and the North African coast. The Greek word \"thalassa\" has been reused by scientists for the huge Panthalassa ocean that surrounded the supercontinent Pangaea hundreds of million years ago.\n\nThe term \"Aethiopian Ocean\", derived from Ancient Ethiopia, was applied to the Southern Atlantic as late as the mid-19th century.\n\nThe International Hydrographic Organization (IHO) defined the limits of the oceans and seas in 1953, but some of these definitions have been revised since then and some are not used by various authorities, institutions, and countries, see for example the CIA World Factbook. Correspondingly, the extent and number of oceans and seas varies.\n\nThe Atlantic Ocean is bounded on the west by North and South America. It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea. To the east, the boundaries of the ocean proper are Europe: the Strait of Gibraltar (where it connects with the Mediterranean Sea–one of its marginal seas–and, in turn, the Black Sea, both of which also touch upon Asia) and Africa.\n\nIn the southeast, the Atlantic merges into the Indian Ocean. The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border. In the 1953 definition it extends south to Antarctica, while in later maps it is bounded at the 60° parallel by the Southern Ocean.\n\nThe Atlantic has irregular coasts indented by numerous bays, gulfs, and seas. These include the Baltic Sea, Black Sea, Caribbean Sea, Davis Strait, Denmark Strait, part of the Drake Passage, Gulf of Mexico, Labrador Sea, Mediterranean Sea, North Sea, Norwegian Sea, almost all of the Scotia Sea, and other tributary water bodies. Including these marginal seas the coast line of the Atlantic measures compared to for the Pacific.\n\nIncluding its marginal seas, the Atlantic covers an area of or 23.5% of the global ocean and has a volume of or 23.3%. Excluding its marginal seas, the Atlantic covers and has a volume of . The North Atlantic covers (11.5%) and the South Atlantic (11.1%). The average depth is and the maximum depth, the Milwaukee Deep in the Puerto Rico Trench, is .\n\nThe bathymetry of the Atlantic is dominated by a submarine mountain range called the Mid-Atlantic Ridge (MAR). It runs from 87°N or south of the North Pole to the subantarctic Bouvet Island at 42°S.\n\nThe MAR divides the Atlantic longitudinally into two halves, in each of which a series of basins are delimited by secondary, transverse ridges. The MAR reaches above along most of its length, but is interrupted by larger transform faults at two places: the Romanche Trench near the Equator and the Gibbs Fracture Zone at 53°N. The MAR is a barrier for bottom water, but at these two transform faults deep water currents can pass from one side to the other.\n\nThe MAR rises above the surrounding ocean floor and its rift valley is the divergent boundary between the North American and Eurasian plates in the North Atlantic and the South American and African plates in the South Atlantic. The MAR produces basaltic volcanoes in Eyjafjallajökull, Iceland, and pillow lava on the ocean floor. The depth of water at the apex of the ridge is less than in most places, while the bottom of the ridge is three times as deep.\n\nThe MAR is intersected by two perpendicular ridges: the Azores–Gibraltar Transform Fault, the boundary between the Nubian and Eurasian plates, intersects the MAR at the Azores Triple Junction, on either side of the Azores microplate, near the 40°N. A much vaguer, nameless boundary, between the North American and South American plates, intersects the MAR near or just north of the Fifteen-Twenty Fracture Zone, approximately at 16°N.\n\nIn the 1870s, the Challenger expedition discovered parts of what is now known as the Mid-Atlantic Ridge, or:\n\nMost of the MAR runs under water but where it reaches the surfaces it has produced volcanic islands. While nine of these have collectively been nominated a World Heritage Site for their geological value, four of them are considered of \"Outstanding Universal Value\" based on their cultural and natural criteria: Þingvellir, Iceland; Landscape of the Pico Island Vineyard Culture, Portugal; Gough and Inaccessible Islands, United Kingdom; and Brazilian Atlantic Islands: Fernando de Noronha and Atol das Rocas Reserves, Brazil.\n\nContinental shelves in the Atlantic are wide off Newfoundland, southern-most South America, and north-eastern Europe.\nIn the western Atlantic carbonate platforms dominate large areas, for example the Blake Plateau and Bermuda Rise.\nThe Atlantic is surrounded by passive margins except at a few locations where active margins form deep trenches: the Puerto Rico Trench ( maximum depth) in the western Pacific and South Sandwich Trench () in the South Atlantic. There are numerous submarine canyons off north-eastern North America, western Europe, and north-western Africa. Some of these canyons extend along the continental rises and farther into the abyssal plains as deep-sea channels.\n\nIn 1922 a historic moment in cartography and oceanography occurred. The USS Stewart used a Navy Sonic Depth Finder to draw a continuous map across the bed of the Atlantic. This involved little guesswork because the idea of sonar is straight forward with pulses being sent from the vessel, which bounce off the ocean floor, then return to the vessel. The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.\n\nThe mean depth between 60°N and 60°S is , or close to the average for the global ocean, with a modal depth between .\n\nIn the South Atlantic the Walvis Ridge and Rio Grande Rise form barriers to ocean currents.\nThe Laurentian Abyss is found off the eastern coast of Canada.\n\nSurface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below to over . Maximum temperatures occur north of the equator, and minimum values are found in the polar regions. In the middle latitudes, the area of maximum temperature variations, values may vary by .\n\nFrom October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea.\n\nThe Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise. The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur during each 24 lunar hours. In latitudes above 40° North some east-west oscillation, known as the North Atlantic oscillation, occurs.\n\nOn average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3 – 3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.\n\nThe high surface salinity in the Atlantic, on which the Atlantic thermohaline circulation is dependent, is maintained by two processes: the Agulhas Leakage/Rings, which brings salty Indian Ocean waters into the South Atlantic, and the \"Atmospheric Bridge\", which evaporates subtropical Atlantic waters and exports it to the Pacific.\n\nThe Atlantic Ocean consists of four major, upper water masses with distinct temperature and salinity. The Atlantic Subarctic Upper Water in the northern-most North Atlantic is the source for Subarctic Intermediate Water and North Atlantic Intermediate Water. North Atlantic Central Water can be divided into the Eastern and Western North Atlantic central Water since the western part is strongly affected by the Gulf Stream and therefore the upper layer is closer to underlying fresher subpolar intermediate water. The eastern water is saltier because of its proximity to Mediterranean Water. North Atlantic Central Water flows into South Atlantic Central Water at 15°N.\n\nThere are five intermediate waters: four low-salinity waters formed at subpolar latitudes and one high-salinity formed through evaporation. Arctic Intermediate Water, flows from north to become the source for North Atlantic Deep Water south of the Greenland-Scotland sill. These two intermediate waters have different salinity in the western and eastern basins. The wide range of salinities in the North Atlantic is caused by the asymmetry of the northern subtropical gyre and the large number of contributions from a wide range of sources: Labrador Sea, Norwegian-Greenland Sea, Mediterranean, and South Atlantic Intermediate Water.\n\nThe North Atlantic Deep Water (NADW) is a complex of four water masses, two that form by deep convection in the open ocean — Classical and Upper Labrador Sea Water — and two that form from the inflow of dense water across the Greenland-Iceland-Scotland sill — Denmark Strait and Iceland-Scotland Overflow Water. Along its path across Earth the composition of the NADW is affected by other water masses, especially Antarctic Bottom Water and Mediterranean Overflow Water.\nThe NADW is fed by a flow of warm shallow water into the northern North Atlantic which is responsible for the anomalous warm climate in Europe. Changes in the formation of NADW have been linked to global climate changes in the past. Since man-made substances were introduced into the environment, the path of the NADW can be traced throughout its course by measuring tritium and radiocarbon from nuclear weapon tests in the 1960s and CFCs.\n\nThe clockwise warm-water North Atlantic Gyre occupies the northern Atlantic, and the counter-clockwise warm-water South Atlantic Gyre appears in the southern Atlantic.\n\nIn the North Atlantic surface circulation is dominated by three inter-connected currents: the Gulf Stream which flows north-east from the North American coast at Cape Hatteras; the North Atlantic Current, a branch of the Gulf Stream which flows northward from the Grand Banks; and the Subpolar Front, an extension of the North Atlantic Current, a wide, vaguely defined region separating the subtropical gyre from the subpolar gyre. This system of currents transport warm water into the North Atlantic, without which temperatures in the North Atlantic and Europe would plunge dramatically.\nNorth of the North Atlantic Gyre, the cyclonic North Atlantic Subpolar Gyre plays a key role in climate variability. It is governed by ocean currents from marginal seas and regional topography, rather than being steered by wind, both in the deep ocean and at sea level.\nThe subpolar gyre forms an important part of the global thermohaline circulation. Its eastern portion includes eddying branches of the North Atlantic Current which transport warm, saline waters from the subtropics to the north-eastern Atlantic. There this water is cooled during winter and forms return currents that merge along the eastern continental slope of Greenland where they form an intense (40–50 Sv) current which flows around the continental margins of the Labrador Sea. A third of this water become parts of the deep portion of the North Atlantic Deep Water (NADW). The NADW, in its turn, feed the meridional overturning circulation (MOC), the northward heat transport of which is threatened by anthropogenic climate change. Large variations in the subpolar gyre on a decade-century scale, associated with the North Atlantic oscillation, are especially pronounced in Labrador Sea Water, the upper layers of the MOC.\n\nThe South Atlantic is dominated by the anti-cyclonic southern subtropical gyre. The South Atlantic Central Water originates in this gyre, while Antarctic Intermediate Water originates in the upper layers of the circumpolar region, near the Drake Passage and Falkland Islands. Both these currents receive some contribution from the Indian Ocean. On the African east coast the small cyclonic Angola Gyre lies embedded in the large subtropical gyre.\nThe southern subtropical gyre is partly masked by a wind-induced Ekman layer. The residence time of the gyre is 4.4–8.5 years. North Atlantic Deep Water flows southerward below the thermocline of the subtropical gyre.\n\nThe Sargasso Sea in the western North Atlantic can be defined as the area where two species of \"Sargassum\" (\"S. fluitans\" and \"natans\") float, an area wide and encircled by the Gulf Stream, North Atlantic Drift, and North Equatorial Current. This population of seaweed probably originated from Tertiary ancestors on the European shores of the former Tethys Ocean and has, if so, maintained itself by vegetative growth, floating in the ocean for millions of years.\nOther species endemic to the Sargasso Sea include the sargassum fish, a predator with algae-like appendages who hovers motionless among the \"Sargassum\". Fossils of similar fishes have been found in fossil bays of the former Tethys Ocean, in what is now the Carpathian region, that were similar to the Sargasso Sea. It is possible that the population in the Sargasso Sea migrated to the Atlantic as the Tethys closed at the end of the Miocene around 17 Ma.\nThe origin of the Sargasso fauna and flora remained enigmatic for centuries. The fossils found in the Carpathians in the mid-20th century, often called the \"quasi-Sargasso assemblage\", finally showed that this assemblage originated in the Carpathian Basin from were it migrated over Sicily to the Central Atlantic where it evolved into modern species of the Sargasso Sea.\n\nThe location of the spawning ground for European eels remained unknown for decades. In the early 19th century it was discovered that the southern Sargasso Sea is the spawning ground for both the European and American eel and that the former migrate more than and the latter . Ocean currents such as the Gulf Stream transport eel larvae from the Sargasso Sea to foraging areas in North America, Europe, and Northern Africa. Recent but disputed research suggests that eels possibly use Earth's magnetic field to navigate through the ocean both as larvae and as adults.\n\nClimate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.\n\nThe oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.\n\nThe Gulf Stream and its northern extension towards Europe, the North Atlantic Drift is thought to have at least some influence on climate. For example, the Gulf Stream helps moderate winter temperatures along the coastline of southeastern North America, keeping it warmer in winter along the coast than inland areas. The Gulf Stream also keeps extreme temperatures from occurring on the Florida Peninsula. In the higher latitudes, the North Atlantic Drift, warms the atmosphere over the oceans, keeping the British Isles and north-western Europe mild and cloudy, and not severely cold in winter like other locations at the same high latitude. The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas.\n\nIcebergs are common from February to August in the Davis Strait, Denmark Strait, and the northwestern Atlantic and have been spotted as far south as Bermuda and Madeira. Ships are subject to superstructure icing in the extreme north from October to May. Persistent fog can be a maritime hazard from May to September, as can hurricanes north of the equator (May to December).\n\nThe United States' southeast coast, especially the Virginia and North Carolina coasts, has a long history of shipwrecks due to its many shoals and reefs.\n\nThe Bermuda Triangle is popularly believed to be the site of numerous aviation and shipping incidents because of unexplained and supposedly mysterious causes, but Coast Guard records do not support this belief.\n\nHurricanes are also a natural hazard in the Atlantic, but mainly in the northern part of the ocean, rarely tropical cyclones form in the southern parts. Hurricanes usually form annually between June and November.\n\nThe break-up of Pangaea began in the Central Atlantic, between North America and Northwest Africa, where rift basins opened during the Late Triassic and Early Jurassic. This period also saw the first stages of the uplift of the Atlas Mountains. The exact timing is controversial with estimates ranging from 200 to 170 Ma.\n\nThe opening of the Atlantic Ocean coincided with the initial break-up of the supercontinent Pangaea, both of which were initiated by the eruption of the Central Atlantic Magmatic Province (CAMP), one of the most extensive and voluminous large igneous provinces in Earth's history associated with the Triassic–Jurassic extinction event, one of Earth's major extinction events.\nTheoliitic dikes, sills, and lava flows from the CAMP eruption at 200 Ma have been found in West Africa, eastern North America, and northern South America. The extent of the volcanism has been estimated to of which covered what is now northern and central Brazil.\n\nThe formation of the Central American Isthmus closed the Central American Seaway at the end of the Pliocene 2.8 Ma ago. The formation of the isthmus resulted in the migration and extinction of many land-living animals, known as the Great American Interchange, but the closure of the seaway resulted in a \"Great American Schism\" as it affected ocean currents, salinity, and temperatures in both the Atlantic and Pacific. Marine organisms on both sides of the isthmus became isolated and either diverged or went extinct.\n\nGeologically the Northern Atlantic is the area delimited to the south by two conjugate margins, Newfoundland and Iberia, and to the north by the Arctic Eurasian Basin. The opening of the Northern Atlantic closely followed the margins of its predecessor, the Iapetus Ocean, and spread from the Central Atlantic in six stages: Iberia–Newfoundland, Porcupine–North America, Eurasia–Greenland, Eurasia–North America. Active and inactive spreading systems in this area are marked by the interaction with the Iceland hotspot.\n\nWest Gondwana (South America and Africa) broke up in the Early Cretaceous to form the South Atlantic. The apparent fit between the coastlines of the two continents was noted on the first maps that included the South Atlantic and it was also the subject of the first computer-assisted plate tectonic reconstructions in 1965. This magnificent fit, however, has since then proven problematic and later reconstructions have introduced various deformation zones along the shorelines to accommodate the northward-propagating break-up. Intra-continental rifts and deformations have also been introduced to subdivide both continental plates into sub-plates.\n\nGeologically the South Atlantic can be divided into four segments: Equatorial segment, from 10°N to the Romanche Fracture Zone (RFZ);; Central segment, from RFZ to Florianopolis Fracture Zone (FFZ, north of Walvis Ridge and Rio Grande Rise); Southern segment, from FFZ to the Agulhas-Falkland Fracture Zone (AFFZ); and Falkland segment, south of AFFZ.\n\nIn the southern segment the Early Cretaceous (133–130 Ma) intensive magmatism of the Paraná–Etendeka Large Igneous Province produced by the Tristan hotspot resulted in an estimated volume of . It covered an area of in Brazil, Paraguay, and Uruguay and in Africa. Dyke swarms in Brazil, Angola, eastern Paraguay, and Namibia, however, suggest the LIP originally covered a much larger area and also indicate failed rifts in all these areas. Associated offshore basaltic flows reach as far south as the Falkland Islands and South Africa. Traces of magmatism in both offshore and onshore basins in the central and southern segments have been dated to 147–49 Ma with two peaks between 143–121 Ma and 90–60 Ma.\n\nIn the Falkland segment rifting began with dextral movements between the Patagonia and Colorado sub-plates between the Early Jurassic (190 Ma) and the Early Cretaceous (126.7 Ma). Around 150 Ma sea-floor spreading propagated northward into the southern segment. No later than 130 Ma rifting had reached the Walvis Ridge–Rio Grande Rise.\n\nIn the central segment rifting started to break Africa in two by opening the Benue Trough around 118 Ma. Rifting in the central segment, however, coincided with the Cretaceous Normal Superchron (also known as the Cretaceous quiet period), a 40 Ma period without magnetic reversals, which makes it difficult to date sea-floor spreading in this segment.\n\nThe equatorial segment is the last phase of the break-up, but, because it is located on the Equator, magnetic anomalies cannot be used for dating. Various estimates date the propagation of sea-floor spreading in this segment to the period 120–96 Ma. This final stage, nevertheless, coincided with or resulted in the end of continental extension in Africa.\n\nAbout 50 Ma the opening of the Drake Passage resulted from a change in the motions and separation rate of the South American and Antarctic plates. First small ocean basins opened and a shallow gateway appeared during the Middle Eocene. 34–30 Ma a deeper seaway developed, followed by an Eocene–Oligocene climatic deterioration and the growth of the Antarctic ice sheet.\n\nAn embryonic subduction margin is potentially developing west of Gibraltar. The Gibraltar Arc in the western Mediterranean is migrating westward into the Central Atlantic where it joins the converging African and Eurasian plates. Together these three tectonic forces are slowly developing into a new subduction system in the eastern Atlantic Basin. Meanwhile, the Scotia Arc and Caribbean Plate in the western Atlantic Basin are eastward-propagating subduction systems that might, together with the Gibraltar system, represent the beginning of the closure of the Atlantic Ocean and the final stage of the Atlantic Wilson Cycle.\n\nHumans evolved in Africa; first by diverging from other apes around 7 Ma; then developing stone tools around 2.6 Ma; to finally evolve as modern humans around 100 kya. The earliest evidences for the complex behavior associated with this behavioral modernity has been found in the Greater Cape Floristic Region (GCFR) along the coast of South Africa. During the latest glacial stages the now-submerged plains of the Agulhas Bank were exposed above sea level, extending the South African coastline farther south by hundreds of kilometers. A small population of modern humans — probably fewer than a thousand reproducing individuals — survived glacial maxima by exploring the high diversity offered by these Palaeo-Agulhas plains. The GCFR is delimited to the north by the Cape Fold Belt and the limited space south of it resulted in the development of social networks out of which complex Stone Age technologies emerged. Human history thus begins on the coasts of South Africa where the Atlantic Benguela Upwelling and Indian Ocean Agulhas Current meet to produce an intertidal zone on which shellfish, fur seal, fishes and sea birds provided the necessary protein sources.\nThe African origin of this modern behaviour is evidenced by 70,000 years-old engravings from Blombos Cave, South Africa.\n\nMitochondrial DNA (mtDNA) studies indicate that 80–60,000 years ago a major demographic expansion within Africa, derived from a single, small population, coincided with the emergence of behavioral complexity and the rapid MIS 5–4 environmental changes. This group of people not only expanded over the whole of Africa, but also started to disperse out of Africa into Asia, Europe, and Australasia around 65,000 years ago and quickly replaced the archaic humans in these regions. During the Last Glacial Maximum (LGM) 20,000 years ago humans had to abandon their initial settlements along the European North Atlantic coast and retreat to the Mediterranean. Following rapid climate changes at the end of the LGM this region was repopulated by Magdalenian culture. Other hunter-gatherers followed in waves interrupted by large-scale hazards such as the Laacher See volcanic eruption, the inundation of Doggerland (now the North Sea), and the formation of the Baltic Sea. The European coasts of the North Atlantic were permanently populated about 9–8.5 thousand years ago.\n\nThis human dispersal left abundant traces along the coasts of the Atlantic Ocean. 50 ka-old, deeply stratified shell middens found in Ysterfontein on the western coast of South Africa are associated with the Middle Stone Age (MSA). The MSA population was small and dispersed and the rate of their reproduction and exploitation was less intense than those of later generations. While their middens resemble 12–11 ka-old Late Stone Age (LSA) middens found on every inhabited continent, the 50–45 ka-old Enkapune Ya Muto in Kenya probably represents the oldest traces of the first modern humans to disperse out of Africa.\n\nThe same development can be seen in Europe. In La Riera Cave (23–13 ka) in Asturias, Spain, only some 26,600 molluscs were deposited over 10 ka. In contrast, 8–7 ka-old shell middens in Portugal, Denmark, and Brazil generated thousands of tons of debris and artefacts. The Ertebølle middens in Denmark, for example, accumulated of shell deposits representing some 50 million molluscs over only a thousand years. This intensification in the exploitation of marine resources has been described as accompanied by new technologies — such as boats, harpoons, and fish-hooks — because many caves found in the Mediterranean and on the European Atlantic coast have increased quantities of marine shells in their upper levels and reduced quantities in their lower. The earliest exploitation, however, took place on the now submerged shelves, and most settlements now excavated were then located several kilometers from these shelves. The reduced quantities of shells in the lower levels can represent the few shells that were exported inland.\n\nDuring the LGM the Laurentide Ice Sheet covered most of northern North America while Beringia connected Siberia to Alaska. In 1973 late U.S. geoscientist Paul S. Martin proposed a \"blitzkrieg\" colonization of America by which Clovis hunters migrated into North America around 13,000 years ago in a single wave through an ice-free corridor in the ice sheet and \"spread southward explosively, briefly attaining a density sufficiently large to overkill much of their prey.\" Others later proposed a \"three-wave\" migration over the Bering Land Bridge. These hypotheses remained the long-held view regarding the settlement of the Americas, a view challenged by more recent archaeological discoveries: the oldest archaeological sites in the Americas have been found in South America; sites in north-east Siberia report virtually no human presence there during the LGM; and most Clovis artefacts have been found in eastern North America along the Atlantic coast. Furthermore, colonisation models based on mtDNA, yDNA, and atDNA data respectively support neither the \"blitzkrieg\" nor the \"three-wave\" hypotheses but they also deliver mutually ambiguous results. Contradictory data from archaeology and genetics will most likely deliver future hypotheses that will, eventually, confirm each other. A proposed route across the Pacific to South America could explain early South American finds and another hypothesis proposes a northern path, through the Canadian Arctic and down the North American Atlantic coast.\nEarly settlements across the Atlantic have been suggested by alternative theories, ranging from purely hypothetical to mostly disputed, including the Solutrean hypothesis and some of the Pre-Columbian trans-oceanic contact theories.\nThe Norse settlement of the Faroe Islands and Iceland began during the 9th and 10th centuries. A settlement on Greenland was established before 1000 CE, but contact with it was lost in 1409 and it was finally abandoned during the early Little Ice Age. This setback was caused by a range of factors: an unsustainable economy resulted in erosion and denudation, while conflicts with the local Inuit resulted in the failure to adapt their Arctic technologies; a colder climate resulted in starvation; and the colony got economically marginalized as the Great Plague and Barbary pirates harvested its victims on Iceland in the 15th century.\nIceland was initially settled 865–930 CE following a warm period when winter temperatures hovered around which made farming favorable at high latitudes. This did not last, however, and temperatures quickly dropped; at 1080 CE summer temperatures had reached a maximum of . The \"Landnámabók\" (\"Book of Settlement\") records disastrous famines during the first century of settlement — \"men ate foxes and ravens\" and \"the old and helpless were killed and thrown over cliffs\" — and by the early 1200s hay had to be abandoned for short-season crops such as barley.\n\nChristopher Columbus discovered the Americas in 1492 under Spanish flag. Six years later Vasco da Gama reached India under Portuguese flag, by navigating south around the Cape of Good Hope, thus proving that the Atlantic and Indian Oceans are connected. In 1500, in his voyage to India following Vasco da Gama, Pedro Alvares Cabral reached Brazil, taken by the currents of the South Atlantic Gyre. Following these explorations, Spain and Portugal quickly conquered and colonized large territories in the New World and forced the Native American population into slavery in order to explore the vast quantities of silver and gold they found. Spain and Portugal monopolized this trade in order to keep other European nations out, but conflicting interests nevertheless lead to a series of Spanish-Portuguese wars. A peace treaty mediated by the Pope divided the conquered territories into Spanish and Portuguese sectors while keeping other colonial powers away. England, France, and the Dutch Republic enviously watched the Spanish and Portuguese wealth grow and allied themselves with pirates such as Henry Mainwaring and Alexandre Exquemelin. They could explore the convoys leaving America because prevailing winds and currents made the transport of heavy metals slow and predictable.\nIn the American colonies depredation, disease, and slavery quickly reduced the indigenous American population to the extent that the Atlantic slave trade had to be introduced to replace them — a trade that became norm and an integral part of the colonization. Between the 15th century and 1888, when Brazil became the last part of America to end slave trade, an estimated ten million Africans were exported as slaves, most of them destined for agricultural labour. The slave trade was officially abolished in the British Empire and the United States in 1808, and slavery itself was abolished in the British Empire in 1838 and in the U.S. in 1865 after the Civil War.\n\nFrom Columbus to the Industrial Revolution Trans-Atlantic trade, including colonialism and slavery, became crucial for Western Europe. For European countries with a direct access to the Atlantic (including Britain, France, the Netherlands, Portugal, and Spain) 1500–1800 was a period of sustained growth during which these countries grew richer than those in Eastern Europe and Asia. Colonialism evolved as part of the Trans-Atlantic trade, but this trade also strengthened the position of merchant groups at the expense of monarchs. Growth was more rapid in non-absolutist countries, such as Britain and the Netherlands, and more limited in absolutist monarchies, such as Portugal, Spain, and France, where profit mostly or exclusively benefited the monarchy and its allies.\n\nTrans-Atlantic trade also resulted in an increasing urbanization: in European countries facing the Atlantic urbanization grew from 8% in 1300, 10.1% in 1500, to 24.5% in 1850; in other European countries from 10% in 1300, 11.4% in 1500, to 17% in 1850. Likewise, GDP doubled in Atlantic countries but rose by only 30% in the rest of Europe. By end of the 17th century the volume of the Trans-Atlantic trade had surpassed that of the Mediterranean trade.\n\nThe Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves.\n\nThe Atlantic harbors petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones.\nGold deposits are a mile or two under water on the ocean floor, however the deposits are also encased in rock that must be mined through. Currently, there is no cost-effective way to mine or extract gold from the ocean to make a profit.\n\nVarious international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea.\n\nThe shelves of the Atlantic hosts one of the world's richest fishing resources. The most productive areas include the Grand Banks of Newfoundland, the Scotian Shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Bay of Fundy, the Dogger Bank of the North Sea, and the Falkland Banks.\nFisheries have, however, undergone significant changes since the 1950s and global catches can now be divided into three groups of which only two are observed in the Atlantic: fisheries in the Eastern Central and South-West Atlantic oscillate around a globally stable value, the rest of the Atlantic is in overall decline following historical peaks. The third group, \"continuously increasing trend since 1950\", is only found in the Indian Ocean and Western Pacific.\nIn the North-East Atlantic total catches decreased between the mid-1970s and the 1990s and reached 8.7 million tons in 2013. Blue whiting reached a 2.4 million tons peak in 2004 but was down to 628,000 tons in 2013. Recovery plans for cod, sole, and plaice have reduced mortality in these species. Arctic cod reached its lowest levels in the 1960s–1980s but is now recovered. Arctic saithe and haddock are considered fully fished; Sand eel is overfished as was capelin which has now recovered to fully fished. Limited data makes the state of redfishes and deep-water species difficult to assess but most likely they remain vulnerable to overfishing. Stocks of northern shrimp and Norwegian lobster are in good condition. In the North-East Atlantic 21% of stocks are considered overfished.\nIn the North-West Atlantic landings have decreased from 4.2 million tons in the early 1970s to 1.9 million tons in 2013. During the 21th century some species have shown weak signs of recovery, including Greenland halibut, yellowtail flounder, Atlantic halibut, haddock, spiny dogfish, while other stocks shown no such signs, including cod, witch flounder, and redfish. Stocks of invertebrates, in contrast, remain at record levels of abundance. 31% of stocks are overfished in the North-west Atlantic.\n\nIn 1497 John Cabot became the first to explore mainland North America and one of his major discoveries was the abundant resources of Atlantic cod off Newfoundland. Referred to as \"Newfoundland Currency\" this discovery supplied mankind with some 200 million tons of fish over five centuries. In the late 19th and early 20th centuries new fisheries started to exploit haddock, mackerel, and lobster. From the 1950s to the 1970s the introduction of European and Asian distant-water fleets in the area dramatically increased the fishing capacity and number of exploited species. It also expanded the exploited areas from near-shore to the open sea and to great depths to include deep-water species such as redfish, Greenland halibut, witch flounder, and grenadiers. Overfishing in the area was recognised as early as the 1960s but, because this was occurring on international waters, it took until the late 1970s before any attempts to regulate was made. In the early 1990s this finally resulted in the collapse of the Atlantic northwest cod fishery. The population of a number of deep-sea fishes also collapsed in the process, including American plaice, redfish, and Greenland halibut, together with flounder and grenadier.\n\nIn the Eastern Central Atlantic small pelagic fishes constitute about 50% of landings with sardine reaching 0.6–1.0 million tons per year. Pelagic fish stocks are considered fully fishes or overfished, with sardines south of Cape Bojador the notable exception. Almost half of stocks are fished at biologically unsustainable levels. Total catches have been fluctuating since the 1970s; reaching 3.9 million tons in 2013 or slightly less than the peak production in 2010.\nIn the Western Central Atlantic catches have been decreasing since 2000 and reached 1.3 million tons in 2013. The most important species in the area, Gulf menhaden, reached a million tons in the mid-1980s but only half a million tons in 2013 and is now considered fully fished. Round sardinella was an important species in the 1990s but is now considered overfished. Groupers and snappers are overfished and northern brown shrimp and American cupped oyster are considered fully fished approaching overfished. 44% of stocks are being fished at unsustainable levels.\nIn the South-East Atlantic catches have decreased from 3.3 million tons in the early 1970s to 1.3 million tons in 2013. Horse mackerel and hake are the most important species, together representing almost half of the landings. Off South Africa and Namibia deep-water hake and shallow-water Cape hake have recovered to sustainable levels since regulations were introduced in 2006 and the states of Southern African pilchard and anchovy have improved to fully fished in 2013.\n\nIn the South-West Atlantic a peak was reached in the mid-1980s and catches now fluctuate between 1.7 and 2.6 million tons. The most important species, the Argentine shortfin squid, which reached half a million tons in 2013 or half the peak value, is considered fully fished to overfished. Another important species was the Brazilian sardinella, with a production of 100,000 tons in 2013 it is now considered overfished. Half the stocks in this area are being fished at unsustainable levels: Whitehead’s round herring has not yet reached fully fished but Cunene horse mackerel is overfished. The sea snail perlemoen abalone is targeted by illegal fishing and remain overfished.\n\nEndangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea.\n\nNorth Atlantic hurricane activity has increased over past decades because of increased sea surface temperature (SST) at tropical latitudes, changes that can be attributed to either the natural Atlantic Multidecadal Oscillation (AMO) or to anthropogenic climate change.\nA 2005 report indicated that the Atlantic meridional overturning circulation (AMOC) slowed down by 30% between 1957 and 2004.\nIf the AMO was responsible for SST variability then the AMOC would have increased in strength, which is apparently not the case. Furthermore, it is clear from statistical analyses of annual tropical cyclones that these changes do not display multidecadal cyclicity. Therefore, these changes in SST must be caused by human activities.\n\nThe ocean mixed layer plays an important role heat storage over seasonal and decadal time-scales, whereas deeper layers are affected over millennia and has a heat capacity about 50 times that of the mixed layer. This heat uptake provides a time-lag for climate change but it also results in a thermal expansion of the oceans which contribute to sea-level rise. 21st century global warming will probably result in an equilibrium sea-level rise five times greater than today, whilst melting of glaciers, including that of the Greenland ice-sheet, expected to have virtually no effect during the 21st century, will probably result in a sea-level rise of 3–6 m over a millennium.\n\nOn 7 June 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature.\n\nMarine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.\n\nMarine debris, which is also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.\n\n\n"}
{"id": "700", "url": "https://en.wikipedia.org/wiki?curid=700", "title": "Arthur Schopenhauer", "text": "Arthur Schopenhauer\n\nArthur Schopenhauer (; 22 February 1788 – 21 September 1860) was a German philosopher. He is best known for his 1818 work \"The World as Will and Representation\" (expanded in 1844), wherein he characterizes the phenomenal world as the product of a blind and insatiable metaphysical will. Proceeding from the transcendental idealism of Immanuel Kant, Schopenhauer developed an atheistic metaphysical and ethical system that has been described as an exemplary manifestation of philosophical pessimism, rejecting the contemporaneous post-Kantian philosophies of German idealism. Schopenhauer was among the first thinkers in Western philosophy to share and affirm significant tenets of Eastern philosophy (e.g., asceticism, the world-as-appearance), having initially arrived at similar conclusions as the result of his own philosophical work.\n\nThough his work failed to garner substantial attention during his life, Schopenhauer has had a posthumous impact across various disciplines, including philosophy, literature, and science. His writing on aesthetics, morality, and psychology would exert important influence on thinkers and artists throughout the 19th and 20th centuries. Those who have cited his influence include Friedrich Nietzsche, Richard Wagner, Leo Tolstoy, Ludwig Wittgenstein, Erwin Schrödinger, Otto Rank, Gustav Mahler, Joseph Campbell, Albert Einstein, Carl Jung, Thomas Mann, Emile Zola, Guy de Maupassant, Jorge Luis Borges and Samuel Beckett, among others. (A thinker who was widely believed to have been influenced by Schopenhauer, but denied it, was George Bernard Shaw.)\n\nSchopenhauer was born on 22 February 1788, in the city of Danzig (then part of the Polish–Lithuanian Commonwealth; present day Gdańsk, Poland) on Heiligegeistgasse (known in the present day as Św. Ducha 47), the son of Johanna Schopenhauer (née Trosiener) and Heinrich Floris Schopenhauer, both descendants of wealthy German patrician families. When Danzig became part of Prussia in 1793, Heinrich moved to Hamburg, although his firm continued trading in Danzig. As early as 1799, Arthur started playing the flute. In 1805, Schopenhauer's father died, possibly by suicide. Arthur endured two long years of drudgery as a merchant in honor of his dead father, but his mother soon moved with his sister Adele to Weimar—then the centre of German literature—to pursue her writing career. He dedicated himself wholly to studies at the Gotha gymnasium () in Saxe-Gotha-Altenburg, but left in disgust after seeing one of the masters lampooned.\nBy that time, Johanna Schopenhauer had already opened her famous salon, and Arthur was not compatible with what he considered its vain and ceremonious ways. He was also disgusted by the ease with which his mother had forgotten his father's memory. He left to become a student at the University of Göttingen in 1809. There he studied metaphysics and psychology under Gottlob Ernst Schulze, the author of \"Aenesidemus\", who advised him to concentrate on Plato and Immanuel Kant. In Berlin, from 1811 to 1812, he had attended lectures by the prominent post-Kantian philosopher Johann Gottlieb Fichte and the theologian Friedrich Schleiermacher.\n\nSchopenhauer had a notably strained relationship with his mother Johanna. He wrote his first book, \"On the Fourfold Root of the Principle of Sufficient Reason\", while at university. His mother informed him that the book was incomprehensible and it was unlikely that anyone would ever buy a copy. In a fit of temper Arthur Schopenhauer told her that his work would be read long after the \"rubbish\" she wrote would have been totally forgotten. In fact, although they considered her novels of dubious quality, the Brockhaus publishing firm held her in high esteem because they consistently sold well. Hans Brockhaus later recalled that, when she brought them some of her son's work, his predecessors \"saw nothing in this manuscript, but wanted to please one of our best-selling authors by publishing her son's work. We published more and more of her son Arthur's work and today nobody remembers Johanna, but her son's works are in steady demand and contribute to Brockhaus'[s] reputation.\" He kept large portraits of the pair in his office in Leipzig for the edification of his new editors.\n\nIn 1814, Schopenhauer began his seminal work \"The World as Will and Representation\" (\"Die Welt als Wille und Vorstellung\"). He finished it in 1818 and Brockhaus published it that December. In Dresden in 1819, Schopenhauer fathered, with a servant, an illegitimate daughter who was born and died the same year. In 1820, Schopenhauer became a lecturer at the University of Berlin. He scheduled his lectures to coincide with those of the famous philosopher G. W. F. Hegel, whom Schopenhauer described as a \"clumsy charlatan\". However, only five students turned up to Schopenhauer's lectures, and he dropped out of academia. A late essay, \"On University Philosophy\", expressed his resentment towards the work conducted in academies.\n\nWhile in Berlin, Schopenhauer was named as a defendant in a lawsuit initiated by a woman named Caroline Marquet. She asked for damages, alleging that Schopenhauer had pushed her. According to Schopenhauer's court testimony, she deliberately annoyed him by raising her voice while standing right outside his door. Marquet alleged that the philosopher had assaulted and battered her after she refused to leave his doorway. Her companion testified that she saw Marquet prostrate outside his apartment. Because Marquet won the lawsuit, Schopenhauer made payments to her for the next twenty years. When she died, he wrote on a copy of her death certificate, \"Obit anus, abit onus\" (\"The old woman dies, the burden is lifted\"). In 1819 the fortunes of his mother and sister, and himself, were threatened by the failure of the firm in Danzig in which his father had been a director and shareholder. His sister accepted a compromise compensation package of 70 per cent, but Schopenhauer angrily refused this, and eventually recovered 9400 thalers.\n\nIn 1821, he fell in love with nineteen-year-old opera singer, Caroline Richter (called Medon), and had a relationship with her for several years, but did not marry her. When he was forty-three years old, he took interest in seventeen-year-old Flora Weiss but she rejected him as recorded in her diary.\nIn 1831, a cholera epidemic broke out in Berlin and Schopenhauer left the city. Schopenhauer settled permanently in Frankfurt in 1833, where he remained for the next twenty-seven years, living alone except for a succession of pet poodles named Atman and Butz. The numerous notes that he made during these years, amongst others on aging, were published posthumously under the title \"Senilia\". Schopenhauer had a robust constitution, but in 1860 his health began to deteriorate. He died of pulmonary-respiratory failure, on 21 September 1860 while sitting at home on his couch. He was 72.\n\nA key focus of Schopenhauer was his investigation of individual motivation. Before Schopenhauer, Hegel had popularized the concept of \"Zeitgeist\", the idea that society consisted of a collective consciousness that moved in a distinct direction, dictating the actions of its members. Schopenhauer, a reader of both Kant and Hegel, criticized their logical optimism and the belief that individual morality could be determined by society and reason. Schopenhauer believed that humans were motivated by only their own basic desires, or (\"Will to Live\"), which directed all of mankind. Will, for Schopenhauer, is what Kant called the \"thing-in-itself\".\n\nFor Schopenhauer, human desire was futile, illogical, directionless, and, by extension, so was all human action in the world. Einstein paraphrased his views as follows: \"Man can indeed do what he wants, but he cannot will what he wants.\" In this sense, he adhered to the Fichtean principle of idealism: \"The world is \"for\" a subject.\" This idealism so presented, immediately commits it to an ethical attitude, unlike the purely epistemological concerns of Descartes and Berkeley. To Schopenhauer, the Will is a blind force that controls not only the actions of individual, intelligent agents, but ultimately all observable phenomena—an evil to be terminated via mankind's duties: asceticism and chastity. He is credited with one of the most famous opening lines of philosophy: \"The world is my representation.\" Friedrich Nietzsche was greatly influenced by this idea of Will, although he eventually rejected it.\n\nFor Schopenhauer, human desiring, \"willing\", and craving cause suffering or pain. A temporary way to escape this pain is through aesthetic contemplation (a method comparable to Zapffe's \"\"Sublimation\"\"). Aesthetic contemplation allows one to escape this pain—albeit temporarily—because it stops one perceiving the world as mere presentation. Instead, one no longer perceives the world as an object of perception (therefore as subject to the Principle of Sufficient Grounds; time, space and causality) from which one is separated; rather one becomes one with that perception: \"\"one can thus no longer separate the perceiver from the perception\"\" (\"The World as Will and Representation\", section 34). From this immersion with the world one no longer views oneself as an individual who suffers in the world due to one's individual will but, rather, becomes a \"\"subject of cognition\"\" to a perception that is \"\"Pure, will-less, timeless\"\" (section 34) where the essence, \"ideas\", of the world are shown. Art is the practical consequence of this brief aesthetic contemplation as it attempts to depict one's immersion with the world, thus tries to depict the essence/pure ideas of the world. Music, for Schopenhauer, was the purest form of art because it was the one that depicted the will itself without it appearing as subject to the Principle of Sufficient Grounds, therefore as an individual object. According to Daniel Albright, \"Schopenhauer thought that music was the only art that did not merely copy ideas, but actually embodied the will itself\".\n\nHe deemed music a timeless, universal language comprehended everywhere, that can imbue global enthusiasm, if in possession of a significant melody.\n\nSchopenhauer's realist views on mathematics are evident in his criticism of the contemporary attempts to prove the parallel postulate in Euclidean geometry. Writing shortly before the discovery of hyperbolic geometry demonstrated the logical independence of the axiom—and long before the general theory of relativity revealed that it does not necessarily express a property of physical space—Schopenhauer criticized mathematicians for trying to use indirect concepts to prove what he held to be directly evident from perception.\nThroughout his writings, Schopenhauer criticized the logical derivation of philosophies and mathematics from mere concepts, instead of from intuitive perceptions.\n\nAlthough Schopenhauer could see no justification for trying to prove Euclid's parallel postulate, he did see a reason for examining another of Euclid's axioms.\nThis follows Kant's reasoning.\n\nSchopenhauer's moral theory proposed that only compassion can drive moral acts. According to Schopenhauer, compassion alone is the good of the object of the acts, that is, they cannot be inspired by either the prospect of personal utility or the feeling of duty. Mankind can also be guided by egoism and malice. Egotistic acts are those guided by self-interest, desire for pleasure or happiness. Schopenhauer believed most of our deeds belong to this class. Acts of malice are different from egotistic acts. As in the case of acts of compassion, these do not target personal utility. Their aim is to cause damage to others, independently of personal gains. He believed, like Swami Vivekananda in the unity of all with one-self and also believed that ego is the origin of pain and conflicts, that reduction of ego frames the moral principles.\n\nAccording to Schopenhauer, whenever we make a choice, \"We assume as necessary that decision was preceded by something from which it ensued, and which we call the ground or reason, or more accurately the motive, of the resultant action.\" Choices are not made freely. Our actions are necessary and determined because \"every human being, even every animal, after the motive has appeared, must carry out the action which alone is in accordance with his inborn and immutable character\". A definite action inevitably results when a particular motive influences a person's given, unchangeable character.\nThe State, Schopenhauer claimed, punishes criminals to prevent future crimes. It does so by placing \"beside every possible motive for committing a wrong a more powerful motive for leaving it undone, in the inescapable punishment. Accordingly, the criminal code is as complete a register as possible of counter-motives to all criminal actions that can possibly be imagined...\"\n\nShould capital punishment be legal? \"For safeguarding the lives of citizens,\" he asserted, \"capital punishment is therefore absolutely necessary\". \"The murderer,\" wrote Schopenhauer, \"who is condemned to death according to the law must, it is true, be now used as a mere \"means\", and with complete right. For public security, which is the principal object of the State, is disturbed by him; indeed it is abolished if the law remains unfulfilled. The murderer, his life, his person, must be the \"means\" of fulfilling the law, and thus of re-establishing public security.\" Schopenhauer disagreed with those who would abolish capital punishment. \"Those who would like to abolish it should be given the answer: 'First remove murder from the world, and then capital punishment ought to follow.\n\nPeople, according to Schopenhauer, cannot be improved. They can only be influenced by strong motives that overpower criminal motives. Schopenhauer declared that \"real moral reform is not at all possible, but only determent from the deed...\".\n\nHe claimed this doctrine was not original to him. Previously, it appeared in the writings of Plato, Seneca, Hobbes, Pufendorf, and Anselm Feuerbach. Schopenhauer declared that their teaching was corrupted by subsequent errors and therefore was in need of clarification.\n\nEven though Schopenhauer ended his treatise on the freedom of human will with the postulate of everyone's responsibility for their character and, consequently, acts—the responsibility following from one's being the Will as noumenon (from which also all the characters and creations come)—he considered his views incompatible with theism, on grounds of fatalism and, more generally, responsibility for evil. In Schopenhauer's philosophy the dogmas of Christianity lose their significance, and the \"Last Judgment\" is no longer preceded by anything—\"The world is itself the Last Judgment on it.\" Whereas God, if he existed, would be evil.\n\nPhilosophers have not traditionally been impressed by the tribulations of sex, but Schopenhauer addressed it and related concepts forthrightly:\n\nHe named a force within man that he felt took invariable precedence over reason: the Will to Live or Will to Life (\"Wille zum Leben\"), defined as an inherent drive within human beings, and indeed all creatures, to stay alive; a force that inveigles us into reproducing.\n\nSchopenhauer refused to conceive of love as either trifling or accidental, but rather understood it as an immensely powerful force that lay unseen within man's psyche and dramatically shaped the world:\n\nThese ideas foreshadowed the discovery of evolution, Freud's concepts of the libido and the unconscious mind, and evolutionary psychology in general.\n\nSchopenhauer's politics were, for the most part, an echo of his system of ethics (the latter being expressed in \"Die beiden Grundprobleme der Ethik\", available in English as two separate books, \"On the Basis of Morality\" and \"On the Freedom of the Will\"). Ethics also occupies about one quarter of his central work, \"The World as Will and Representation\".\n\nIn occasional political comments in his \"Parerga and Paralipomena\" and \"Manuscript Remains\", Schopenhauer described himself as a proponent of limited government. What was essential, he thought, was that the state should \"leave each man free to work out his own salvation\", and so long as government was thus limited, he would \"prefer to be ruled by a lion than one of [his] fellow rats\"—i.e., by a monarch, rather than a democrat. Schopenhauer shared the view of Thomas Hobbes on the necessity of the state, and of state action, to check the destructive tendencies innate to our species. He also defended the independence of the legislative, judicial and executive branches of power, and a monarch as an impartial element able to practise justice (in a practical and everyday sense, not a cosmological one). He declared monarchy as \"that which is natural to man\" for \"intelligence has always under a monarchical government a much better chance against its irreconcilable and ever-present foe, stupidity\" and disparaged republicanism as \"unnatural as it is unfavourable to the higher intellectual life and the arts and sciences\".\n\nSchopenhauer, by his own admission, did not give much thought to politics, and several times he writes proudly of how little attention he had paid \"to political affairs of [his] day\". In a life that spanned several revolutions in French and German government, and a few continent-shaking wars, he did indeed maintain his aloof position of \"minding not the times but the eternities\". He wrote many disparaging remarks about Germany and the Germans. A typical example is, \"For a German it is even good to have somewhat lengthy words in his mouth, for he thinks slowly, and they give him time to reflect.\"\n\nSchopenhauer attributed civilizational primacy to the northern \"white races\" due to their sensitivity and creativity (except for the ancient Egyptians and Hindus whom he saw as equal):\n\nThe highest civilization and culture, apart from the ancient Hindus and Egyptians, are found exclusively among the white races; and even with many dark peoples, the ruling caste or race is fairer in colour than the rest and has, therefore, evidently immigrated, for example, the Brahmans, the Incas, and the rulers of the South Sea Islands. All this is due to the fact that necessity is the mother of invention because those tribes that emigrated early to the north, and there gradually became white, had to develop all their intellectual powers and invent and perfect all the arts in their struggle with need, want and misery, which in their many forms were brought about by the climate. This they had to do in order to make up for the parsimony of nature and out of it all came their high civilization.\n\nDespite this, he was adamantly against differing treatment of races, was fervently anti-slavery, and supported the abolitionist movement in the United States. He describes the treatment of \"[our] innocent black brothers whom force and injustice have delivered into [the slave-master's] devilish clutches\" as \"belonging to the blackest pages of mankind's criminal record\".\n\nSchopenhauer additionally maintained a marked metaphysical and political anti-Judaism. Schopenhauer argued that Christianity constituted a revolt against what he styled the materialistic basis of Judaism, exhibiting an Indian-influenced ethics reflecting the Aryan-Vedic theme of spiritual \"self-conquest\". This he saw as opposed to what he held to be the ignorant drive toward earthly utopianism and superficiality of a worldly \"Jewish\" spirit:\n\nWhile all other religions endeavor to explain to the people by symbols the metaphysical significance of life, the religion of the Jews is entirely immanent and furnishes nothing but a mere war-cry in the struggle with other nations.\n\nIn Schopenhauer's 1851 essay \"On Women\", he expressed his opposition to what he called \"Teutonico-Christian stupidity\" of reflexive unexamined reverence (\"abgeschmackten Weiberveneration\") for the female. Schopenhauer wrote that \"Women are directly fitted for acting as the nurses and teachers of our early childhood by the fact that they are themselves childish, frivolous and short-sighted.\" He opined that women are deficient in artistic faculties and sense of justice, and expressed opposition to monogamy. Indeed, Rodgers and Thompson in \"Philosophers Behaving Badly\" call Schopenhauer \"a misogynist without rival in...Western philosophy\". He claimed that \"woman is by nature meant to obey\". The essay does give some compliments, however: that \"women are decidedly more sober in their judgment than [men] are\", and are more sympathetic to the suffering of others.\n\nSchopenhauer's controversial writings have influenced many, from Friedrich Nietzsche to nineteenth-century feminists. Schopenhauer's biological analysis of the difference between the sexes, and their separate roles in the struggle for survival and reproduction, anticipates some of the claims that were later ventured by sociobiologists and evolutionary psychologists.\n\nAfter the elderly Schopenhauer sat for a sculpture portrait by Elisabet Ney, he told Richard Wagner's friend Malwida von Meysenbug, \"I have not yet spoken my last word about women. I believe that if a woman succeeds in withdrawing from the mass, or rather raising herself above the mass, she grows ceaselessly and more than a man.\"\n\nNote for clarity in the following that \"genetics\" are but one component of \"heredity\". Though commonly used interchangeably, \"heritable traits\" would include socio-economic and other psycho-social potentialities. (See e.g. The difference between genetic and hereditary | Evolutionary Biology).\n\nSchopenhauer believed that personality and intellect were inherited. He quotes Horace's saying, \"From the brave and good are the brave descended\" (\"Odes\", iv, 4, 29) and Shakespeare's line from \"Cymbeline\", \"Cowards father cowards, and base things sire base\" (IV, 2) to reinforce his hereditarian argument.\nMechanistically, Schopenhauer believed that a person inherits his level of intellect through his mother, and personal character through one's father.\nThis belief in heritability of traits informed Schopenhauer's view of love – placing it at the highest level of importance. For Schopenhauer the \"final aim of all love intrigues, be they comic or tragic, is really of more importance than all other ends in human life. What it all turns upon is nothing less than the composition of the next generation... It is not the weal or woe of any one individual, but that of the human race to come, which is here at stake.\" This view of the importance for the species of whom we choose to love was reflected in his views on eugenics or good breeding. Here Schopenhauer wrote:\n\nWith our knowledge of the complete unalterability both of character and of mental faculties, we are led to the view that a real and thorough improvement of the human race might be reached not so much from outside as from within, not so much by theory and instruction as rather by the path of generation. Plato had something of the kind in mind when, in the fifth book of his \"Republic\", he explained his plan for increasing and improving his warrior caste. If we could castrate all scoundrels and stick all stupid geese in a convent, and give men of noble character a whole harem, and procure men, and indeed thorough men, for all girls of intellect and understanding, then a generation would soon arise which would produce a better age than that of Pericles.\n\nIn another context, Schopenhauer reiterated his antidemocratic-eugenic thesis: \"If you want Utopian plans, I would say: the only solution to the problem is the despotism of the wise and noble members of a genuine aristocracy, a genuine nobility, achieved by mating the most magnanimous men with the cleverest and most gifted women. This proposal constitutes my Utopia and my Platonic Republic.\" Analysts (e.g., Keith Ansell-Pearson) have suggested that Schopenhauer's advocacy of anti-egalitarianism and eugenics influenced the neo-aristocratic philosophy of Friedrich Nietzsche, who initially considered Schopenhauer his mentor.\n\nAs a consequence of his monistic philosophy, Schopenhauer was very concerned about the welfare of animals. For him, all individual animals, including humans, are essentially the same, being phenomenal manifestations of the one underlying Will. The word \"will\" designated, for him, force, power, impulse, energy, and desire; it is the closest word we have that can signify both the real essence of all external things and also our own direct, inner experience. Since every living thing possesses will, then humans and animals are fundamentally the same and can recognize themselves in each other. For this reason, he claimed that a good person would have sympathy for animals, who are our fellow sufferers.\n\nCompassion for animals is intimately associated with goodness of character, and it may be confidently asserted that he who is cruel to living creatures cannot be a good man.\nNothing leads more definitely to a recognition of the identity of the essential nature in animal and human phenomena than a study of zoology and anatomy.\n\nThe assumption that animals are without rights and the illusion that our treatment of them has no moral significance is a positively outrageous example of Western crudity and barbarity. Universal compassion is the only guarantee of morality.\n\nIn 1841, he praised the establishment, in London, of the Society for the Prevention of Cruelty to Animals, and also the Animals' Friends Society in Philadelphia. Schopenhauer even went so far as to protest against the use of the pronoun \"it\" in reference to animals because it led to the treatment of them as though they were inanimate things. To reinforce his points, Schopenhauer referred to anecdotal reports of the look in the eyes of a monkey who had been shot and also the grief of a baby elephant whose mother had been killed by a hunter.\n\nHe was very attached to his succession of pet poodles. Schopenhauer criticized Spinoza's belief that animals are to be used as a mere means for the satisfaction of humans.\n\nIn the third, expanded edition of \"The World as Will and Representation\" (1859), Schopenhauer added an appendix to his chapter on the \"Metaphysics of Sexual Love\". He also wrote that homosexuality did have the benefit of preventing ill-begotten children. Concerning this, he stated that \"the vice we are considering appears to work directly against the aims and ends of nature, and that in a matter that is all important and of the greatest concern to her it must in fact serve these very aims, although only indirectly, as a means for preventing greater evils.\"\n\nHe wrote that only those who were too old or too young to reproduce strong, healthy children would resort to pederasty (Schopenhauer considered pederasty in itself a vice). Shrewdly anticipating the interpretive distortion, on the part of the popular mind, of his attempted scientific \"explanation\" of pederasty as personal \"advocacy\" (when he had otherwise described the act, in terms of spiritual ethics, as an \"objectionable aberration\"), Schopenhauer sarcastically concludes the appendix with the statement that \"by expounding these paradoxical ideas, I wanted to grant to the professors of philosophy a small favour, for they are very disconcerted by the ever-increasing publicization of my philosophy which they so carefully concealed. I have done so by giving them the opportunity of slandering me by saying that I defend and commend pederasty.\"\n\nSchopenhauer read the Latin translation of the ancient Hindu texts, The Upanishads, which French writer Anquetil du Perron had translated from the Persian translation of Prince Dara Shikoh entitled \"Sirre-Akbar\" (\"The Great Secret\"). He was so impressed by their philosophy that he called them \"the production of the highest human wisdom\", and believed they contained superhuman concepts. The Upanishads was a great source of inspiration to Schopenhauer. Writing about them, he said:\n\nIt is the most satisfying and elevating reading (with the exception of the original text) which is possible in the world; it has been the solace of my life and will be the solace of my death.\n\nIt is well known that the book \"Oupnekhat\" (Upanishad) always lay open on his table, and he invariably studied it before sleeping at night. He called the opening up of Sanskrit literature \"the greatest gift of our century\", and predicted that the philosophy and knowledge of the Upanishads would become the cherished faith of the West.\n\nSchopenhauer was first introduced to the 1802 Latin Upanishad translation through Friedrich Majer. They met during the winter of 1813–1814 in Weimar at the home of Schopenhauer's mother according to the biographer Safranski. Majer was a follower of Herder, and an early Indologist. Schopenhauer did not begin a serious study of the Indic texts, however, until the summer of 1814. Sansfranski maintains that between 1815 and 1817, Schopenhauer had another important cross-pollination with Indian thought in Dresden. This was through his neighbor of two years, Karl Christian Friedrich Krause. Krause was then a minor and rather unorthodox philosopher who attempted to mix his own ideas with that of ancient Indian wisdom. Krause had also mastered Sanskrit, unlike Schopenhauer, and the two developed a professional relationship. It was from Krause that Schopenhauer learned meditation and received the closest thing to expert advice concerning Indian thought.\n\nMost noticeable, in the case of Schopenhauer’s work, was the significance of the Chandogya Upanishad, whose Mahavakya, Tat Tvam Asi is mentioned throughout \"The World as Will and Representation\".\n\nSchopenhauer noted a correspondence between his doctrines and the Four Noble Truths of Buddhism. Similarities centered on the principles that life involves suffering, that suffering is caused by desire (taṇhā), and that the extinction of desire leads to liberation. Thus three of the four \"truths of the Buddha\" correspond to Schopenhauer's doctrine of the will. In Buddhism, however, while greed and lust are always unskillful, desire is ethically variable – it can be skillful, unskillful, or neutral.\n\nFor Schopenhauer, Will had ontological primacy over the intellect; in other words, desire is understood to be prior to thought. Schopenhauer felt this was similar to notions of puruṣārtha or goals of life in Vedānta Hinduism.\n\nIn Schopenhauer's philosophy, denial of the will is attained by either:\n\nHowever, Buddhist nirvāṇa is not equivalent to the condition that Schopenhauer described as denial of the will. Nirvāṇa is not the extinguishing of the \"person\" as some Western scholars have thought, but only the \"extinguishing\" (the literal meaning of nirvana) of the flames of greed, hatred, and delusion that assail a person's character. Occult historian Joscelyn Godwin (1945– ) stated, \"It was Buddhism that inspired the philosophy of Arthur Schopenhauer, and, through him, attracted Richard Wagner. This Orientalism reflected the struggle of the German Romantics, in the words of Leon Poliakov, to \"free themselves from Judeo-Christian fetters\". In contradistinction to Godwin's claim that Buddhism inspired Schopenhauer, the philosopher himself made the following statement in his discussion of religions:\nIf I wished to take the results of my philosophy as the standard of truth, I should have to concede to Buddhism pre-eminence over the others. In any case, it must be a pleasure to me to see my doctrine in such close agreement with a religion that the majority of men on earth hold as their own, for this numbers far more followers than any other. And this agreement must be yet the more pleasing to me, inasmuch as \"in my philosophizing I have certainly not been under its influence\" [emphasis added]. For up till 1818, when my work appeared, there was to be found in Europe only a very few accounts of Buddhism.\nBuddhist philosopher Nishitani Keiji, however, sought to distance Buddhism from Schopenhauer. While Schopenhauer's philosophy may sound rather mystical in such a summary, his methodology was resolutely empirical, rather than speculative or transcendental:\nPhilosophy ... is a science, and as such has no articles of faith; accordingly, in it nothing can be assumed as existing except what is either positively given empirically, or demonstrated through indubitable conclusions.\nAlso note:\nThis actual world of what is knowable, in which we are and which is in us, remains both the material and the limit of our consideration.\nThe argument that Buddhism affected Schopenhauer's philosophy more than any other Dharmic faith loses more credence when viewed in light of the fact that Schopenhauer did not begin a serious study of Buddhism until after the publication of \"The World as Will and Representation\" in 1818. Scholars have started to revise earlier views about Schopenhauer's discovery of Buddhism. Proof of early interest and influence, however, appears in Schopenhauer's 1815/16 notes (transcribed and translated by Urs App) about Buddhism. They are included in a recent case study that traces Schopenhauer's interest in Buddhism and documents its influence. Other scholarly work questions how similar Schopenhauer's philosophy actually is to Buddhism.\n\nSchopenhauer said he was influenced by the Upanishads, Immanuel Kant and Plato. References to Eastern philosophy and religion appear frequently in his writing. As noted above, he appreciated the teachings of the Buddha and even called himself a Buddhist. He said that his philosophy could not have been conceived before these teachings were available.\n\nConcerning the Upanishads and Vedas, he writes in \"The World as Will and Representation\":\nIf the reader has also received the benefit of the Vedas, the access to which by means of the Upanishads is in my eyes the greatest privilege which this still young century (1818) may claim before all previous centuries, if then the reader, I say, has received his initiation in primeval Indian wisdom, and received it with an open heart, he will be prepared in the very best way for hearing what I have to tell him. It will not sound to him strange, as to many others, much less disagreeable; for I might, if it did not sound conceited, contend that every one of the detached statements which constitute the Upanishads, may be deduced as a necessary result from the fundamental thoughts which I have to enunciate, though those deductions themselves are by no means to be found there.\n\nAmong Schopenhauer's other influences were: Shakespeare, Jean-Jacques Rousseau, John Locke, Thomas Reid, Baruch Spinoza, Matthias Claudius, George Berkeley, David Hume, and René Descartes.\n\nSchopenhauer accepted Kant's double-aspect of the universe—the phenomenal (world of experience) and the noumenal (the true world, independent of experience). Some commentators suggest that Schopenhauer claimed that the noumenon, or thing-in-itself, was the basis for Schopenhauer's concept of the will. Other commentators suggest that Schopenhauer considered will to be only a subset of the \"thing-in-itself\" class, namely that which we can most directly experience.\n\nSchopenhauer's identification of the Kantian \"noumenon\" (i.e., the actually existing entity) with what he termed \"will\" deserves some explanation. The noumenon was what Kant called the \"Ding an sich\" (the Thing in Itself), the reality that is the foundation of our sensory and mental representations of an external world. In Kantian terms, those sensory and mental representations are mere phenomena. Schopenhauer departed from Kant in his description of the relationship between the phenomenon and the noumenon. According to Kant, things-in-themselves ground the phenomenal representations in our minds; Schopenhauer, on the other hand, believed that phenomena and noumena are two different sides of the same coin. Noumena do not \"cause\" phenomena, but rather phenomena are simply the way by which our minds perceive the noumena, according to the principle of sufficient reason. This is explained more fully in Schopenhauer's doctoral thesis, \"On the Fourfold Root of the Principle of Sufficient Reason\" (1813).\n\nSchopenhauer's second major departure from Kant's epistemology concerns the body. Kant's philosophy was formulated as a response to the radical philosophical skepticism of David Hume, who claimed that causality could not be observed empirically. Schopenhauer begins by arguing that Kant's demarcation between external objects, knowable only as phenomena, and the Thing in Itself of noumenon, contains a significant omission. There is, in fact, one physical object we know more intimately than we know any object of sense perception: our own body.\n\nWe know our human bodies have boundaries and occupy space, the same way other objects known only through our named senses do. Though we seldom think of our body as a physical object, we know even before reflection that it shares some of an object's properties. We understand that a watermelon cannot successfully occupy the same space as an oncoming truck; we know that if we tried to repeat the experiment with our own body, we would obtain similar results—we know this even if we do not understand the physics involved.\n\nWe know that our consciousness inhabits a physical body, similar to other physical objects only known as phenomena. Yet our consciousness is not commensurate with our body. Most of us possess the power of voluntary motion. We usually are not aware of the breathing of our lungs or the beating of our heart unless somehow our attention is called to them. Our ability to control either is limited. Our kidneys command our attention on their schedule rather than one we choose. Few of us have any idea what our liver is doing right now, though this organ is as needful as lungs, heart, or kidneys. The conscious mind is the servant, not the master, of these and other organs. These organs have an agenda the conscious mind did not choose, and over which it has limited power.\n\nWhen Schopenhauer identifies the \"noumenon\" with the desires, needs, and impulses in us that we name \"will\", what he is saying is that we participate in the reality of an otherwise unachievable world outside the mind through will. We cannot \"prove\" that our mental picture of an outside world corresponds with a reality by reasoning; through will, we know—without thinking—that the world can stimulate us. We suffer fear, or desire: these states arise involuntarily; they arise prior to reflection; they arise even when the conscious mind would prefer to hold them at bay. The rational mind is, for Schopenhauer, a leaf borne along in a stream of pre-reflective and largely unconscious emotion. That stream is will, and through will, if not through logic, we can participate in the underlying reality beyond mere phenomena. It is for this reason that Schopenhauer identifies the \"noumenon\" with what we call our will.\n\nIn his criticism of Kant, Schopenhauer claimed that sensation and understanding are separate and distinct abilities. Yet, for Kant, an object is known through each of them. Kant wrote: \"[T]here are two stems of human knowledge ... namely, sensibility and understanding, objects being given by the former [sensibility] and thought by the latter [understanding].\" Schopenhauer disagreed. He asserted that mere sense impressions, not objects, are given by sensibility. According to Schopenhauer, objects are intuitively perceived by understanding and are discursively thought by reason (Kant had claimed that (1) the understanding thinks objects through concepts and that (2) reason seeks the unconditioned or ultimate answer to \"why?\"). Schopenhauer said that Kant's mistake regarding perception resulted in all of the obscurity and difficult confusion that is exhibited in the Transcendental Analytic section of his critique.\n\nLastly, Schopenhauer departed from Kant in how he interpreted the Platonic ideas. In \"The World as Will and Representation\" Schopenhauer explicitly stated:\n\n...Kant used the word [Idea] wrongly as well as illegitimately, although Plato had already taken possession of it, and used it most appropriately.\n\nInstead Schopenhauer relied upon the Neoplatonist interpretation of the biographer Diogenes Laërtius from \"Lives and Opinions of Eminent Philosophers\". In reference to Plato's Ideas, Schopenhauer quotes Laërtius verbatim in an explanatory footnote.\n\nDiogenes Laërtius (III, 12) Plato ideas in natura velut exemplaria dixit subsistere; cetera his esse similia, ad istarum similitudinem consistencia.\n\nSchopenhauer expressed his dislike for the philosophy of his contemporary Georg Wilhelm Friedrich Hegel many times in his published works. The following quotations are typical:\n\nIn his Foreword to the first edition of his work \"Die beiden Grundprobleme der Ethik\", Schopenhauer suggested that he had shown Hegel to have fallen prey to the \"Post hoc ergo propter hoc\" fallacy.\n\nSchopenhauer suggested that Hegel's works were filled with \"castles of abstraction\", and that Hegel used deliberately impressive but ultimately vacuous verbiage. He also thought that his glorification of church and state were designed for personal advantage and had little to do with the search for philosophical truth. For instance, the Right Hegelians interpreted Hegel as viewing the Prussian state of his day as perfect and the goal of all history up until then.\n\nThe British philosopher and historian Bertrand Russell deemed Schopenhauer an insincere person, because judging by his life:\n\nBryan Magee argues that \"the answer to such shallow, but not uncommon criticism\" is found in a quotation from Schopenhauer:\n\nSchopenhauer has had a massive influence upon later thinkers, though more so in the arts (especially literature and music) and psychology than in philosophy. His popularity peaked in the early twentieth century, especially during the Modernist era, and waned somewhat thereafter. Nevertheless, a number of recent publications have reinterpreted and modernised the study of Schopenhauer. His theory is also being explored by some modern philosophers as a precursor to evolutionary theory and modern evolutionary psychology.\n\nRussian writer and philosopher Leo Tolstoy was greatly influenced by Schopenhauer. After reading Schopenhauer's \"The World as Will and Representation\", Tolstoy gradually became converted to the ascetic morality upheld in that work as the proper spiritual path for the upper classes: \"Do you know what this summer has meant for me? Constant raptures over Schopenhauer and a whole series of spiritual delights which I've never experienced before. ... no student has ever studied so much on his course, and learned so much, as I have this summer\"\n\nRichard Wagner, writing in his autobiography, remembered his first impression that Schopenhauer left on him (when he read \"World as Will and Representation\"):\n\nWagner also commented on that \"serious mood, which was trying to find ecstatic expression\" created by Schopenhauer inspired the conception of Tristan und Isolde. See also Influence of Schopenhauer on Tristan und Isolde.\n\nFriedrich Nietzsche owed the awakening of his philosophical interest to reading \"The World as Will and Representation\" and admitted that he was one of the few philosophers that he respected, dedicating to him his essay \"Schopenhauer als Erzieher\" one of his \"Untimely Meditations\".\n\nJorge Luis Borges remarked that the reason he had never attempted to write a systematic account of his world view, despite his penchant for philosophy and metaphysics in particular, was because Schopenhauer had already written it for him.\n\nAs a teenager, Ludwig Wittgenstein adopted Schopenhauer's epistemological idealism. However, after his study of the philosophy of mathematics, he rejected epistemological idealism for Gottlob Frege's conceptual realism. In later years, Wittgenstein was highly dismissive of Schopenhauer, describing him as an ultimately shallow thinker: \"Schopenhauer has quite a crude mind... where real depth starts, his comes to an end.\"\n\nThe philosopher Gilbert Ryle read Schopenhauer's works as a student, but later largely forgot them, only to unwittingly recycle ideas from Schopenhauer in his \"The Concept of Mind\" (1949).\n\n\n\n\n\n\n\n"}
{"id": "701", "url": "https://en.wikipedia.org/wiki?curid=701", "title": "Angola", "text": "Angola\n\nAngola , officially the Republic of Angola ( ; Kikongo, Kimbundu and Umbundu: \"Repubilika ya Ngola\"), is a country in Southern Africa. It is the seventh-largest country in Africa and is bordered by Namibia to the south, the Democratic Republic of the Congo to the north, Zambia to the east, and the Atlantic Ocean to west. The exclave province of Cabinda has borders with the Republic of the Congo and the Democratic Republic of the Congo. The capital and largest city of Angola is Luanda.\n\nAlthough its territory has been inhabited since the Paleolithic Era, what is now the modern country of Angola was influenced by Portuguese colonisation, which began with, and was for centuries limited to, coastal settlements and trading posts established beginning in the 16th century. In the 19th century, European settlers slowly and hesitantly began to establish themselves in the interior. As a Portuguese colony, Angola did not encompass its present borders until the early 20th century, following resistance by groups such as the Cuamato, the Kwanyama and the Mbunda. Independence was achieved in 1975 under a Marxist-Leninist one party state, backed by the Soviet Union and Cuba after a protracted anti-colonial struggle. However, the country soon descended into an even lengthier civil war that lasted until 2002. It has since become a relatively stable unitary presidential republic.\n\nAngola has vast mineral and petroleum reserves, and its economy is among the fastest growing in the world, especially since the cessation of the civil war. In spite of this, the standard of living remains low for the majority of the population, and life expectancy in Angola is among the lowest in the world, while infant mortality rates are among the highest. Angola's economic growth is highly uneven, with the majority of the nation's wealth concentrated in a disproportionately small sector of the population.\n\nAngola is a member state of the United Nations, OPEC, African Union, the Community of Portuguese Language Countries, and the Southern African Development Community. A highly multiethnic country, Angola's 25.8 million people span various tribal groups, customs, and traditions. Angolan culture reflects centuries of Portuguese rule, namely in the predominance of the Portuguese language and the Catholic Church, combined with diverse indigenous influences.\n\nThe name \"Angola\" comes from the Portuguese colonial name \"Reino de Angola (Kingdom of Angola)\", appearing as early as Dias de Novais's 1571 charter. The toponym was derived by the Portuguese from the title \"ngola\" held by the kings of Ndongo. Ndongo was a kingdom in the highlands, between the Kwanza and Lukala Rivers, nominally tributary to the king of Kongo but which was seeking greater independence during the 16th century.\n\nModern Angola was populated predominantly by nomadic Khoi and San prior to the first Bantu migrations. The Khoi and San peoples were neither pastoralists nor cultivators, following a hunter-gatherer lifestyle. They were displaced by Bantu peoples arriving from the north, most of whom likely originated in what is today northwestern Nigeria and southern Niger. Bantu speakers introduced the cultivation of bananas and taro, as well as large cattle herds, to Angola's central highlands and the Luanda plain.\n\nDuring this time, the Bantu established a number of political entities in most of what today comprises Angola. The best known of these was the Kingdom of the Kongo, which was based in Angola, but also extended northward to what is now the Democratic Republic of the Congo and the Republic of the Congo, as well as Gabon. It established trade routes with other city-states and civilisations up and down the coast of southwestern and western Africa and even with Great Zimbabwe and the Mutapa Empire but engaged in little or no transoceanic trade. To its south lay the Kingdom of Ndongo, from which the area of the later Portuguese colony was sometimes known as \"Dongo\".\n\nThe region, now known as Angola, was reached by the Portuguese explorer Diogo Cão in 1484. The year before, the Portuguese had established relations with the Kongo, which stretched at the time from modern Gabon in the north to the Kwanza River in the south. The Portuguese established their primary early trading post at Soyo, which is now the northernmost city in Angola apart from the Cabinda exclave. Paulo Dias de Novais founded São Paulo de Loanda (Luanda) in 1575 with a hundred families of settlers and four hundred soldiers. Benguela was fortified in 1587 and elevated to a township in 1617.\n\nThe Portuguese established several other settlements, forts and trading posts along the Angolan coast, principally trading in Angolan slaves for Brazilian plantations. Local slave dealers provided a large number of slaves for the Portuguese Empire, usually sold in exchange for manufactured goods from Europe. This part of the Atlantic slave trade continued until after Brazil's independence in the 1820s.\n\nDespite Portugal's territorial claims to Angola, its control over much of the country's vast interior was minimal. In the 16th century Portugal gained control of the coast through a series of treaties and wars. Life for European colonists was difficult and progress proved to be slow. Iliffe notes that \"Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys\".\n\nDuring the Portuguese Restoration War, the Dutch West India Company occupied the principal settlement of Luanda in 1641, using alliances with local peoples to carry out attacks against Portuguese holdings elsewhere. A fleet under Salvador de Sá retook Luanda in 1648; reconquest of the rest of the territory was completed by 1650. New treaties with the Kongo were signed in 1649; others with Njinga's Kingdom of Matamba and Ndongo followed in 1656. The conquest of Pungo Andongo in 1671 was the last major Portuguese expansion from Luanda, as attempts to invade Kongo in 1670 and Matamba in 1681 failed. Colonial outposts also expanded inward from Benguela, but until the late 19th century the inroads from Luanda and Benguela were very limited. Hamstrung by a series of political upheavals in the early 1800s, Portugal was slow to mount a large scale annexation of Angolan territory.\n\nThe slave trade was abolished in Angola in 1836, and in 1854 the colonial government freed all its existing slaves. Four years later, a more progressive administration appointed by Lisbon abolished slavery altogether. However, these decrees remained largely unenforceable, and the Portuguese were dependent on assistance from the Royal Navy to enforce their ban on the slave trade. This coincided with a series of renewed military expeditions on the hinterland, and by the mid-nineteenth century Portugal had established its dominion as far east as the Congo River and as far south as Mossâmedes. Until the late 1880s, Lisbon entertained proposals to link Angola with its colony in Mozambique, but was blocked by British and Belgian opposition. During this whole period, the Portuguese came up against different forms of armed resistance from various peoples in Angola.\n\nThe Berlin Conference in 1884-1885 fixed the colony's borders, delineating the boundaries of Portuguese claims to Angola., although many details were resolved afterwards, until the 1920s. Trade between Portugal and her African territories also rapidly increased as a result of protective tariffs, leading to increased development, as well as a wave of new Portuguese migrants.\n\nUnder colonial law, black Angolans were forbidden from forming political parties or labour unions. The first nationalist movements did not take root until after World War II, spearheaded by a largely Westernised, Portuguese-speaking urban class which included many mestiços. During the early 1960s they were joined by other associations stemming from \"ad hoc\" labour activism in the rural workforce. Portugal's refusal to address increasing Angolan demands for self-determination provoked an armed conflict which erupted in 1961 with the Baixa de Cassanje revolt and gradually evolved into a protracted war of independence that persisted for the next twelve years. Throughout the conflict, three militant nationalist movements with their own partisan guerrilla wings emerged from the fighting between the Portuguese government and local forces, supported to varying degrees by the Portuguese Communist Party.\n\nThe \"National Front for the Liberation of Angola\" (FNLA) recruited from Bakongo refugees in Zaire. Benefiting from particularly favourable political circumstances in Léopoldville, and especially from a common border with Zaire, Angolan political exiles were able to build up a power base among a large expatriate community from related families, clans, and tradition. People on both sides of the border spoke mutually intelligible dialects and enjoyed shared ties to the historical Kingdom of Kongo. Though as foreigners skilled Angolans could not take advantage of Mobutu Sese Seko's state employment programme, some found work as middlemen for the absentee owners of various lucrative private ventures. The migrants eventually formed the FNLA with the intention of making a bid for political power upon their envisaged return to Angola.\n\nA largely Ovimbundu guerrilla initiative against the Portuguese in central Angola from 1966 was spearheaded by Jonas Savimbi and the \"National Union for the Total Independence of Angola\" (UNITA). It remained handicapped by its geographic remoteness from friendly borders, by the ethnic fragmentation of the Ovimbundu, and the isolation of peasants on European plantations where they had little opportunity to mobilise.\n\nAgainst the background of these simultaneous efforts, the rising of the Marxist \"Popular Movement for the Liberation of Angola\" (MPLA) in the east and Dembos hills north of Luanda came to hold special significance. Formed as a coalition resistance by the Angolan Communist Party, the organisation's leadership remained predominantly Ambundu and courted public sector workers in Luanda. Although both the MPLA and its rivals had accepted material assistance from the Soviet Union or the People's Republic of China, the former harboured strong anti-imperialist views and was openly critical of the United States and its support for Portugal. This allowed it to win important ground on the diplomatic front, soliciting support from nonaligned governments in Morocco, Ghana, Guinea, Mali, and the United Arab Republic.\n\nThe MPLA attempted to move its headquarters from Conakry to Léopoldville in October 1961, renewing efforts to create a common front with the FNLA, then known as the \"Union of Angolan Peoples\" (UPA) and its leader Holden Roberto. Roberto turned down the offer. When the MPLA first attempted to insert its own insurgents into Angola, the cadres were ambushed and annihilated by UPA partisans on Roberto's orders—setting a precedent for the bitter factional strife which would later ignite the Angolan Civil War.\n\nThroughout the war of independence, the three rival nationalist movements were severely hampered by political and military factionalism, as well as their inability to unite guerrilla efforts against the Portuguese. Between 1961 and 1975 the MPLA, UNITA, and the FNLA competed for influence in the Angolan population and the international community. The Soviet Union and Cuba became especially sympathetic towards the MPLA and supplied that party with arms, ammunition, funding, and training. They also backed UNITA militants until it became clear that the latter was at irreconcilable odds with the MPLA.\n\nThe collapse of Portugal's Estado Novo government following the 1974 Carnation Revolution led to a suspension of all Portuguese military activities in Africa and the brokering of a ceasefire pending negotiations for Angolan independence. Encouraged by the Organisation of African Unity, Holden Roberto, Jonas Savimbi, and MPLA chairman Agostinho Neto met in Mombasa in early January 1975 and agreed to form a coalition government. This was ratified by the Alvor Agreement later that month, which called for general elections and set the country's independence date for 11 November 1975. All three factions, however, followed up on the ceasefire by taking advantage of the gradual Portuguese withdrawal to seize various strategic positions, acquire more arms, and enlarge their militant forces. The rapid influx of weapons from numerous external sources, especially the Soviet Union and the United States, as well as the escalation of tensions between the nationalist parties, fueled a new outbreak of hostilities. With tacit American and Zairean support the FNLA began massing large numbers of troops in northern Angola in an attempt to gain military superiority. Meanwhile, the MPLA began securing control of Luanda, a traditional Ambundu stronghold. Sporadic violence broke out in Luanda over the next few months after the FNLA attacked MPLA forces in March 1975. The fighting intensified with street clashes in April and May, and UNITA became involved after over two hundred of its members were massacred by an MPLA contingent that June. An upswing in Soviet arms shipments to the MPLA influenced a decision by the Central Intelligence Agency to likewise provide substantial covert aid to the FNLA and UNITA.\n\nIn August 1975, the MPLA requested direct assistance from the Soviet Union in the form of ground troops. The Soviets declined, offering to send advisers but no troops; however, Cuba was more forthcoming and in late September dispatched nearly five hundred combat personnel to Angola, along with sophisticated weaponry and supplies. By independence there were over a thousand Cuban soldiers in the country. They were kept supplied by a massive airbridge carried out with Soviet aircraft. The persistent buildup of Cuban and Soviet military aid allowed the MPLA to drive its opponents from Luanda and blunt an abortive intervention by Zairean and South African troops, which had deployed in a belated attempt to assist the FNLA and UNITA. The FNLA was largely annihilated, although UNITA managed to withdraw its civil officials and militia from Luanda and seek sanctuary in the southern provinces. From there, Savimbi continued to mount a determined insurgent campaign against the MPLA.\n\nBetween 1975 and 1991, the MPLA implemented an economic and political system based on the principles of scientific socialism, incorporating central planning and a Marxist-Leninist one-party state. It embarked on an ambitious programme of nationalisation and made a concerted attempt to eliminate the preexisting private sector. All locally owned enterprises in every sector of economic activity were subject to nationalisation by the state. These were then incorporated into a single umbrella of state-owned enterprises known as \"Unidades Economicas Estatais\" (UEE). Under the MPLA, Angola experienced a significant degree of modern industrialisation. However, corruption and graft also increased as public resources were either allocated inefficiently or simply embezzled by officials for personal enrichment. The ruling party survived an attempted coup d'état by the Maoist-oriented Communist Organisation of Angola (OCA) in 1977, which was suppressed after a series of bloody political purges that left thousands of OCA supporters dead.\n\nThe MPLA abandoned its former Marxist ideology at its third party congress in 1990, and declared social democracy to be its new platform. Angola subsequently became a member of the International Monetary Fund; restrictions on the market economy were also reduced in an attempt to draw foreign investment. By May 1991 it had reached a peace agreement with UNITA, the Bicesse Accords, which scheduled new general elections for September 1992. When the MPLA secured a major electoral victory, UNITA objected to the results of both the presidential and legislative vote count and returned to war.\n\nOn 22 March 2002, Jonas Savimbi was killed in action against government troops. UNITA and the MPLA reached a cease-fire shortly afterwards. UNITA gave up its armed wing and assumed the role of a major opposition party. Although the political situation of the country began to stabilise, regular democratic processes did not prevail until the elections in Angola in 2008 and 2012 and the adoption of a new constitution in 2010, all of which strengthened the prevailing dominant-party system.\n\nAngola has a serious humanitarian crisis; the result of the prolonged war, of the abundance of minefields, of the continued political (and to a much lesser degree) military activities in favour of the independence of the exclave of Cabinda (carried out in the context of the protracted Cabinda Conflict by the Frente para a Libertação do Enclave de Cabinda, (FLEC)), but most of all, by the depredation of the country's rich mineral resources by the régime. While most of the internally displaced have now settled around the capital, in the so-called \"musseques\", the general situation for Angolans remains desperate.\n\nDrought in 2016 caused the worst food crisis in Southern Africa in 25 years. Drought affected 1.4 million people across seven of Angola's 18 provinces. Food prices rose and acute malnutrition rates doubled, with more than 95,000 children affected. Food insecurity was expected to worsen from July to December 2016.\n\nAt , Angola is the world's twenty-third largest country. It is comparable in size to Mali, or twice the size of France or Texas. It lies mostly between latitudes 4° and 18°S, and longitudes 12° and 24°E.\n\nAngola is bordered by Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east and the South Atlantic Ocean to the west. The coastal exclave of Cabinda in the north, borders the Republic of the Congo to the north, and the Democratic Republic of the Congo to the south. Angola's capital, Luanda, lies on the Atlantic coast in the northwest of the country.\n\nAngola, although located in a tropical zone, has a climate that is not characterized for this region, due to the confluence of three factors:\n\n\nAs a result, Angola's climate is characterized by two seasons: rainfall from October to April and drought, known as \"Cacimbo\", from May to August, drier, as the name implies, and with lower temperatures. On the other hand, while the coastline has high rainfall rates, decreasing from North to South and from 800 mm to 50 mm, with average annual temperatures above 23 ° C, the interior zone can be divided into three areas:\n\n\n \nThe Angolan government is composed of three branches of government: executive, legislative and judicial. The executive branch of the government is composed of the President, the Vice-Presidents and the Council of Ministers. The legislative branch comprises a 220-seat unicameral legislature elected from both provincial and nationwide constituencies. For decades, political power has been concentrated in the presidency.\n\nThe Constitution of 2010 establishes the broad outlines of government structure and delineates the rights and duties of citizens. The legal system is based on Portuguese law and customary law but is weak and fragmented, and courts operate in only 12 of more than 140 municipalities. A Supreme Court serves as the appellate tribunal; a Constitutional Court does not hold the powers of judicial review. Governors of the 18 provinces are appointed by the president.\n\nAfter the end of the civil war the regime came under pressure from within as well as from the international community to become more democratic and less authoritarian. Its reaction was to implement a number of changes without substantially changing its character.\n\nAngola is classified as 'not free' by Freedom House in the Freedom in the World 2014 report. The report noted that the August 2012 parliamentary elections, in which the ruling Popular Movement for the Liberation of Angola won more than 70% of the vote, suffered from serious flaws, including outdated and inaccurate voter rolls. Voter turnout dropped from 80% in 2008 to 60%.\n\nAngola scored poorly on the 2013 Ibrahim Index of African Governance. It was ranked 39 out of 52 sub-Saharan African countries, scoring particularly badly in the areas of participation and human rights, sustainable economic opportunity and human development. The Ibrahim Index uses a number of variables to compile its list which reflects the state of governance in Africa.\nThe new constitution, adopted in 2010, did away with presidential elections, introducing a system in which the president and the vice-president of the political party that wins the parliamentary elections automatically become president and vice-president. Directly or indirectly, the president controls all other organs of the state, so there is de facto no separation of powers. In terms of the classifications used in constitutional law, this form of government fall under the category of \"authoritarian regime\".\n\nOn 16 October 2014, Angola was elected for the second time as a non-permanent member of the UN Security Council, with 190 favourable votes out of 193. The mandate begins on 1 January 2015 and lasts for two years.\n\nAlso in that month, the country took on the leadership of the Group of African Ministers and Governors at the International Monetary Fund and the World Bank, following the debates at the annual meetings of both entities.\n\nSince January 2014 the Republic of Angola has held the presidency of the International Conference on the Great Lakes Region (ICGLR). In 2015, the executive secretary of ICGLR, Ntumba Luaba, said that Angola is the example to be followed by members of the organisation, because of the significant progress made over the 12 years of peace, particularly in terms of socioeconomic and political-military stability.\n\nAfter 38 years of rule, President dos Santos will be stepping down from the MPLA leadership. The leader of the winning party at the Parliamentary Elections in August 2017 will become the next president of Angola. The MPLA has selected Defense Minister, General João Lourenço, for their front-runner to take over from President dos Santos if the ruling party wins the August elections.\n\nThe Angolan Armed Forces (AAF) is headed by a Chief of Staff who reports to the Minister of Defence. There are three divisions—the Army (Exército), Navy (Marinha de Guerra, MGA) and National Air Force (Força Aérea Nacional, FAN). Total manpower is about 110,000. Its equipment includes Russian-manufactured fighters, bombers and transport planes. There are also Brazilian-made EMB-312 Tucano for training role, Czech-made L-39 for training and bombing role, Czech Zlin for training role and a variety of western made aircraft such as C-212\\Aviocar, Sud Aviation Alouette III, etc. A small number of AAF personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville).\n\nThe National Police departments are Public Order, Criminal Investigation, Traffic and Transport, Investigation and Inspection of Economic Activities, Taxation and Frontier Supervision, Riot Police and the Rapid Intervention Police. The National Police are in the process of standing up an air wing, which will provide helicopter support for operations. The National Police are developing their criminal investigation and forensic capabilities. The force has an estimated 6,000 patrol officers, 2,500 taxation and frontier supervision officers, 182 criminal investigators and 100 financial crimes detectives and around 90 economic activity inspectors.\n\nThe National Police have implemented a modernisation and development plan to increase the capabilities and efficiency of the total force. In addition to administrative reorganisation, modernisation projects include procurement of new vehicles, aircraft and equipment, construction of new police stations and forensic laboratories, restructured training programmes and the replacement of AKM rifles with 9 mm Uzis for officers in urban areas.\n\nA Supreme Court serves as a court of appeal. The Constitutional Court is the supreme body of the constitutional jurisdiction, its Organic Law was approved by Law no. 2/08, of June 17, and its first task was the validation of the candidacies of the political parties to the legislative elections of 5 September 2008. The legal system is based on Portuguese and customary laws, but it is weak and fragmented. There are only 12 courts in more than 140 counties in the country. A Supreme Court serves as a court of appeal. With the approval of Law no. 2/08, of June 17 - Organic Law of the Constitutional Court and Law n. 3/08, of June 17 - Organic Law of the Constitutional Process, the Legal Creation of the Constitutional Court. Thus, on June 25, 2008, the Constitutional Court was institutionalized and its Judicial Counselors assumed the position before the President of the Republic. On this date, seven advisory judges took office, four men and three women.\n\nIn 2014, a new penal code took effect in Angola. The classification of money-laundering as a crime is one of the novelties in the new legislation.\n\nOn 16 October 2014, Angola was elected for the second time a non-permanent member of the United Nations Security Council, with 190 favorable votes out of a total of 193. The term of office begins on 1 January 2015 and lasts for two Years. \n\nSince January 2014, the Republic of Angola has been chairing the International Conference for the Great Lakes Region (CIRGL). [80] In 2015, CIRGL Executive Secretary Ntumba Luaba said that Angola is the example to be followed by the members of the organization, due to the significant progress made during the 12 years of peace, namely in terms of socio-economic stability and political- military.\n\nHomosexual acts are currently illegal in Angola. However, in February 2017, the Angolan Parliament approved a new penal code which does not outlaw homosexual acts. The law will come effect in late 2017. In 2010, the Angolan Government refused to receive openly gay Isi Yanouka as the new Israeli ambassador, allegedly due to his sexual orientation.\n\n, Angola is divided into eighteen provinces (\"províncias\") and 162 municipalities. The municipalities are further divided into 559 communes (townships). The provinces are:\n\nWith an area of approximately , the Northern Angolan province of Cabinda is unusual in being separated from the rest of the country by a strip, some wide, of the Democratic Republic of Congo along the lower Congo River. Cabinda borders the Congo Republic to the north and north-northeast and the DRC to the east and south. The town of Cabinda is the chief population centre.\n\nAccording to a 1995 census, Cabinda had an estimated population of 600,000, approximately 400,000 of whom live in neighbouring countries. Population estimates are, however, highly unreliable. Consisting largely of tropical forest, Cabinda produces hardwoods, coffee, cocoa, crude rubber and palm oil. The product for which it is best known, however, is its oil, which has given it the nickname, \"the Kuwait of Africa\". Cabinda's petroleum production from its considerable offshore reserves now accounts for more than half of Angola's output. Most of the oil along its coast was discovered under Portuguese rule by the Cabinda Gulf Oil Company (CABGOC) from 1968 onwards.\n\nEver since Portugal handed over sovereignty of its former overseas province of Angola to the local independence groups (MPLA, UNITA and FNLA), the territory of Cabinda has been a focus of separatist guerrilla actions opposing the Government of Angola (which has employed its armed forces, the FAA—Forças Armadas Angolanas) and Cabindan separatists. The Front for the Liberation of the Enclave of Cabinda-Armed Forces of Cabinda (FLEC-FAC) announced a virtual Federal Republic of Cabinda under the Presidency of N'Zita Henriques Tiago. One of the characteristics of the Cabindan independence movement is its constant fragmentation, into smaller and smaller factions.\n\nAngola has a rich subsoil heritage, from diamonds, oil, gold, copper and a rich wildlife (dramatically impoverished during the civil war), forest and fossils. Since independence, oil and diamonds have been the most important economic resource. Smallholder and plantation agriculture have dramatically dropped because of the Angolan Civil War, but have begun to recover after 2002. The transformation industry that had come into existence in the late colonial period collapsed at independence, because of the exodus of most of the ethnic Portuguese population, but has begun to reemerge with updated technologies, partly because of the influx of new Portuguese entrepreneurs. Similar developments can be verified in the service sector.\n\nOverall, Angola's economy has in recent years moved on from the disarray caused by a quarter-century of civil war to become the fastest-growing economy in Africa and one of the fastest in the world, with an average GDP growth of 20 percent between 2005 and 2007. In the period 2001–10, Angola had the world's highest annual average GDP growth, at 11.1 percent. In 2004, the Exim Bank of China approved a $2 billion line of credit to Angola. The loan was to be used to rebuild Angola's infrastructure, and also to limited the influence of the International Monetary Fund in the country. China is Angola's biggest trade partner and export destination as well as the fourth-largest importer. Bilateral trade reached $27.67 billion in 2011, up 11.5% year-on-year. China's imports, mainly crude oil and diamonds, increased 9.1% to $24.89 billion while China's exports, including mechanical and electrical products, machinery parts and construction materials, surged 38.8%. The oil glut led to a local unleaded gasoline \"pricetag\" of £0.37 per gallon.\n\n\"The Economist\" reported in 2008 that diamonds and oil make up 60% of Angola's economy, almost all of the country's revenue and are its dominant exports. Growth is almost entirely driven by rising oil production which surpassed in late 2005 and was expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate owned by the Angolan government. In December 2006, Angola was admitted as a member of OPEC. However, operations in diamond mines include partnerships between state-run Endiama and mining companies such as ALROSA which continue operations in Angola. The economy grew 18% in 2005, 26% in 2006 and 17.6% in 2007. However, due to the global recession the economy contracted an estimated −0.3% in 2009. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.\n\nAlthough the country's economy has developed significantly since it achieved political stability in 2002, mainly thanks to the fast-rising earnings of the oil sector, Angola faces huge social and economic problems. These are in part a result of the almost continual state of conflict from 1961 onwards, although the highest level of destruction and socio-economic damage took place after the 1975 independence, during the long years of civil war. However, high poverty rates and blatant social inequality are chiefly the outcome of a combination of a persistent political authoritarianism, of \"neo-patrimonial\" practices at all levels of the political, administrative, armed forces and economic apparatuses, and of a pervasive corruption. The main beneficiary of this situation is a social segment constituted during the last decades, around the political, administrative, economic and military power holders, which has accumulated (and continues accumulating) enormous wealth. \"Secondary beneficiaries\" are the middle strata which are about to become social classes. However, overall almost half the population has to be considered as poor, but in this respect there are dramatic differences between the countryside and the cities (where by now slightly more than 50% of the people live).\n\nAn inquiry carried out in 2008 by the Angolan Instituto Nacional de Estatística has it that in the rural areas roughly 58% must be classified as \"poor\", according to UN norms, but in the urban areas only 19%, while the overall rate is 37%. In the cities, a majority of families, well beyond those officially classified as poor, have to adopt a variety of survival strategies. At the same time, in urban areas social inequality is most evident and assumes extreme forms in the capital, Luanda. In the Human Development Index Angola constantly ranks in the bottom group.\n\nAccording to the Heritage Foundation, a conservative American think tank, oil production from Angola has increased so significantly that Angola now is China's biggest supplier of oil. “China has extended three multibillion dollar lines of credit to the Angolan government; two loans of $2 billion from China Exim Bank, one in 2004, the second in 2007, as well as one loan in 2005 of $2.9 billion from China International Fund Ltd.” Growing oil revenues have also created opportunities for corruption: according to a recent Human Rights Watch report, 32 billion US dollars disappeared from government accounts from 2007 to 2010. Furthermore, Sonangol, the state run oil company, has control of 51% of Cabinda’s oil. Due to this market control the company ends up determining the profit given to the government and the taxes paid. The council of foreign affairs states that the World Bank mentioned that Sonangol \" is a taxpayer, it carries out quasi-fiscal activities, it invests public funds, and, as concessionaire, it is a sector regulator. This multifarious work programme creates conflicts of interest and characterises a complex relationship between Sonangol and the government that weakens the formal budgetary process and creates uncertainty as regards the actual fiscal stance of the state.\"\n\nBefore independence in 1975, Angola was a breadbasket of southern Africa and a major exporter of bananas, coffee and sisal, but three decades of civil war (1975–2002) destroyed fertile countryside, left it littered with landmines and drove millions into the cities. The country now depends on expensive food imports, mainly from South Africa and Portugal, while more than 90% of farming is done at the family and subsistence level. Thousands of Angolan small-scale farmers are trapped in poverty.\n\nThe enormous differences between the regions pose a serious structural problem for the Angolan economy, illustrated by the fact that about one third of economic activities are concentrated in Luanda and neighbouring Bengo province, while several areas of the interior suffer economic stagnation and even regression.\n\nOne of the economic consequences of the social and regional disparities is a sharp increase in Angolan private investments abroad. The small fringe of Angolan society where most of the accumulation takes place seeks to spread its assets, for reasons of security and profit. For the time being, the biggest share of these investments is concentrated in Portugal where the Angolan presence (including that of the family of the state president) in banks as well as in the domains of energy, telecommunications, and mass media has become notable, as has the acquisition of vineyards and orchards as well as of touristic enterprises.\n\nSub-Saharan Africa nations are globally achieving impressive improvements in well-being, according to a report by Tony Blair Africa Governance Initiative and the Boston Consulting Group. Angola has upgraded critical infrastructure, an investment made possible by funds from the nation's development of oil resources. According to this report, just slightly more than ten years after the end of the civil war Angola's standard of living has overall greatly improved. Life expectancy, which was just 46 years in 2002, reached 51 in 2011. Mortality rates for children fell from 25 percent in 2001 to 19 percent in 2010 and the number of students enrolled in primary school has tripled since 2001. However, at the same time the social and economic inequality that has characterised the country since long has not diminished, but on the contrary deepened in all respects.\n\nWith a stock of assets corresponding to 70 billion Kz (6.8 billion USD), Angola is now the third largest financial market in sub-Saharan Africa, surpassed only by Nigeria and South Africa. According to the Angolan Minister of Economy, Abraão Gourgel, the financial market of the country grew modestly from 2002 and now lies in third place at the level of sub-Saharan Africa.\n\nAngola's economy is expected to grow by 3.9 percent in 2014 said the International Monetary Fund (IMF). According to the Fund, robust growth in the non-oil economy, mainly driven by a very good performance in the agricultural sector, is expected to offset a temporary drop in oil production.\n\nAngola's financial system is maintained by the National Bank of Angola and managed by governor . According to a study on the banking sector, carried out by Deloitte, the monetary policy led by Banco Nacional de Angola (BNA), the Angolan national bank, allowed a decrease in the inflation rate put at 7.96% in December 2013, which contributed to the sector's growth trend. According to estimates released by Angola's central bank, the country's economy should grow at an annual average rate of 5 percent over the next four years, boosted by the increasing participation of the private sector.\n\nOn 19 December 2014, the Capital Market in Angola started. BODIVA (Angola Securities and Debt Stock Exchange, in English) received the secondary public debt market, and it is expected to start the corporate debt market by 2015, but the stock market should be a reality only in 2016.\n\nAgriculture and forestry is an area of opportunity for the country. “Angola requires 4.5 million tonnes a year of grain but grows only about 55% of the corn it needs, 20% of the rice and just 5% of its required wheat”(African economic Outlook) but “less than 3 percent of Angola's abundant fertile land is cultivated and the economic potential of the forestry sector remains largely unexploited” (World Bank). From this fact we can appreciate the capacity that Angola has to increase production for not only for the national market but also the international one. Investing in this sector can help reduce unemployment and more specifically in the rural areas. This will undoubtedly have consequences on the living standard of rural civilians.\n\nTransport in Angola consists of:\n\nTravel on highways outside of towns and cities in Angola (and in some cases within) is often not best advised for those without four-by-four vehicles. While a reasonable road infrastructure has existed within Angola, time and the war have taken their toll on the road surfaces, leaving many severely potholed, littered with broken asphalt. In many areas drivers have established alternate tracks to avoid the worst parts of the surface, although careful attention must be paid to the presence or absence of landmine warning markers by the side of the road. The Angolan government has contracted the restoration of many of the country's roads. The road between Lubango and Namibe, for example, was completed recently with funding from the European Union, and is comparable to many European main routes. Completing the road infrastructure is likely to take some decades, but substantial efforts are already being made.\n\nTransport is an important aspect in Angola because it is strategically located and it could become a regional logistics hub. In addition Angola has some of the most important and biggest ports and so it is vital to connect them to the interior of the country as well as to neighbouring countries.\n\nTourism is creaking to its feet on the heels of the long ended stop in the civil war, and very few tourists venture anywhere in Angola yet due to lack of infrastructure.\n\nThe telecommunications industry is considered one of the main strategic sectors in Angola.\n\nIn October 2014, the building of an optic fiber underwater cable was announced. This project aims to turn Angola into a continental hub, thus improving Internet connections both nationally and internationally.\n\nOn 11 March 2015, the First Angolan Forum of Telecommunications and Information Technology was held in Luanda under the motto \"The challenges of telecommunications in the current context of Angola\". The purpose of this forum was to promote the debate on topical issues on telecommunications in Angola and worldwide. A study about this sector was also presented at this forum, and some of its conclusions were: Angola had the first telecommunications operator in Africa to test the High Speed Internet technology (LTE-Advanced with speeds up to 400Mbit/s); It has a mobile penetration rate of about 75%; There are about 3.5 million smartphones in the Angolan market; There are about of optical fibre installed in the country.\n\nThe first Angolan satellite, AngoSat-1, will be ready for launch into orbit in 2017 and it will ensure telecommunications throughout the country. According to Aristides Safeca, Secretary of State for Telecommunications, the satellite will provide telecommunications services, TV, internet and e-government and will remain into orbit \"at best\" for 18 years.\n\nThe management of the domain '.ao' on web pages, will go from Portugal to Angola in 2015, following the approval of a new legislation by the Angolan Government. The joint decree of the minister of Telecommunications and Information Technologies, José Carvalho da Rocha, and the minister of Science and Technology, Maria Cândida Pereira Teixeira, states that \"under the massification\" of that Angolan domain, \"conditions are created for the transfer of the domain root '.ao' of Portugal to Angola\".\n\nAngola has a population of 24,383,301 inhabitants according to the preliminary results of its 2014 census, the first one conducted or carried out since 15 December 1970. It is composed of Ovimbundu (language Umbundu) 37%, Ambundu (language Kimbundu) 23%, Bakongo 13%, and 32% other ethnic groups (including the Chokwe, the Ovambo, the Ganguela and the Xindonga) as well as about 2% \"mestiços\" (mixed European and African), 1.6% Chinese and 1% European. The Ambundu and Ovimbundu ethnic groups combined form a majority of the population, at 62%. The population is forecast to grow to over 60 million people to 2050, 2.7 times the 2014 population. However, on 23 March 2016, official data revealed by Angola's National Statistic Institute – Instituto Nacional de Estatística (INE), states that Angola has a population of 25.789.024 inhabitants.\n\nIt is estimated that Angola was host to 12,100 refugees and 2,900 asylum seekers by the end of 2007. 11,400 of those refugees were originally from the Democratic Republic of Congo, who arrived in the 1970s. there were an estimated 400,000 Democratic Republic of the Congo migrant workers, at least 220,000 Portuguese, and about 259,000 Chinese living in Angola.\n\nSince 2003, more than 400,000 Congolese migrants have been expelled from Angola. Prior to independence in 1975, Angola had a community of approximately 350,000 Portuguese, but the vast majority left after independence and the ensuing civil war. However, Angola has recovered its Portuguese minority in recent years; currently, there are about 200,000 registered with the consulates, and increasing due to the debt crisis in Portugal and the relative prosperity in Angola. The Chinese population stands at 258,920, mostly composed of temporary migrants. Also, there is a small Brazilian community of about 5,000 people.\n\nThe total fertility rate of Angola is 5.54 children born per woman (2012 estimates), the 11th highest in the world.\n\nThe languages in Angola are those originally spoken by the different ethnic groups and Portuguese, introduced during the Portuguese colonial era. The most widely spoken indigenous languages are Umbundu, Kimbundu and Kikongo, in that order. Portuguese is the official language of the country.\n\nAlthough the exact numbers of those fluent in Portuguese or who speak Portuguese as a first language are unknown, a 2012 study mentions that Portuguese is the first language of 39% of the population. In 2014, a census carried out by the Instituto Nacional de Estatística in Angola mentions that 71.15% of the nearly 25.8 million inhabitants of Angola (meaning around 18.3 million people) use Portuguese as a first or second language.\n\nThere are about 1,000 religious communities, mostly Christian, in Angola. While reliable statistics are nonexistent, estimates have it that more than half of the population are Catholics, while about a quarter adhere to the Protestant churches introduced during the colonial period: the Congregationalists mainly among the Ovimbundu of the Central Highlands and the coastal region to its west, the Methodists concentrating on the Kimbundu speaking strip from Luanda to Malanje, the Baptists almost exclusively among the Bakongo of the north-west (now present in Luanda as well) and dispersed Adventists, Reformed and Lutherans. In Luanda and region there subsists a nucleus of the \"syncretic\" Tocoists and in the north-west a sprinkling of Kimbanguism can be found, spreading from the Congo/Zaïre. Since independence, hundreds of Pentecostal and similar communities have sprung up in the cities, where by now about 50% of the population is living; several of these communities/churches are of Brazilian origin.\n\nIn a study assessing nations' levels of religious regulation and persecution with scores ranging from 0 to 10 where 0 represented low levels of regulation or persecution, Angola was scored 0.8 on Government Regulation of Religion, 4.0 on Social Regulation of Religion, 0 on Government Favoritism of Religion and 0 on Religious Persecution.\n\nForeign missionaries were very active prior to independence in 1975, although since the beginning of the anti-colonial fight in 1961 the Portuguese colonial authorities expelled a series of Protestant missionaries and closed mission stations based on the belief that the missionaries were inciting pro-independence sentiments. Missionaries have been able to return to the country since the early 1990s, although security conditions due to the civil war have prevented them until 2002 from restoring many of their former inland mission stations.\n\nThe Catholic Church and some major Protestant denominations mostly keep to themselves in contrast to the \"New Churches\" which actively proselytize. Catholics, as well as some major Protestant denominations, provide help for the poor in the form of crop seeds, farm animals, medical care and education.\n\nThe \"substrate\" of Angolan culture is African, predominantly Bantu, while Portuguese culture has had a significant impact, specifically in terms of language and religion. The diverse ethnic communities – the Ovimbundu, Ambundu, Bakongo, Chokwe, Mbunda and other peoples – to varying degrees maintain their own cultural traits, traditions and languages, but in the cities, where slightly more than half of the population now lives, a mixed culture has been emerging since colonial times; in Luanda, since its foundation in the 16th century. In this urban culture, the Portuguese heritage has become more and more dominant. African roots are evident in music and dance, and is moulding the way in which Portuguese is spoken. This process is well reflected in contemporary Angolan literature, especially in the works of Angolan authors.\n\nIn 2014, Angola resumed the National Festival of Angolan Culture after a 25-year break. The festival took place in all the provincial capitals and lasted for 20 days, with the theme \"Culture as a Factor of Peace and Development\".\n\nEpidemics of cholera, malaria, rabies and African hemorrhagic fevers like Marburg hemorrhagic fever, are common diseases in several parts of the country. Many regions in this country have high incidence rates of tuberculosis and high HIV prevalence rates. Dengue, filariasis, leishmaniasis and onchocerciasis (river blindness) are other diseases carried by insects that also occur in the region. Angola has one of the highest infant mortality rates in the world and one of the world's lowest life expectancies. A 2007 survey concluded that low and deficient niacin status was common in Angola. Demographic and Health Surveys is currently conducting several surveys in Angola on malaria, domestic violence and more.\n\nIn September 2014, the Angolan Institute for Cancer Control (IACC) was created by presidential decree, and it will integrate the National Health Service in Angola. The purpose of this new centre is to ensure the health and medical care in oncology, policy implementation, programmes and plans for prevention and specialised treatment. This cancer institute will be assumed as a reference institution in the central and southern regions of Africa.\n\nIn 2014, Angola launched a national campaign of vaccination against measles, extended to every child under ten years old and aiming to go to all 18 provinces in the country. The measure is part of the Strategic Plan for the Elimination of Measles 2014–2020 created by the Angolan Ministry of Health which includes strengthening routine immunisation, a proper dealing with measles cases, national campaigns, introducing a second dose of vaccination in the national routine vaccination calendar and active epidemiological surveillance for measles. This campaign took place together with the vaccination against polio and vitamin A supplementation.\n\nA yellow fever outbreak, the worst in the country in three decades began in December 2015. By August 2016, when the outbreak began to subside, nearly 4,000 people were suspected of being infected. As many as 369 may have died. The outbreak began in the capital, Luanda, and spread to at least 16 of the 18 provinces.\n\nAlthough by law education in Angola is compulsory and free for eight years, the government reports that a percentage of pupils are not attending due to a lack of school buildings and teachers. Pupils are often responsible for paying additional school-related expenses, including fees for books and supplies.\n\nIn 1999, the gross primary enrollment rate was 74 percent and in 1998, the most recent year for which data are available, the net primary enrollment rate was 61 percent. Gross and net enrollment ratios are based on the number of pupils formally registered in primary school and therefore do not necessarily reflect actual school attendance. There continue to be significant disparities in enrollment between rural and urban areas. In 1995, 71.2 percent of children ages 7 to 14 years were attending school. It is reported that higher percentages of boys attend school than girls. During the Angolan Civil War (1975–2002), nearly half of all schools were reportedly looted and destroyed, leading to current problems with overcrowding.\n\nThe Ministry of Education recruited 20,000 new teachers in 2005 and continued to implement teacher trainings. Teachers tend to be underpaid, inadequately trained and overworked (sometimes teaching two or three shifts a day). Some teachers may reportedly demand payment or bribes directly from their pupils. Other factors, such as the presence of landmines, lack of resources and identity papers, and poor health prevent children from regularly attending school. Although budgetary allocations for education were increased in 2004, the education system in Angola continues to be extremely under-funded.\n\nAccording to estimates by the UNESCO Institute for Statistics, the adult literacy rate in 2011 was 70.4%. 82.9% of males and 54.2% of women are literate as of 2001. Since independence from Portugal in 1975, a number of Angolan students continued to be admitted every year at high schools, polytechnical institutes and universities in Portugal, Brazil and Cuba through bilateral agreements; in general, these students belong to the elites.\n\nIn September 2014, the Angolan Ministry of Education announced an investment of 16 million Euros in the computerisation of over 300 classrooms across the country. The project also includes training teachers at a national level, \"as a way to introduce and use new information technologies in primary schools, thus reflecting an improvement in the quality of teaching.\"\n\nIn 2010, the Angolan government started building the Angolan Media Libraries Network, distributed throughout several provinces in the country to facilitate the people's access to information and knowledge. Each site has a bibliographic archive, multimedia resources and computers with Internet access, as well as areas for reading, researching and socialising. The plan envisages the establishment of one media library in each Angolan province by 2017. The project also includes the implementation of several media libraries, in order to provide the several contents available in the fixed media libraries to the most isolated populations in the country. At this time, the mobile media libraries are already operating in the provinces of Luanda, Malanje, Uíge, Cabinda and Lunda South. As for REMA, the provinces of Luanda, Benguela, Lubango and Soyo have currently working media libraries.\n\nAngola hosted the 2010 Africa Cup of Nations.\nAngola is the top basketball team of FIBA Africa, and a regular competitor at the Summer Olympic Games and the FIBA World Cup. The Angola national football team qualified for the 2006 FIFA World Cup, as this was their first appearance on the World Cup finals stage. They were eliminated after one defeat and two draws in the group stage. They won 3 COSAFA Cups and finished runner up in 2011 African Nations Championship. Angola has participated in the World Women's Handball Championship for several years. The country has also appeared in the Summer Olympics for seven years and both compete and have hosted the FIRS Roller Hockey World Cup. Angola is also often believed to have historic roots in the martial art \"Capoeira Angola\" and \"Batuque\" which were practiced by enslaved African Angolans transported as part of the Atlantic slave trade.\n\n\n\n"}
{"id": "704", "url": "https://en.wikipedia.org/wiki?curid=704", "title": "Demographics of Angola", "text": "Demographics of Angola\n\nThis article is about the demographic features of the population of Angola, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.\nAccording to 2014 census data, Angola had a population of 25,789,024 inhabitants in 2014.\nEthnically, there are three main groups, each speaking a Bantu language: the Ovimbundu who represent 37% of the population, the Ambundu with 25%, and the Bakongo 13%. Other numerically important groups include the closely interrelated Chokwe and Lunda, the Ganguela and Nyaneka-Khumbi (in both cases classification terms that stand for a variety of small groups), the Ovambo, the Herero, the Xindonga and scattered residual groups of San. In addition, mixed race (European and African) people amount to about 2%, with a small (1%) population of whites, mainly ethnically Portuguese.\n\nAs a former overseas territory of Portugal until 1975, Angola possesses a Portuguese population of over 200,000, a number that has been growing from 2000 onwards, because of Angola's growing demand for qualified human resources. Besides the Portuguese, significant numbers of people from other European and from diverse Latin American countries (especially Brazil) can be found. From the 2000s, many Chinese have settled and started up small businesses, while at least as many have come as workers for large enterprises (construction or other). Observers claim that the Chinese community in Angola might include as many as 300,000 persons at the end of 2010, but reliable statistics are not at this stage available. In 1974/75, over 25,000 Cuban soldiers arrived in Angola to help the MPLA forces at the beginning of the Angolan Civil War. Once this was over, a massive development cooperation in the field of health and education brought in numerous civil personnel from Cuba. However, only a very small percentage of all these people has remained in Angola, either for personal reasons (intermarriage) or as professionals (e.g., medical doctors).\n\nThe largest religious denomination is Catholicism, to which adheres about half the population. Roughly 26% are followers of traditional forms of Protestantism (Congregationals, Methodists, Baptista, Lutherans, Reformed), but over the last decades there has in addition been a growth of Pentecostal communities and African Initiated Churches. In 2006, one out of 221 people were Jehovah's Witnesses. Blacks from Mali, Nigeria and Senegal are mostly Sunnite Muslims, but do not make up more than 1 - 2% of the population. By now few Angolans retain African traditional religions following different ethnic faiths.\n\nAccording to the total population was in , compared to only 4 148 000 in 1950. The proportion of children below the age of 15 in 2010 was 46.6%, 50.9% was between 15 and 65 years of age, while 2.5% was 65 years or older\n\nStructure of the population (DHS 2011) (Males 19 707, Females 20 356 = 40 063) :\n\nRegistration of vital events is in Angola not complete. The Population Department of the United Nations prepared the following estimates.\n\nTotal Fertility Rate (TFR) (Wanted TFR) and Crude Birth Rate (CBR):\n\nThe following demographic statistics are from the CIA World Factbook, unless otherwise indicated.\n\n\nThe population is growing by 2.184% annually. There are 44.51 births and 24.81 deaths per 1,000 citizens. The net migration rate is 2.14 migrants per 1,000 citizens. The fertility rate of Angola is 5.97 children born per woman as of 2011. The infant mortality rate is 184.44 deaths for every 1,000 live births with 196.55 deaths for males and 171.72 deaths for females for every 1,000 live births. Life expectancy at birth is 37.63 years; 36.73 years for males and 38.57 years for females.\n\n\nAccording to the CIA World Factbook, 2% of adults (aged 15–49) are living with HIV/AIDS (as of 2009). The risk of contracting disease is very high. There are food and waterborne diseases, bacterial and protozoal diarrhea, hepatitis A, and typhoid fever; vectorborne diseases, malaria, African trypanosomiasis (sleeping sickness); respiratory disease: meningococcal meningitis, and schistosomiasis, a water contact disease, as of 2005.\n\nRoughly 37% of Angolans are Ovimbundu, 25% are Ambundu, 13% are Bakongo, 2% are mestiço, 1-2% are white Africans, and people from other African ethnicities make up 22% of Angola's population.\n\nAngola is a majority Christian country. Official statistics don't exist, but it is estimated that over 80% belong to a Christian church or community. More than half are Catholic, the remaining ones comprising members of traditional Protestant churches as well as of Pentecostal communities. Only 1 - 2% are Muslims - generally immigrants from other African countries. Traditional indigenous religions are practized by a very small minority, generally in peripheral rural societies.\n\nLiteracy is quite low, with 67.4% of the population over the age of 15 able to read and write in Portuguese. 82.9% of males and 54.2% of women are literate as of 2001.\n\nPortuguese is the official language of Angola, but Bantu and other African languages are also widely spoken. In fact, Kikongo, Kimbundu, Umbundu, Tuchokwe, Nganguela, and Ukanyama have the official status of \"national languages\". The mastery of Portuguese is widespread; in the cities the overwhelming majority are either fluent in Portuguese or have at least a reasonable working knowledge of this language; an increasing minority are native Portuguese speakers and have a poor, if any, knowledge of an African language.\n\n\n"}
{"id": "705", "url": "https://en.wikipedia.org/wiki?curid=705", "title": "Politics of Angola", "text": "Politics of Angola\n\nSince the adoption of a new constitution in 2010, the politics of Angola takes place in a framework of a presidential republic, whereby the President of Angola is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in the President, the government and parliament.\n\nAngola changed from a one-party Marxist-Leninist system ruled by the Popular Movement for the Liberation of Angola (MPLA), in place since independence in 1975, to a multiparty democracy based on a new constitution adopted in 1992. That same year the first parliamentary and presidential elections were held. The MPLA won an absolute majority in the parliamentary elections. In the presidential elections, President José Eduardo dos Santos won the first round election with more than 49% of the vote to Jonas Savimbi's 40%. A runoff election would have been necessary, but never took place. The renewal of civil war immediately after the elections, which were considered as fraudulent by UNITA, and the collapse of the Lusaka Protocol, created a split situation. To a certain degree the new democratic institutions worked, notably the National Assembly, with the active participation of UNITA's and the FNLA's elected MPs - while José Eduardo dos Santos continued to exercise his functions without democratic legitimation. However the armed forces of the MPLA (now the official armed forces of the Angolan state) and of UNITA fought each other until the leader of UNITA, Jonas Savimbi, was killed in action in 2002.\n\nFrom 2002 to 2010, the system as defined by the constitution of 1992 functioned in a relatively normal way. The executive branch of the government was composed of the President, the Prime Minister and Council of Ministers. The Council of Ministers, composed of all ministers and vice ministers, met regularly to discuss policy issues. Governors of the 18 provinces were appointed by and served at the pleasure of the president. The Constitutional Law of 1992 established the broad outlines of government structure and the rights and duties of citizens. The legal system was based on Portuguese and customary law but was weak and fragmented. Courts operated in only 12 of more than 140 municipalities. A Supreme Court served as the appellate tribunal; a Constitutional Court with powers of judicial review was never constituted despite statutory authorization. In practice, power was more and more concentrated in the hands of the President who, supported by an ever-increasing staff, largely controlled parliament, government, and the judiciary.\n\nThe 26-year-long civil war has ravaged the country's political and social institutions. The UN estimates of 1.8 million internally displaced persons (IDPs), while generally the accepted figure for war-affected people is 4 million. Daily conditions of life throughout the country and specifically Luanda (population approximately 6 million) mirror the collapse of administrative infrastructure as well as many social institutions. The ongoing grave economic situation largely prevents any government support for social institutions. Hospitals are without medicines or basic equipment, schools are without books, and public employees often lack the basic supplies for their day-to-day work.\n\nThe 2010 constitution grants the President almost absolute power. Elections for the National assembly are to take place every five years, and the President is automatically the leader of the winning party or coalition. It is for the President to appoint (and dismiss) all of the following:\nThe President is also provided a variety of powers, like defining the policy of the country. Even though it's not up to him/her to make laws (only to promulgate them and make edicts), the President is the leader of the winning party.\nThe only \"relevant\" post that is not directly appointed by the President is the Vice-President, which is the second in the winning party.\n\nThe National Assembly (\"Assembleia Nacional\") has 223 members, elected for a four-year term, 130 members by proportional representation, 90 members in provincial districts, and 3 members to represent Angolans abroad. The next general elections, due for 1997, have been rescheduled for 5 September 2008. The ruling party MPLA won 82% (191 seats in the National Assembly) and the main opposition party won only 10% (16 seats). The elections however have been described as only partly free but certainly not fair. A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008.\n\nSupreme Court (or \"Tribunal da Relacao\") judges of the Supreme Court are appointed by the president. The Constitutional Court, with the power of judicial review, contains 11 justices. Four are appointed by the President, four by the National Assembly, two by the Superior Council of the Judiciary, and one elected by the public.\n\nAngola has eighteen provinces (provincias, singular - provincia); Bengo, Benguela, Bie, Cabinda, Cuando Cubango, Cuanza Norte, Cuanza Sul, Cunene, Huambo, Huila, Luanda, Lunda Norte, Lunda Sul, Malanje, Moxico, Namibe, Uige, Zaire\n\nFront for the Liberation of the Enclave of Cabinda or FLEC (Henrique N'zita Tiago; António Bento Bembe)\n\nAfrican, Caribbean and Pacific Group of States, AfDB, CEEAC, United Nations Economic Commission for Africa, FAO, Group of 77, IAEA, IBRD, ICAO, International Criminal Court (signatory), ICFTU, International Red Cross and Red Crescent Movement, International Development Association, IFAD, IFC, IFRCS, International Labour Organization, International Monetary Fund, International Maritime Organization, Interpol, IOC, International Organization for Migration, ISO (correspondent), ITU, Non-Aligned Council (temporary), UNCTAD, UNESCO, UNIDO, UPU, World Customs Organization, World Federation of Trade Unions, WHO, WIPO, WMO, WToO, WTrO\n\n\n"}
{"id": "706", "url": "https://en.wikipedia.org/wiki?curid=706", "title": "Economy of Angola", "text": "Economy of Angola\n\nThe Economy of Angola is one of the fastest-growing in the world, with reported annual average GDP growth of 11.1 percent from 2001 to 2010. It is still recovering from 27 years of the civil war that plagued the country from its independence in 1975 to 2002. Despite extensive oil and gas resources, diamonds, hydroelectric potential, and rich agricultural land, Angola remains poor, and a third of the population relies on subsistence agriculture. Since 2002, when the 27-year civil war ended, the nation has worked to repair and improve ravaged infrastructure and weakened political and social institutions. High international oil prices and rising oil production have contributed to the very strong economic growth since 1998, but corruption and public-sector mismanagement remain, particularly in the oil sector, which accounts for over 50 percent of GDP, over 90 percent of export revenue, and over 80 percent of government revenue.\n\nThe Portuguese explorer Diogo Cão reached the Angolan coast in 1484, after which Portugal began to found trading posts and forts along the shore. Paulo Dias de Novais founded Sāo Paulo de Loanda (Luanda) in 1575. São Felipe de Benguella (Benguela) followed in 1587.\n\nThe principal early trade was in slaves. Portuguese merchants purchased the slaves from the local Imbangala and Mbundu peoples, notable slave hunters, and sold them to the sugarcane plantations in Brazil. Brazilian ships were frequent visitors to Luanda and Benguela and Angola functioned as a kind of colony of Brazil, with Brazilian Jesuits active in its religious and educational centers.\n\nThe Portuguese Empire was neglected during the period of the Iberian Union, which lasted from 1580 to 1640. The Dutch, bitter enemies of their former masters in Spain, invaded many Portuguese overseas possessions. During Portugal's separatist war against Spain, the Dutch occupied Luanda from 1640 to 1648, calling it \"Fort Aardenburgh\". The Dutch used the territory to supply their own slaves to the sugarcane plantations of Northeastern Brazil (Pernambuco, Olinda, Recife), which they had also seized from Portugal. John Maurice, Prince of Nassau-Siegen, conquered the Portuguese possessions of Saint George del Mina, Saint Thomas, and Luanda, Angola, on the west coast of Africa. Portugal recovered the territory between 1648 and 1650.\n\nIn the high plains, the Planalto, the most important native states were Bié and Bailundo, the latter being noted for its production of foodstuffs and rubber. Portugal expanded into their territory, but did not control much of the interior prior to the late 19th century.\n\nThe Portuguese started to develop townships, trading posts, logging camps and small processing factories. From 1764 onwards, there was a gradual change from a slave-based society to one based on production for domestic consumption and export. Following the independence of Brazil in 1822, the slave trade was formally abolished in 1836. However it did continue locally into the 20th century. In 1844, Angola's ports were opened to foreign shipping.\n\nBy 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside of Mainland Portugal, full of trading companies, exporting peanut oil, copal, timber, and cocoa. The principal exports of the post-slave economy in the 19th century were rubber, beeswax, and ivory. Maize, tobacco, dried meat and cassava flour also began to be locally produced. Prior to the First World War, exportation of coffee, palm kernels and oil, cattle, leather and hides, and salt fish joined the principal exports, with small quantities of gold and cotton also being produced. Grains, sugar, and rum were also produced for local consumption. The principal imports were foodstuffs, cotton goods, hardware, and British coal. Legislation against foreign traders was implemented in the 1890s. The territory's prosperity, however, continued to depend on plantations worked by labor \"indentured\" from the interior.\n\nFrom the 1920s to the 1960s, strong economic growth, abundant natural resources and development of infrastructure, led to the arrival of even more Portuguese settlers. Petroleum was known to exist as early as the mid-19th century, but modern exploitation didn't begin until in 1955. Production began in the Cuanza basin in the 1950s, in the Congo basin in the 1960s, and in the exclave of Cabinda in 1968. The Portuguese government granted operating rights for Block Zero to the Cabinda Gulf Oil Company, a subsidiary of ChevronTexaco, in 1955. Oil production surpassed the exportation of coffee as Angola's largest export in 1973.\n\nA leftist military-led coup d'état, started on April 25, 1974, in Lisbon, overthrew the Marcelo Caetano government in Portugal, and promised to hand over power to an independent Angolan government. Mobutu Sese Seko, the President of Zaire, met with António de Spínola, the transitional President of Portugal, on September 15, 1974, on Sal island in Cape Verde, crafting a plan to empower Holden Roberto of the National Liberation Front of Angola, Jonas Savimbi of UNITA, and Daniel Chipenda of the MPLA's eastern faction at the expense of MPLA leader Agostinho Neto while retaining the façade of national unity. Mobutu and Spínola wanted to present Chipenda as the MPLA head, Mobutu particularly preferring Chipenda over Neto because Chipenda supported autonomy for Cabinda. The Angolan exclave has immense petroleum reserves estimated at around 300 million tons (~300 kg) which Zaire, and thus the Mobutu government, depended on for economic survival. After independence thousands of white Portuguese left, most of them to Portugal and many travelling overland to South Africa. There was an immediate crisis because the indigenous African population lacked the skills and knowledge needed to run the country and maintain its well-developed infrastructure.\n\nThe Angolan government created Sonangol, a state-run oil company, in 1976. Two years later Sonangol received the rights to oil exploration and production in all of Angola. After independence from Portugal in 1975, Angola was ravaged by a horrific civil war between 1975 and 2002.\n\nUnited Nations Angola Verification Mission III and MONUA spent USD1.5 billion overseeing implementation of the Lusaka Protocol, a 1994 peace accord that ultimately failed to end the civil war. The protocol prohibited UNITA from buying foreign arms, a provision the United Nations largely did not enforce, so both sides continued to build up their stockpile. UNITA purchased weapons in 1996 and 1997 from private sources in Albania and Bulgaria, and from Zaire, South Africa, Republic of the Congo, Zambia, Togo, and Burkina Faso. In October 1997 the UN imposed travel sanctions on UNITA leaders, but the UN waited until July 1998 to limit UNITA's exportation of diamonds and freeze UNITA bank accounts. While the U.S. government gave USD250 million to UNITA between 1986 and 1991, UNITA made USD1.72 billion between 1994 and 1999 exporting diamonds, primarily through Zaire to Europe. At the same time the Angolan government received large amounts of weapons from the governments of Belarus, Brazil, Bulgaria, China, and South Africa. While no arms shipment to the government violated the protocol, no country informed the U.N. Register on Conventional Weapons as required.\n\nDespite the increase in civil warfare in late 1998, the economy grew by an estimated 4% in 1999. The government introduced new currency denominations in 1999, including a 1 and 5 kwanza note.\n\nAn economic reform effort was launched in 1998. Angola ranked 160 of 174 nations in the United Nations Human Development Index in 2000. In April 2000 Angola started an International Monetary Fund (IMF) Staff-Monitored Program (SMP). The program formally lapsed in June 2001, but the IMF remains engaged. In this context the Government of Angola has succeeded in unifying exchange rates and has raised fuel, electricity, and water rates. The Commercial Code, telecommunications law, and Foreign Investment Code are being modernized. A privatization effort, prepared with World Bank assistance, has begun with the BCI bank. Nevertheless, a legacy of fiscal mismanagement and corruption persists. The civil war internally displaced 3.8 million people, 32% of the population, by 2001. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.\n\nAngola produced over of diamonds in 2003, and production was expected to grow to per year by 2007. In 2004 China's Eximbank approved a $2 billion line of credit to Angola to rebuild infrastructure. The economy grew 18% in 2005 and growth was expected to reach 26% in 2006 and stay above 10% for the rest of the decade.\n\nThe construction industry is taking advantage of the growing economy, with various housing projects stimulated by the government initiatives for example the \"Angola Investe\" program and the \"Casa Feliz\" or \"Meña\" projects. Not all public construction projects are functional. A case in point: Kilamba Kiaxi, where a whole new satellite town of Luanda, consisting of housing facilities for several hundreds of thousands of people, was completely uninhabited for over four years because of skyrocketing prices, but completely sold out after the government decreased the original price and created mortgage plans at around the election time thus made it affordable for middle-class people. \nChevronTexaco started pumping from Block 14 in January 2000, but production decreased to in 2007 due to poor-quality oil. Angola joined the Organization of the Petroleum Exporting Countries on January 1, 2007.\n\nCabinda Gulf Oil Company found Malange-1, an oil reservoir in Block 14, on August 9, 2007.\n\nDespite its abundant natural resources, output per capita is among the world's lowest. Subsistence agriculture provides the main livelihood for 85% of the population. Oil production and the supporting activities are vital to the economy, contributing about 45% to GDP and 90% of exports. Growth is almost entirely driven by rising oil production which surpassed in late-2005 and which is expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate owned by the Angolan government. With revenues booming from oil exports, the government has started to implement ambitious development programs to build roads and other basic infrastructure for the nation.\n\nIn the last decade of the colonial period, Angola was a major African food exporter but now imports almost all its food. Severe wartime conditions, including extensive planting of landmines throughout the countryside, have brought agricultural activities to a near-standstill. Some efforts to recover have gone forward, however, notably in fisheries. Coffee production, though a fraction of its pre-1975 level, is sufficient for domestic needs and some exports. Expanding oil production is now almost half of GDP and 90% of exports, at . Diamonds provided much of the revenue for Jonas Savimbi's UNITA rebellion through illicit trade. Other rich resources await development: gold, forest products, fisheries, iron ore, coffee, and fruits.\n\nThis is a chart of trend of nominal gross domestic product of Angola at market prices using International Monetary Fund data; figures are in millions of units.\nExports in 2004 reached US$10,530,764,911. The vast majority of Angola's exports, 92% in 2004, are petroleum products. US$785 million worth of diamonds, 7.5% of exports, were sold abroad that year. Nearly all of Angola's oil goes to the United States, in 2006, making it the eighth largest supplier of oil to the United States, and to China, in 2006. In the first quarter of 2008, Angola became the main exporter of oil to China. The rest of its petroleum exports go to Europe and Latin America. U.S. companies account for more than half the investment in Angola, with Chevron-Texaco leading the way. The U.S. exports industrial goods and services, primarily oilfield equipment, mining equipment, chemicals, aircraft, and food, to Angola, while principally importing petroleum. Trade between Angola and South Africa exceeded USD 300 million in 2007. From the 2000s many Chinese have settled and started up businesses.\n\nAngola produces and exports more petroleum than any other nation in sub-Saharan Africa, surpassing Nigeria in the 2000s. In January 2007 Angola became a member of OPEC. By 2010 production is expected to double the 2006 output level with development of deep-water offshore oil fields. Oil sales generated USD 1.71 billion in tax revenue in 2004 and now makes up 80% of the government's budget, a 5% increase from 2003, and 45% of GDP.\n\nChevron Corporation produces and receives , 27% of Angolan oil. Total S.A., Chevron Corporation, ExxonMobil, Eni, Petrobras, and BP also operate in the country.\n\nBlock Zero provides the majority of Angola's crude oil production with produced annually. The largest fields in Block Zero are Takula (Area A), Numbi (Area A), and Kokongo (Area B). Chevron operates in Block Zero with a 39.2% share. SONANGOL, the state oil company, Total, and Eni own the rest of the block. Chevron also operates Angola's first producing deepwater section, Block 14, with .\n\nThe United Nations has criticized the Angolan government for using torture, rape, summary executions, arbitrary detention, and disappearances, actions which Angolan government has justified on the need to maintain oil output.\n\nAngola is the third-largest trading partner of the United States in Sub-Saharan Africa, largely because of its petroleum exports. The U.S. imports 7% of its oil from Angola, about three times as much as it imported from Kuwait just prior to the Gulf War in 1991. The U.S. Government has invested USD $4 billion in Angola's petroleum sector.\n\nOil makes up over 90% of Angola's exports.\n\nAngola is the third largest producer of diamonds in Africa and has only explored 40% of the diamond-rich territory within the country, but has had difficulty in attracting foreign investment because of corruption, human rights violations, and diamond smuggling. Production rose by 30% in 2006 and Endiama, the national diamond company of Angola, expects production to increase by 8% in 2007 to 10 million carats annually. The government is trying to attract foreign companies to the provinces of Bié, Malanje and Uíge.\n\nThe Angolan government loses $375 million annually from diamond smuggling. In 2003 the government began Operation Brilliant, an anti-smuggling investigation that arrested and deported 250,000 smugglers between 2003 and 2006. Rafael Marques, a journalist and human rights activist, described the diamond industry in his 2006 \"Angola's Deadly Diamonds\" report as plagued by \"murders, beatings, arbitrary detentions and other human rights violations.\" Marques called on foreign countries to boycott Angola's \"conflict diamonds\". In December 2014, the Bureau of International Labor Affairs issued a \"List of Goods Produced by Child Labor or Forced Labor\" that classified Angola as one of the major diamond-producing African countries relying on both child labor and forced labor. The U.S. Department of Labor reported that \"there is little publicly available information on [Angola's] efforts to enforce child labor law\". Diamonds accounted for 1.48% of Angolan exports in 2014.\n\nUnder Portuguese rule, Angola began mining iron in 1957, producing 1.2 million tons in 1967 and 6.2 million tons by 1971. In the early 1970s, 70% of Portuguese Angola's iron exports went to Western Europe and Japan. After independence in 1975, the Angolan Civil War (1975–2002) destroyed most of the territory's mining infrastructure. The redevelopment of the Angolan mining industry started in the late 2000s.\n\n\n\n"}
{"id": "708", "url": "https://en.wikipedia.org/wiki?curid=708", "title": "Transport in Angola", "text": "Transport in Angola\n\nTransport in Angola comprises:\n\nThere are three separate railway lines in Angola:\n\nReconstruction of these three lines began in 2005 and is expected to be completed by the end of the year 2012. The Benguela Railway already connects to the Democratic Republic of the Congo.\n\n\n\nIn April 2012, the Zambian Development Agency (ZDA) and an Angolan company signed a memorandum of understanding (MoU) to build a multi-product pipeline from Lobito to Lusaka, Zambia, to deliver various refined products to Zambia.\n\nAngola plans to build an oil refinery in Lobito in the coming years.\n\nThe government plans to build a deep-water port at Barra do Dande, north of Luanda, in Bengo province near Caxito.\n\n\n\n\n\nAngola had an estimated total of 43 airports as of 2004, of which 31 had paved runways as of 2005. There is an international airport at Luanda. International and domestic services are maintained by TAAG Angola Airlines, Aeroflot, British Airways, Brussels Airlines, Lufthansa, Air France, Air Namibia, Cubana, Ethiopian Airlines, Emirates, Delta Air Lines, Royal Air Maroc, Iberia, Hainan Airlines, Kenya Airways, South African Airways, TAP Air Portugal and several regional carriers. In 2003, domestic and international carriers carried 198,000 passengers. There are airstrips for domestic transport at Benguela, Cabinda, Huambo, Namibe, and Catumbela.\n\n\"This article comes from the CIA World Factbook 2003.\"\n"}
{"id": "709", "url": "https://en.wikipedia.org/wiki?curid=709", "title": "Angolan Armed Forces", "text": "Angolan Armed Forces\n\nThe Angolan Armed Forces (Portuguese: \"Forças Armadas Angolanas\") or FAA are the military of Angola.\n\nThe FAA include the General Staff of the Armed Forces and three components: the Army (\"Exército\"), the Navy (\"Marinha de Guerra\") and the National Air Force (\"National Air Force\"). Reported total manpower in 2013 was about 107,000.\n\nThe FAA is headed by Chief of the General Staff Geraldo Sachipengo Nunda since 2010, who reports to the Minister of National Defense, currently João Lourenço.\n\nThe FAA succeeded to the previous People's Armed Forces for the Liberation of Angola (FAPLA) following the abortive Bicesse Accord with the Armed Forces of the Liberation of Angola (FALA), armed wing of the National Union for the Total Independence of Angola (UNITA). As part of the peace agreement, troops from both armies were to be demilitarized and then integrated. Integration was never completed as UNITA and FALA went back to war in 1992. Later, consequences for FALA personnel in Luanda were harsh with FAPLA veterans persecuting their erstwhile opponents in certain areas and reports of vigilantism.\n\nThe Army (\"Exército\") is the land component of the FAA. It is organized in six military regions (Cabinda, Luanda, North, Center, East and South), with an infantry division being based in each one. Distributed by the six military regions / infantry divisions, there are 25 motorized infantry brigades, one tank brigade and one engineering brigade. The Army also includes an artillery regiment, the Military Artillery School, the Army Military Academy, an anti-aircraft defense group, a composite land artillery group, a military police regiment, a logistical transportation regiment and a field artillery brigade. The Army further includes the Special Forces Brigade (including Commandos and Special Operations units), but this unit is under the direct command of the General Staff of the FAA.\n\nOn August 1, 1974 a few months after a military coup d'état had overthrown the Lisbon regime and proclaimed its intention of granting independence to Angola, the MPLA announced the formation of FAPLA, which replaced the EPLA. By 1976 FAPLA had been transformed from lightly armed guerrilla units into a national army capable of sustained field operations.\n\nIn 1990-91, the Army had ten military regions and an estimated 73+ 'brigades', each with a mean strength of 1,000 and comprising inf, tank, APC, artillery, and AA units as required. The Library of Congress said in 1990 that '[t]he regular army's 91,500 troops were organized into more than seventy brigades ranging from 750 to 1,200 men each and deployed throughout the ten military regions. Most regions were commanded by lieutenant colonels, with majors as deputy commanders, but some regions were commanded by majors. Each region consisted of one to four provinces, with one or more infantry brigades assigned to it. The brigades were generally dispersed in battalion or smaller unit formations to protect strategic terrain, urban centers, settlements, and critical infrastructure such as bridges and factories. Counterintelligence agents were assigned to all field units to thwart UNITA infiltration. The army's diverse combat capabilities were indicated by its many regular and motorised infantry brigades with organic or attached armor, artillery, and air defense units; two militia infantry brigades; four antiaircraft artillery brigades; ten tank battalions; and six artillery battalions. These forces were concentrated most heavily in places of strategic importance and recurring conflict: the oil-producing Cabinda Province, the area around the capital, and the southern provinces where UNITA and South African forces operated.'\n\nIt was reported in 2011 that the army was by far the largest of the services with about 120,000 men and women. The Angolan Army has around 29,000 \"ghost workers\" who remain enrolled in the ranks of the FAA and therefore receive a salary.\n\nIn 2013, the International Institute for Strategic Studies reported that the FAA had six divisions, the 1st, 5th, and 6th with two or three infantry brigades, and the 2nd, 3rd, and 4th with five to six infantry brigades. The 4th Division included a tank regiment. A separate tank brigade and special forces brigade were also reported.\n\nAs of 2011, the IISS reported the ground forces had 42 armoured/infantry regiments ('detachments/groups - strength varies') and 16 infantry 'brigades'. These probably comprised infantry, tanks, APC, artillery, and AA units as required. Major equipment included over 140 main battle tanks, 600 reconnaissance vehicles, over 920 AFVs, infantry fighting vehicles, 298 howitzers.\n\nIt was reported on May 3, 2007, that the Special Forces Brigade of the Angolan Armed Forces (FAA) located at Cabo Ledo region, northern Bengo Province, would host a 29th anniversary celebration for the entire armed forces. The brigade was reportedly formed on 5 May 1978 and under the command at the time of Colonel Paulo Falcao.\n\nThe Army operates a large amount of Russian, Soviet and ex-Warsaw pact hardware. A large amount of its equipment was acquired in the 1980s and 1990s most likely because of hostilities with neighbouring countries and its civil war which lasted from November 1975 until 2002. There is an interest from the Angolan Army for the Brazilian ASTROS II multiple rocket launcher.\n\nMany of Angola's weapons are of Portuguese colonial and Warsaw Pact origin. Jane's Information Group lists the following as in service:\n\n\n\n\n\n\nThe National Air Force of Angola (FANA, \"Força Aérea Nacional de Angola\") is the air component of the FAA. It is organized in six aviation regiments, each including several squadrons. To each of the regiments correspond an air base. Besides the aviation regiments, there is also a Pilot Training School.\n\nThe Air Force's personnel total about 8,000; its equipment includes transport aircraft and six Russian-manufactured Sukhoi Su-27 fighter aircraft. In 2002 one was lost during the civil war with UNITA forces.\n\nIn 1991, the Air Force/Air Defense Forces had 8,000 personnel and 90 combat-capable aircraft, including 22 fighters, 59 fighter ground attack aircraft and 16 attack helicopters.\n\nThe Angola Navy (MGA, \"Marinha de Guerra de Angola\") is the naval component of the FAA. It is organized in two naval zones (North and South), with naval bases in Luanda, Lobito and Namibe. It includes a Marines Brigade and a Marines School, based in Ambriz. The Navy numbers about 1,000 personnel and operates only a handful of small patrol craft and barges.\n\nThe Navy has been neglected and ignored as a military arm mainly due to the guerrilla struggle against the Portuguese and the nature of the civil war. From the early 1990s to the present the Angolan Navy has shrunk from around 4,200 personnel to around 1,000, resulting in the loss of skills and expertise needed to maintain equipment. In order to protect Angola’s 1 600 km long coastline, the Angolan Navy is undergoing modernisation but is still lacking in many ways. Portugal has been providing training through its Technical Military Cooperation (CTM) programme. The Navy is requesting procurement of a frigate, three corvettes, three offshore patrol vessel and additional fast patrol boats.\n\nMost of the vessels in the navy's inventory dates back from the 1980s or earlier, and many of its ships are inoperable due to age and lack of maintenance. However the navy acquired new boats from Spain and France in the 1990s. Germany has delivered several Fast Attack Craft for border protection in 2011.\n\nIn September 2014 it was reported that the Angolan Navy would acquire seven Macaé-class patrol vessels from Brazil as part of a Technical Memorandum of Understanding (MoU) covering the production of the vessels as part of Angola’s Naval Power Development Programme (Pronaval). The military of Angola aims to modernize its naval capability, presumably due to a rise in maritime piracy within the Gulf of Guinea which may have an adverse effect on the country's economy.\n\nThe navy's current known inventory includes the following:\n\n\nThe navy also has several aircraft for maritime patrol:\n\nThe FAA include several types of special forces, namely the Commandos, the Special Operations and the Marines. The Angolan special forces follow the general model of the analogous Portuguese special forces, receiving a similar training.\n\nThe Commandos and the Special forces are part of the Special Forces Brigade (BRIFE, \"Brigada de Forças Especiais\"), based at Cabo Ledo, in the Bengo Province. The BRIFE includes two battalions of commandos, a battalion of special operations and sub-units of combat support and service support. The BRIFE also included the Special Actions Group (GAE, \"Grupo de Ações Especiais\"), which is presently inactive and that was dedicated to long range reconnaissance, covert and sabotage operations. In the Cabo Ledo base is also installed the Special Forces Training School (EFFE, \"Escola de Formação de Forças Especiais\"). Both the BRIFE and the EFFE are directly under the Directorate of Special Forces of the General Staff of the Armed Forces.\n\nThe marines (\"fuzileiros navais\") constitute the Marines Brigade of the Angolan Navy. The Marines Brigade is not permanently dependent of the Directorate of Special Forces, but can detach their units and elements to be put under the command of that body for the conduction of exercises or real operations.\n\nSince the disbandment of the Angolan Parachute Battalion in 2004, the FAA do not have a specialized paratrooper unit. However, elements of the commandos, special operations and marines are parachute qualified.\n\nThe FAPLA's main counterinsurgency effort was directed against UNITA in the southeast, and its conventional capabilities were demonstrated principally in the undeclared South African Border War. The FAPLA first performed its external assistance mission with the dispatch of 1,000 to 1,500 troops to São Tomé and Príncipe in 1977 to bolster the socialist regime of President Manuel Pinto da Costa. During the next several years, Angolan forces conducted joint exercises with their counterparts and exchanged technical operational visits. The Angolan expeditionary force was reduced to about 500 in early 1985.\n\nThe Angolan Armed Forces were controversially involved in training the armed forces of fellow Lusophone states Cape Verde and Guinea-Bissau. In the case of the latter, the 2012 Guinea-Bissau coup d'état was cited by the coup leaders as due to Angola's involvement in trying to \"reform\" the military in connivance with the civilian leadership.\n\nA small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville). A presence during the unrest in Ivory Coast, 2010–2011, were not officially confirmed. However, the \"Frankfurter Allgemeine Zeitung\", citing \"Jeune Afrique\", said that among President Gbagbo's guards were 92 personnel of President Dos Santos's Presidential Guard Unit. Angola is basically interested in the participation of the FAA operations of the African Union and has formed special units for this purpose.\n\nDavid Birmingham, African Affairs, Vol. 77, No. 309 (Oct., 1978), pp. 554–564\nPublished by: Oxford University Press on behalf of The Royal African Society\n\n"}
{"id": "710", "url": "https://en.wikipedia.org/wiki?curid=710", "title": "Foreign relations of Angola", "text": "Foreign relations of Angola\n\nThe foreign relations of Angola are based on Angola's strong support of U.S. foreign policy as the Angolan economy is dependent on U.S. foreign aid.\n\nFrom 1975 to 1989, Angola was aligned with the Eastern bloc, in particular the Soviet Union, Libya, and Cuba. Since then, it has focused on improving relationships with Western countries, cultivating links with other Portuguese-speaking countries, and asserting its own national interests in Central Africa through military and diplomatic intervention. In 1993, it established formal diplomatic relations with the United States. It has entered the Southern African Development Community as a vehicle for improving ties with its largely Anglophone neighbors to the south. Zimbabwe and Namibia joined Angola in its military intervention in the Democratic Republic of the Congo, where Angolan troops remain in support of the Joseph Kabila government. It also has intervened in the Republic of the Congo (Brazzaville) to support the existing government in that country.\n\nSince 1998, Angola has successfully worked with the United Nations Security Council to impose and carry out sanctions on UNITA. More recently, it has extended those efforts to controls on conflict diamonds, the primary source of revenue for UNITA during the Civil War that ended in 2002. At the same time, Angola has promoted the revival of the Community of Portuguese-Speaking Countries (CPLP) as a forum for cultural exchange and expanding ties with Portugal (its former ruler) and Brazil (which shares many cultural affinities with Angola) in particular. Angola is a member of the Port Management Association of Eastern and Southern Africa (PMAESA).\n\nCape Verde signed a friendship accord with Angola in December 1975, shortly after Angola gained its independence. Cape Verde and Guinea-Bissau served as stop-over points for Cuban troops on their way to Angola to fight UNITA rebels and South African troops. Prime Minister Pedro Pires sent FARP soldiers to Angola where they served as the personal bodyguards of Angolan President José Eduardo dos Santos.\n\nMany thousands of Angolans fled the country after the civil war. More than 20,000 people were forced to leave the Democratic Republic of the Congo in 2009, an action the DR Congo said was in retaliation for regular expulsion of Congolese diamond miners who were in Angola illegally. Angola sent a delegation to DR Congo's capital Kinshasa and succeeded in stopping government-forced expulsions which had become a \"tit-for-tat\" immigration dispute. \"Congo and Angola have agreed to suspend expulsions from both sides of the border,\" said Lambert Mende, DR Congo information minister, in October 2009. \"We never challenged the expulsions themselves; we challenged the way they were being conducted — all the beating of people and looting their goods, even sometimes their clothes,\" Mende said.\n\nFollowing a request by the government of Guinea-Bissau, Angola sent there a contingent of about 300 troops meant to help putting an end to the political-military unrest in that country, and to reorganize the local military forces. In fact, these troops were perceived as a kind of Pretorian Guard for the ruling party, PAIGC. In the beginning of April 2012, when a new military Coup d'état was under preparation, the Angolan regime decided to withdraw its military mission from Guinea-Bissau.\n\nNamibia borders Angola to the south. In 1999 Namibia signed a mutual defense pact with its northern neighbor Angola.\nThis affected the Angolan Civil War that had been ongoing since Angola's independence in 1975. Namibia's ruling party SWAPO sought to support the ruling party MPLA in Angola against the rebel movement UNITA, whose stronghold is in southern Angola, bordering to Namibia. The defence pact allowed Angolan troops to use Namibian territory when attacking Jonas Savimbi's UNITA.\n\nAngolan-Nigerian relations are primarily based on their roles as oil exporting nations. Both are members of the Organization of the Petroleum Exporting Countries, the African Union and other multilateral organizations.\n\nAngola-South Africa relations are quite strong as the ruling parties in both nations, the African National Congress in South Africa and the MPLA in Angola, fought together during the Angolan Civil War and South African Border War. They fought against UNITA rebels, based in Angola, and the apartheid-era government in South Africa who supported them. Nelson Mandela mediated between the MPLA and UNITA factions during the last years of Angola's civil war.\n\nAngola-Zimbabwe relations have remained cordial since the birth of both states, Angola in 1975 and Zimbabwe in 1979, during the Cold War. While Angola's foreign policy shifted to a pro-U.S. stance based on substantial economic ties, under the rule of President Robert Mugabe Zimbabwe's ties with the West soured in the late 1990s.\n\n\n\n\nRelations between the two countries have not always been cordial due to the former French government's policy of supporting militant separatists in Angola's Cabinda province and the international Angolagate scandal embarrassed both governments by exposing corruption and illicit arms deals. Following French President Nicolas Sarkozy's visit in 2008, relations have improved.\n\nAngola-Portugal relations have significantly improved since the Angolan government abandoned communism and nominally embraced democracy in 1991, embracing a pro-U.S. and to a lesser degree pro-Europe foreign policy. Portugal ruled Angola for 400 years, colonizing the territory from 1483 until independence in 1975. Angola's war for independence did not end in a military victory for either side, but was suspended as a result of a coup in Portugal that replaced the Caetano regime.\n\nRussia has an embassy in Luanda. Angola has an embassy in Moscow and an honorary consulate in Saint Petersburg. Angola and the precursor to Russia, the Soviet Union, established relations upon Angola's independence.\n\n\nThe Defence Minister of Serbia, Dragan Šutanovac, stated in a 2011 meeting in Luanda that Serbia would negotiate with the Angolan military authorities for the construction of a new military hospital in Angola.\n\n\n\nCommercial and economic ties dominate the relations of each country. Parts of both countries were part of the Portuguese Empire from the early 16th century until Brazil's independence in 1822. As of November 2007, \"trade between the two countries is booming as never before\"\n\nCanada-Angola relations were established in 1978, and Canada is accredited to Angola from a mission in Harare, Zimbabwe. Trade was established through Canada Trade Office Johannesburg, South Africa, & ties have grown since the end of the civil war in 2002, w/ increased engagement in areas of mutual interest. As Chair of the United Nations Security Council's Angola Sanctions Committee, Canada limited the ability of UNITA to continue its military campaign, sanctions helped to bring a ceasefire agreement to end Angola’s conflict. \n\n\nDuring Angola's civil war Cuban forces fought to install a Marxist–Leninist MPLA-PT government, against Western-backed UNITA and FLNA guerrillas and the South-African army.\n\n\nFrom the mid-1980s through at least 1992, the United States was the primary source of military and other support for the UNITA rebel movement, which was led from its creation through 2002 by Jonas Savimbi. The U.S. refused to recognize Angola diplomatically during this period.\n\nRelations between the United States of America and the Republic of Angola (formerly the People's Republic of Angola) have warmed since Angola's ideological renunciation of Marxism before the 1992 elections.\n\nChinese Prime Minister Wen Jiabao visited Angola in June 2006, offering a US$9 billion loan for infrastructure improvements in return for petroleum. The PRC has invested heavily in Angola since the end of the civil war in 2002. João Manuel Bernardo, the current ambassador of Angola to China, visited the PRC in November 2007.\n\nIn February 2006, Angola surpassed Saudi Arabia to become the number one supplier of oil to China. \n\nAngola-Israel relations, primarily based on trade and pro-United States foreign policies, are excellent. In March 2006, the trade volume between the two countries amounted to $400 million. The Israeli ambassador to Angola is Avraham Benjamin.[1] In 2005, President José Eduardo dos Santos visited Israel.\n\nDiplomatic relations between Japan and Angola were established in September 1976. Japan maintains an embassy at Luanda and Angola has an embassy in Tokyo. As of 2007, economic relations played \"a fundamental role in the bilateral relations between the two governments\". Japan has donated towards demining following the civil war.\n\nThe Government of Angola called for the support of Pakistan for the candidature of Angola to the seat of non-permanent member of the UN Security Council, whose election is set for September this year, during the 69th session of the General Assembly of United Nations. On the fringes of the ceremony, the Angolan diplomat also met with officials in charge of the economic and commercial policy of Pakistan, to assess the business opportunities between the two states. It asked to discuss aspects related to the cooperation on several domains of common interest.\n\nEstablishment of diplomatic relations 6 January 1992. The number of South Koreans living in Angola in 2011 was 279.\n\nAngola-Vietnam relations were established in August 1971, four years before Angola gained its independence, when future President of Angola Agostinho Neto visited Vietnam. Angola and Vietnam have steadfast partners as both transitioned from Cold War-era foreign policies of international communism to pro-Western pragmatism following the fall of the Soviet Union.\n\n\n"}
